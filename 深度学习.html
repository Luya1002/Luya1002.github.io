<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>

<style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; --title-bar-height:20px; }
.mac-os-11 { --title-bar-height:28px; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; inset: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 36px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
thead, tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-fences-adv-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
svg { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li blockquote { margin: 1rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; padding-bottom: 0px !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
  .typora-export-show-outline .typora-export-sidebar { display: none; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
.MathJax_ref { fill: currentcolor; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.reversefootnote { font-family: ui-monospace, sans-serif; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.6; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
.md-expand mark .md-meta { opacity: 0.3 !important; }
mark .md-meta { color: rgb(0, 0, 0); }
@media print {
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
}
.md-diagram-panel .messageText { stroke: none !important; }
.md-diagram-panel .start-state { fill: var(--node-fill); }
.md-diagram-panel .edgeLabel rect { opacity: 1 !important; }
.md-fences.md-fences-math { font-size: 1em; }
.md-fences-advanced:not(.md-focus) { padding: 0px; white-space: nowrap; border: 0px; }
.md-fences-advanced:not(.md-focus) { background: inherit; }
.typora-export-show-outline .typora-export-content { max-width: 1440px; margin: auto; display: flex; flex-direction: row; }
.typora-export-sidebar { width: 300px; font-size: 0.8rem; margin-top: 80px; margin-right: 18px; }
.typora-export-show-outline #write { --webkit-flex:2; flex: 2 1 0%; }
.typora-export-sidebar .outline-content { position: fixed; top: 0px; max-height: 100%; overflow: hidden auto; padding-bottom: 30px; padding-top: 60px; width: 300px; }
@media screen and (max-width: 1024px) {
  .typora-export-sidebar, .typora-export-sidebar .outline-content { width: 240px; }
}
@media screen and (max-width: 800px) {
  .typora-export-sidebar { display: none; }
}
.outline-content li, .outline-content ul { margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; list-style: none; }
.outline-content ul { margin-top: 0px; margin-bottom: 0px; }
.outline-content strong { font-weight: 400; }
.outline-expander { width: 1rem; height: 1.42857rem; position: relative; display: table-cell; vertical-align: middle; cursor: pointer; padding-left: 4px; }
.outline-expander::before { content: ""; position: relative; font-family: Ionicons; display: inline-block; font-size: 8px; vertical-align: middle; }
.outline-item { padding-top: 3px; padding-bottom: 3px; cursor: pointer; }
.outline-expander:hover::before { content: ""; }
.outline-h1 > .outline-item { padding-left: 0px; }
.outline-h2 > .outline-item { padding-left: 1em; }
.outline-h3 > .outline-item { padding-left: 2em; }
.outline-h4 > .outline-item { padding-left: 3em; }
.outline-h5 > .outline-item { padding-left: 4em; }
.outline-h6 > .outline-item { padding-left: 5em; }
.outline-label { cursor: pointer; display: table-cell; vertical-align: middle; text-decoration: none; color: inherit; }
.outline-label:hover { text-decoration: underline; }
.outline-item:hover { border-color: rgb(245, 245, 245); background-color: var(--item-hover-bg-color); }
.outline-item:hover { margin-left: -28px; margin-right: -28px; border-left: 28px solid transparent; border-right: 28px solid transparent; }
.outline-item-single .outline-expander::before, .outline-item-single .outline-expander:hover::before { display: none; }
.outline-item-open > .outline-item > .outline-expander::before { content: ""; }
.outline-children { display: none; }
.info-panel-tab-wrapper { display: none; }
.outline-item-open > .outline-children { display: block; }
.typora-export .outline-item { padding-top: 1px; padding-bottom: 1px; }
.typora-export .outline-item:hover { margin-right: -8px; border-right: 8px solid transparent; }
.typora-export .outline-expander::before { content: "+"; font-family: inherit; top: -1px; }
.typora-export .outline-expander:hover::before, .typora-export .outline-item-open > .outline-item > .outline-expander::before { content: "−"; }
.typora-export-collapse-outline .outline-children { display: none; }
.typora-export-collapse-outline .outline-item-open > .outline-children, .typora-export-no-collapse-outline .outline-children { display: block; }
.typora-export-no-collapse-outline .outline-expander::before { content: "" !important; }
.typora-export-show-outline .outline-item-active > .outline-item .outline-label { font-weight: 700; }
.md-inline-math-container mjx-container { zoom: 0.95; }


:root {
  --mermaid-theme: night;
}

[lang='mermaid'] .label {
  color: #333;
}

/* CSS Document */

/** code highlight */

.cm-s-inner .cm-variable,
.cm-s-inner .cm-operator,
.cm-s-inner .cm-property {
    color: #b8bfc6;
}

.cm-s-inner .cm-keyword {
    color: #C88FD0;
}

.cm-s-inner .cm-tag {
    color: #7DF46A;
}

.cm-s-inner .cm-attribute {
    color: #7575E4;
}

.CodeMirror div.CodeMirror-cursor {
    border-left: 1px solid #b8bfc6;
    z-index: 3;
}

.cm-s-inner .cm-string {
    color: #D26B6B;
}

.cm-s-inner .cm-comment,
.cm-s-inner.cm-comment {
    color: #DA924A;
}

.cm-s-inner .cm-header,
.cm-s-inner .cm-def,
.cm-s-inner.cm-header,
.cm-s-inner.cm-def {
    color: #8d8df0;
}

.cm-s-inner .cm-quote,
.cm-s-inner.cm-quote {
    color: #57ac57;
}

.cm-s-inner .cm-hr {
    color: #d8d5d5;
}

.cm-s-inner .cm-link {
    color: #d3d3ef;
}

.cm-s-inner .cm-negative {
    color: #d95050;
}

.cm-s-inner .cm-positive {
    color: #50e650;
}

.cm-s-inner .cm-string-2 {
    color: #f50;
}

.cm-s-inner .cm-meta,
.cm-s-inner .cm-qualifier {
    color: #b7b3b3;
}

.cm-s-inner .cm-builtin {
    color: #f3b3f8;
}

.cm-s-inner .cm-bracket {
    color: #997;
}

.cm-s-inner .cm-atom,
.cm-s-inner.cm-atom {
    color: #84B6CB;
}

.cm-s-inner .cm-number {
    color: #64AB8F;
}

.cm-s-inner .cm-variable {
    color: #b8bfc6;
}

.cm-s-inner .cm-variable-2 {
    color: #9FBAD5;
}

.cm-s-inner .cm-variable-3 {
    color: #1cc685;
}

.CodeMirror-selectedtext,
.CodeMirror-selected {
    background: #4a89dc;
    color: #fff !important;
    text-shadow: none;
}

.CodeMirror-gutters {
    border-right: none;
}

/* CSS Document */

/** markdown source **/
.cm-s-typora-default .cm-header, 
.cm-s-typora-default .cm-property
{
    color: #cebcca;
}

.CodeMirror.cm-s-typora-default div.CodeMirror-cursor{
    border-left: 3px solid #b8bfc6;
}

.cm-s-typora-default .cm-comment {
    color: #9FB1FF;
}

.cm-s-typora-default .cm-string {
    color: #A7A7D9
}

.cm-s-typora-default .cm-atom, .cm-s-typora-default .cm-number {
    color: #848695;
    font-style: italic;
}

.cm-s-typora-default .cm-link {
    color: #95B94B;
}

.cm-s-typora-default .CodeMirror-activeline-background {
    background: rgba(51, 51, 51, 0.72);
}

.cm-s-typora-default .cm-comment, .cm-s-typora-default .cm-code {
	color: #8aa1e1;
}@import "";
@import "";
@import "";

:root {
    --bg-color:  #363B40;
    --side-bar-bg-color: #2E3033;
    --text-color: #b8bfc6;

    --select-text-bg-color:#4a89dc;

    --item-hover-bg-color: #0a0d16;
    --control-text-color: #b7b7b7;
    --control-text-hover-color: #eee;
    --window-border: 1px solid #555;

    --active-file-bg-color: rgb(34, 34, 34);
    --active-file-border-color: #8d8df0;

    --primary-color: #a3d5fe;

    --active-file-text-color: white;
    --item-hover-bg-color: #70717d;
    --item-hover-text-color: white;
    --primary-color: #6dc1e7;

    --rawblock-edit-panel-bd: #333;

    --search-select-bg-color: #428bca;
}

html {
    font-size: 16px;
    -webkit-font-smoothing: antialiased;
}

html,
body {
    -webkit-text-size-adjust: 100%;
    -ms-text-size-adjust: 100%;
    background: #363B40;
    background: var(--bg-color);
    fill: currentColor;
    line-height: 1.625rem;
}

#write {
    max-width: 914px;
}


@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

html,
body,
button,
input,
select,
textarea,
div.code-tooltip-content {
    color: #b8bfc6;
    border-color: transparent;
}

div.code-tooltip,
.md-hover-tip .md-arrow:after {
    background: #333;
}

.native-window #md-notification {
    border: 1px solid #70717d;
}

.popover.bottom > .arrow:after {
    border-bottom-color: #333;
}

html,
body,
button,
input,
select,
textarea {
    font-family: "Helvetica Neue", Helvetica, Arial, 'Segoe UI Emoji', sans-serif;
}

hr {
    height: 2px;
    border: 0;
    margin: 24px 0 !important;
}

h1,
h2,
h3,
h4,
h5,
h6 {
    font-family: "Lucida Grande", "Corbel", sans-serif;
    font-weight: normal;
    clear: both;
    -ms-word-wrap: break-word;
    word-wrap: break-word;
    margin: 0;
    padding: 0;
    color: #DEDEDE
}

h1 {
    font-size: 2.5rem;
    /* 36px */
    line-height: 2.75rem;
    /* 40px */
    margin-bottom: 1.5rem;
    /* 24px */
    letter-spacing: -1.5px;
}

h2 {
    font-size: 1.63rem;
    /* 24px */
    line-height: 1.875rem;
    /* 30px */
    margin-bottom: 1.5rem;
    /* 24px */
    letter-spacing: -1px;
    font-weight: bold;
}

h3 {
    font-size: 1.17rem;
    /* 18px */
    line-height: 1.5rem;
    /* 24px */
    margin-bottom: 1.5rem;
    /* 24px */
    letter-spacing: -1px;
    font-weight: bold;
}

h4 {
    font-size: 1.12rem;
    /* 16px */
    line-height: 1.375rem;
    /* 22px */
    margin-bottom: 1.5rem;
    /* 24px */
    color: white;
}

h5 {
    font-size: 0.97rem;
    /* 16px */
    line-height: 1.25rem;
    /* 22px */
    margin-bottom: 1.5rem;
    /* 24px */
    font-weight: bold;
}

h6 {
    font-size: 0.93rem;
    /* 16px */
    line-height: 1rem;
    /* 16px */
    margin-bottom: 0.75rem;
    color: white;
}

@media (min-width: 980px) {
    h3.md-focus:before,
    h4.md-focus:before,
    h5.md-focus:before,
    h6.md-focus:before {
        color: #ddd;
        border: 1px solid #ddd;
        border-radius: 3px;
        position: absolute;
        left: -1.642857143rem;
        top: .357142857rem;
        float: left;
        font-size: 9px;
        padding-left: 2px;
        padding-right: 2px;
        vertical-align: bottom;
        font-weight: normal;
        line-height: normal;
    }

    h3.md-focus:before {
        content: 'h3';
    }

    h4.md-focus:before {
        content: 'h4';
    }

    h5.md-focus:before {
        content: 'h5';
        top: 0px;
    }

    h6.md-focus:before {
        content: 'h6';
        top: 0px;
    }
}

a {
    text-decoration: none;
    outline: 0;
}

a:hover {
    outline: 0;
}

a:focus {
    outline: thin dotted;
}

sup.md-footnote {
    background-color: #555;
    color: #ddd;
}

p {
    -ms-word-wrap: break-word;
    word-wrap: break-word;
}

p,
ul,
dd,
ol,
hr,
address,
pre,
table,
iframe,
.wp-caption,
.wp-audio-shortcode,
.wp-video-shortcode {
    margin-top: 0;
    margin-bottom: 1.5rem;
    /* 24px */
}

audio:not([controls]) {
    display: none;
}

[hidden] {
    display: none;
}

::-moz-selection {
    background: #4a89dc;
    color: #fff;
    text-shadow: none;
}

*.in-text-selection,
::selection {
    background: #4a89dc;
    color: #fff;
    text-shadow: none;
}

ul,
ol {
    padding: 0 0 0 1.875rem;
    /* 30px */
}

ul {
    list-style: square;
}

ol {
    list-style: decimal;
}

ul ul,
ol ol,
ul ol,
ol ul {
    margin: 0;
}

b,
th,
dt,
strong {
    font-weight: bold;
}

i,
em,
dfn,
cite {
    font-style: italic;
}

blockquote {
    padding-left: 1.875rem;
    margin: 0 0 1.875rem 1.875rem;
    border-left: solid 2px #474d54;
    padding-left: 30px;
    margin-top: 35px;
}

pre,
code,
kbd,
tt,
var {
    font-size: 0.875em;
    font-family: Monaco, Consolas, "Andale Mono", "DejaVu Sans Mono", monospace;
}

code,
tt,
var {
    background: rgba(0, 0, 0, 0.05);
}

kbd {
    padding: 2px 4px;
    font-size: 90%;
    color: #fff;
    background-color: #333;
    border-radius: 3px;
    box-shadow: inset 0 -1px 0 rgba(0,0,0,.25);
}

pre.md-fences {
    padding: 10px 10px 10px 30px;
    margin-bottom: 20px;
    background: #333;
}

.CodeMirror-gutters {
    background: #333;
    border-right: 1px solid transparent;
}

.enable-diagrams pre.md-fences[lang="sequence"] .code-tooltip,
.enable-diagrams pre.md-fences[lang="flow"] .code-tooltip,
.enable-diagrams pre.md-fences[lang="mermaid"] .code-tooltip {
    bottom: -2.2em;
    right: 4px;
}

code,
kbd,
tt,
var {
    padding: 2px 5px;
}

table {
    max-width: 100%;
    width: 100%;
    border-collapse: collapse;
    border-spacing: 0;
}

th,
td {
    padding: 5px 10px;
    vertical-align: top;
}

a {
    -webkit-transition: all .2s ease-in-out;
    transition: all .2s ease-in-out;
}

hr {
    background: #474d54;
    /* variable */
}

h1 {
    margin-top: 2em;
}

a {
    color: #e0e0e0;
    text-decoration: underline;
}

a:hover {
    color: #fff;
}

.md-inline-math script {
    color: #81b1db;
}

b,
th,
dt,
strong {
    color: #DEDEDE;
    /* variable */
}

mark {
    background: #D3D40E;
}

blockquote {
    color: #9DA2A6;
}

table a {
    color: #DEDEDE;
    /* variable */
}

th,
td {
    border: solid 1px #474d54;
    /* variable */
}

.task-list {
    padding-left: 0;
}

.md-task-list-item {
    padding-left: 1.25rem;
}

.md-task-list-item > input {
    top: auto;
}

.md-task-list-item > input:before {
    content: "";
    display: inline-block;
    width: 0.875rem;
    height: 0.875rem;
    vertical-align: middle;
    text-align: center;
    border: 1px solid #b8bfc6;
    background-color: #363B40;
    margin-top: -0.4rem;
}

.md-task-list-item > input:checked:before,
.md-task-list-item > input[checked]:before {
    content: '\221A';
    /*◘*/
    font-size: 0.625rem;
    line-height: 0.625rem;
    color: #DEDEDE;
}

/** quick open **/
.auto-suggest-container {
    border: 0px;
    background-color: #525C65;
}

#typora-quick-open {
    background-color: #525C65;
}

#typora-quick-open input{
    background-color: #525C65;
    border: 0;
    border-bottom: 1px solid grey;
}

.typora-quick-open-item {
    background-color: inherit;
    color: inherit;
}

.typora-quick-open-item.active,
.typora-quick-open-item:hover {
    background-color: #4D8BDB;
    color: white;
}

.typora-quick-open-item:hover {
    background-color: rgba(77, 139, 219, 0.8);
}

.typora-search-spinner > div {
  background-color: #fff;
}

#write pre.md-meta-block {
    border-bottom: 1px dashed #ccc;
    background: transparent;
    padding-bottom: 0.6em;
    line-height: 1.6em;
}

.btn,
.btn .btn-default {
    background: transparent;
    color: #b8bfc6;
}

.ty-table-edit {
    border-top: 1px solid gray;
    background-color: #363B40;
}

.popover-title {
    background: transparent;
}

.md-image>.md-meta {
    color: #BBBBBB;
    background: transparent;
}

.md-expand.md-image>.md-meta {
    color: #DDD;
}

#write>h3:before,
#write>h4:before,
#write>h5:before,
#write>h6:before {
    border: none;
    border-radius: 0px;
    color: #888;
    text-decoration: underline;
    left: -1.4rem;
    top: 0.2rem;
}

#write>h3.md-focus:before {
    top: 2px;
}

#write>h4.md-focus:before {
    top: 2px;
}

.md-toc-item {
    color: #A8C2DC;
}

#write div.md-toc-tooltip {
    background-color: #363B40;
}

.dropdown-menu .btn:hover,
.dropdown-menu .btn:focus,
.md-toc .btn:hover,
.md-toc .btn:focus {
    color: white;
    background: black;
}

#toc-dropmenu {
    background: rgba(50, 54, 59, 0.93);
    border: 1px solid rgba(253, 253, 253, 0.15);
}

#toc-dropmenu .divider {
    background-color: #9b9b9b;
}

.outline-expander:before {
    top: 2px;
}

#typora-sidebar {
    box-shadow: none;
    border-right: 1px dashed;
    border-right: none;
}

.sidebar-tabs {
    border-bottom:0;
}

#typora-sidebar:hover .outline-title-wrapper {
    border-left: 1px dashed;
}

.outline-title-wrapper .btn {
    color: inherit;
}

.outline-item:hover {
    border-color: #363B40;
    background-color: #363B40;
    color: white;
}

h1.md-focus .md-attr,
h2.md-focus .md-attr,
h3.md-focus .md-attr,
h4.md-focus .md-attr,
h5.md-focus .md-attr,
h6.md-focus .md-attr,
.md-header-span .md-attr {
    color: #8C8E92;
    display: inline;
}

.md-comment {
    color: #5a95e3;
    opacity: 0.8;
}

.md-inline-math svg {
    color: #b8bfc6;
}

#math-inline-preview .md-arrow:after {
    background: black;
}

.modal-content {
    background: var(--bg-color);
    border: 0;
}

.modal-title {
    font-size: 1.5em;
}

.modal-content input {
    background-color: rgba(26, 21, 21, 0.51);
    color: white;
}

.modal-content .input-group-addon {
    color: white;
}

.modal-backdrop {
    background-color: rgba(174, 174, 174, 0.7);
}

.modal-content .btn-primary {
    border-color: var(--primary-color);
}

.md-table-resize-popover {
    background-color: #333;
}

.form-inline .input-group .input-group-addon {
    color: white;
}

#md-searchpanel {
    border-bottom: 1px dashed grey;
}

/** UI for electron */

.context-menu,
#spell-check-panel,
#footer-word-count-info {
    background-color: #42464A;
}

.context-menu.dropdown-menu .divider,
.dropdown-menu .divider {
    background-color: #777777;
    opacity: 1;
}

footer {
    color: inherit;
}

@media (max-width: 1000px) {
    footer {
        border-top: none;
    }
    footer:hover {
        color: inherit;
    }
}

#file-info-file-path .file-info-field-value:hover {
    background-color: #555;
    color: #dedede;
}

.megamenu-content,
.megamenu-opened header {
    background: var(--bg-color);
}

.megamenu-menu-panel h2,
.megamenu-menu-panel h1,
.long-btn {
    color: inherit;
}

.megamenu-menu-panel input[type='text'] {
    background: inherit;
    border: 0;
    border-bottom: 1px solid;
}

#recent-file-panel-action-btn {
    background: inherit;
    border: 1px grey solid;
}

.megamenu-menu-panel .dropdown-menu > li > a {
    color: inherit;
    background-color: #2F353A;
    text-decoration: none;
}

.megamenu-menu-panel table td:nth-child(1) {
    color: inherit;
    font-weight: bold;
}

.megamenu-menu-panel tbody tr:hover td:nth-child(1) {
    color: white;
}

.modal-footer .btn-default, 
.modal-footer .btn-primary,
.modal-footer .btn-default:not(:hover) {
    border: 1px solid;
    border-color: transparent;
}

.btn-primary {
    color: white;
}

.btn-default:hover, .btn-default:focus, .btn-default.focus, .btn-default:active, .btn-default.active, .open > .dropdown-toggle.btn-default {
    color: white;
    border: 1px solid #ddd;
    background-color: inherit;
}

.modal-header {
    border-bottom: 0;
}

.modal-footer {
    border-top: 0;
}

#recent-file-panel tbody tr:nth-child(2n-1) {
    background-color: transparent !important;
}

.megamenu-menu-panel tbody tr:hover td:nth-child(2) {
    color: inherit;
}

.megamenu-menu-panel .btn {
    border: 1px solid #eee;
    background: transparent;
}

.mouse-hover .toolbar-icon.btn:hover,
#w-full.mouse-hover,
#w-pin.mouse-hover {
    background-color: inherit;
}

.typora-node::-webkit-scrollbar {
    width: 5px;
}

.typora-node::-webkit-scrollbar-thumb:vertical {
    background: rgba(250, 250, 250, 0.3);
}

.typora-node::-webkit-scrollbar-thumb:vertical:active {
    background: rgba(250, 250, 250, 0.5);
}

#w-unpin {
    background-color: #4182c4;
}

#top-titlebar, #top-titlebar * {
    color: var(--item-hover-text-color);
}

.typora-sourceview-on #toggle-sourceview-btn,
#footer-word-count:hover,
.ty-show-word-count #footer-word-count {
    background: #333333;
}

#toggle-sourceview-btn:hover {
    color: #eee;
    background: #333333;
}

/** focus mode */
.on-focus-mode .md-end-block:not(.md-focus):not(.md-focus-container) * {
    color: #686868 !important;
}

.on-focus-mode .md-end-block:not(.md-focus) img,
.on-focus-mode .md-task-list-item:not(.md-focus-container)>input {
    opacity: #686868 !important;
}

.on-focus-mode li[cid]:not(.md-focus-container){
    color: #686868;
}

.on-focus-mode .md-fences.md-focus .CodeMirror-code>*:not(.CodeMirror-activeline) *,
.on-focus-mode .CodeMirror.cm-s-inner:not(.CodeMirror-focused) * {
    color: #686868 !important;
}

.on-focus-mode .md-focus,
.on-focus-mode .md-focus-container {
    color: #fff;
}

.on-focus-mode #typora-source .CodeMirror-code>*:not(.CodeMirror-activeline) * {
    color: #686868 !important;
}


/*diagrams*/
#write .md-focus .md-diagram-panel {
    border: 1px solid #ddd;
    margin-left: -1px;
    width: calc(100% + 2px);
}

/*diagrams*/
#write .md-focus.md-fences-with-lineno .md-diagram-panel {
    margin-left: auto;
}

.md-diagram-panel-error {
    color: #f1908e;
}

.active-tab-files #info-panel-tab-file,
.active-tab-files #info-panel-tab-file:hover,
.active-tab-outline #info-panel-tab-outline,
.active-tab-outline #info-panel-tab-outline:hover {
    color: #eee;
}

.sidebar-footer-item:hover,
.footer-item:hover {
    background: inherit;
    color: white;
}

.ty-side-sort-btn.active,
.ty-side-sort-btn:hover,
.selected-folder-menu-item a:after {
    color: white;
}

#sidebar-files-menu {
    border:solid 1px;
    box-shadow: 4px 4px 20px rgba(0, 0, 0, 0.79);
    background-color: var(--bg-color);
}

.file-list-item {
    border-bottom:none;
}

.file-list-item-summary {
    opacity: 1;
}

.file-list-item.active:first-child {
    border-top: none;
}

.file-node-background {
    height: 32px;
}

.file-library-node.active>.file-node-content,
.file-list-item.active {
    color: white;
    color: var(--active-file-text-color);
}

.file-library-node.active>.file-node-background{
    background-color: rgb(34, 34, 34);
    background-color: var(--active-file-bg-color);
}
.file-list-item.active {
    background-color: rgb(34, 34, 34);
    background-color: var(--active-file-bg-color);
}

#ty-tooltip {
    background-color: black;
    color: #eee;
}

.md-task-list-item>input {
    margin-left: -1.3em;
    margin-top: 0.3rem;
    -webkit-appearance: none;
}

.md-mathjax-midline {
    background-color: #57616b;
    border-bottom: none;
}

footer.ty-footer {
    border-color: #656565;
}

.ty-preferences .btn-default {
    background: transparent;
}
.ty-preferences .btn-default:hover {
    background: #57616b;
}

.ty-preferences select {
    border: 1px solid #989698;
    height: 21px;
}

.ty-preferences .nav-group-item.active,
.export-item.active,
.export-items-list-control,
.export-detail {
    background: var(--item-hover-bg-color);
}

.ty-preferences input[type="search"] {
    border-color: #333;
    background: #333;
    line-height: 22px;
    border-radius: 6px;
    color: white;
}

.ty-preferences input[type="search"]:focus {
    box-shadow: none;
}

[data-is-directory="true"] .file-node-content {
    margin-bottom: 0;
}

.file-node-title {
    line-height: 22px;
}

.html-for-mac .file-node-open-state, .html-for-mac .file-node-icon {
    line-height: 26px;
}

::-webkit-scrollbar-thumb {
    background: rgba(230, 230, 230, 0.30);
}

::-webkit-scrollbar-thumb:active {
    background: rgba(230, 230, 230, 0.50);
}

#typora-sidebar:hover div.sidebar-content-content::-webkit-scrollbar-thumb:horizontal {
    background: rgba(230, 230, 230, 0.30);
}

.nav-group-item:active {
    background-color: #474d54 !important;
}

.md-search-hit {
    background: rgba(199, 140, 60, 0.81);
    color: #eee;
}

.md-search-hit * {
    color: #eee;
}

#md-searchpanel input {
    color: white;
}

.modal-backdrop.in {
    opacity: 1;
    backdrop-filter: blur(1px);
}

.clear-btn-icon {
    top: 8px;
}

/* try fix https://github.com/typora/typora-issues/issues/5253 */
.file-node-expanded>.file-node-children {
    display: grid;
  }


mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
  min-height: 1px;
  min-width: 1px;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line], svg[data-table] > g > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame], svg[data-table] > g > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed, svg[data-table] > g > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted, svg[data-table] > g > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > g > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

mjx-container[jax="SVG"] path[data-c], mjx-container[jax="SVG"] use[data-c] {
  stroke-width: 3;
}

g[data-mml-node="xypic"] path {
  stroke-width: inherit;
}

.MathJax g[data-mml-node="xypic"] path {
  stroke-width: inherit;
}
mjx-container[jax="SVG"] path[data-c], mjx-container[jax="SVG"] use[data-c] {
							stroke-width: 0;
						}
</style><title>深度学习</title>
</head>
<body class='typora-export os-windows'><div class='typora-export-content'>
<div id='write'  class=''><h1 id='深度学习'><span>深度学习</span></h1><h2 id='12-什么是神经网络'><span>1.2 什么是神经网络</span></h2><ul><li><h3 id='深度学习指的是大规模多层次的神经网络'><span>深度学习指的是大规模、多层次的神经网络</span></h3></li><li><p><span>ReLU函数  修正线性单元</span></p><ul><li><span>修正：指的是不取小于0的值</span></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230424205032130.png" referrerpolicy="no-referrer" alt="image-20230424205032130"></li></ul></li></ul><h2 id='13-用神经网络进行监督学习'><span>1.3 用神经网络进行监督学习</span></h2><ul><li><h3 id='目前由神经网络创造的真实价值基本是来自于监督学习'><span>目前由神经网络创造的真实价值基本是来自于监督学习</span></h3><ul><li><h3 id='做预测和推荐-用标准的神经网络比较多'><span>做预测和推荐 用标准的神经网络比较多</span></h3></li><li><p><span>序列数据（包括像音频、一段自然语言）用 循环神经网络 RNN</span></p></li><li><p><span>在图像领域经常应用的是CNN 卷积神经网络</span></p></li></ul></li><li><p><span>标准神经网络长什么样子</span></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230424205946399.png" referrerpolicy="no-referrer" alt="image-20230424205946399"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230424205959181.png" referrerpolicy="no-referrer" alt="image-20230424205959181"></p><ul><li><p><span>机器学习中的结构化数据和非结构化数据</span></p><ul><li><p><span>结构化</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230424210110171.png" referrerpolicy="no-referrer" alt="image-20230424210110171"></p></li><li><p><span>非结构化</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230424210120738.png" referrerpolicy="no-referrer" alt="image-20230424210120738"></p></li></ul></li></ul><h2 id='14-为什么最近深度学习才兴起'><span>1.4 为什么最近深度学习才兴起？</span></h2><p><span>深度学习和神经网络背后的技术理念已经有好几十年了</span></p><ul><li><p><span>规模推动着深度学习的发展</span></p><ul><li><p><span>规模指的是神经网络的规模以及数据的规模</span></p><p><span>在以前，收集数据比较难，并且以前的传统学习算法例如SVM支持向量机 逻辑回归 之类的，并不能很好的处理海量数据，所以无论喂给它们多少数据，最后的性能会趋于平稳（红线）</span></p><p><span>但是移动设备和互联网的加入，给收集数据提供了非常便利的条件。因此现在的数据量一下子就起来了。并且在训练过一个小型神经网络之后发现喂给他的数据越多，它的性能表现越好。</span></p><p><span>并且发现随着神经网络规模的扩大，它们可以更好的利用数据，达到更强的性能。</span></p></li><li><p><span>所以这就是为什么 规模 推动着深度学习的发展</span></p></li><li><p><span>图上左侧的small training sets表示的是，当数据集比较小的时候，所有的方法性能并不能有很明显的差距，性能差距更多的是在于算法的细节调整</span></p></li><li><p><span>而到了x轴的最右侧，数据量达到了大数据级别，这时候的性能一下子就拉开差距了。</span></p></li></ul></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230424210738297.png" referrerpolicy="no-referrer" alt="image-20230424210738297"></p><ul><li><p><span>总的来说深度学习的兴起是由规模来推动，可以看做三个因素</span></p><ul><li><span>数据量 - 随着时代发展，数据越来越多，不用再担心数据量不够的问题</span></li><li><span>计算速度 - 随着GPU CPU 计算硬件的算力提升，神经网络的迭代速度（也就是研发速度）变得很快</span></li><li><span>算法 - 算法主要是加快计算速度存在的，并且近几年一直也在有算法的创新，比如原来使用sigmoid作为激活函数会存在当y靠近0或者1的时候，斜率几乎为0，这就导致梯度下降的速度变慢（学习速度变满）。但是ReLU 修正线性单元的出现改变了这一点。.</span></li><li><span>右侧是机器学习思想的迭代图，基本上是 提出一个想法 、 编码、实验，然后看实验结果，提出想法，优化自己的代码，就是这样循环</span></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230424211536009.png" referrerpolicy="no-referrer" alt="image-20230424211536009"></p></li></ul><p>&nbsp;</p><p>&nbsp;</p><h1 id='2-神经网络编程基础'><span>2 神经网络编程基础</span></h1><h2 id='21-二分类-binary-classification'><span>2.1 二分类 Binary Classification</span></h2><p><span>目标：介绍如何处理数据集、前向传播、前向暂停、反向传播、反向暂停是啥，为什么神经网络的训练过程可以分为两个部分</span></p><p><span>注意：这节课使用逻辑回归来讲述</span></p><ul><li><h3 id='如何将图片转化为一个特征向量'><span>如何将图片转化为一个特征向量？</span></h3><p><span>假设一个64 x 64的彩色图片，它是通过三个64 x 64 矩阵来实现颜色存储的。 这三个矩阵分别存储了图片每一个像素位置的红、绿、蓝色彩强度。 如果要转换为一个特征向量把它们存储起来，那就需要从红色矩阵第一行第一个元素开始，一直到红色矩阵结束，再从绿色矩阵第一行第一个开始、最后从蓝色矩阵第一行第一个开始，到结束，以此来存储这个图片的特征向量</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425153124340.png" referrerpolicy="no-referrer" alt="image-20230425153124340"></p></li><li><h3 id='一些符号'><span>一些符号</span></h3><ul><li><p><span>（x,y）代表单独的一对样本</span></p></li><li><p><span>x是输入数据，维度为(n_x,1)，也就是一个n_x维的列向量</span></p></li><li><p><span>y是输出结果，取值为(0,1)</span></p></li><li><p><span>n_x 表示输入的特征向量的维度，简写为n</span></p></li><li><p><span>m 代表训练样本的个数</span></p><ul><li><span>强调训练集样本个数 m_test</span></li><li><span>强调测试集样本个数 m_test</span></li></ul></li><li><p><span>( x^(i) , y^(i) )代表样本 i 的输入输出</span>
<span>例如( x^1 , y^1 ) 表示第一个样本的输入输出</span></p><p>&nbsp;</p></li><li><p><span>X表示训练集，它是一个n_x * m的矩阵，里面的每一列都是一个训练样本x，所以矩阵的列数就是样本的个数。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425151535598.png" referrerpolicy="no-referrer" alt="image-20230425151535598"></p><ul><li><span>除了这种列向量的排列方式，还有一种行向量的排列方式，</span><strong><span>在实现神经网络中，这种按列向量的排列方式更为高效，会让整个实现过程变得简单</span></strong></li><li><span>python中有一个shape属性输出矩阵的维数，</span></li><li><span>如果使用 X.shape 即输出一个数组[n_x , m]</span></li></ul></li><li><p><span>Y表示输出标签y的集合，它是一个维度为(1 , m)的矩阵 </span></p><ul><li><span>如果使用 Y.shape 即输出一个数组[1 , m]</span></li></ul></li></ul></li></ul><h2 id='22-逻辑回归'><span>2.2 逻辑回归</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425154846790.png" referrerpolicy="no-referrer" alt="image-20230425154846790"></p><h2 id='23-逻辑回归损失函数-losserror-function'><span>2.3 逻辑回归损失函数 Loss(error) function </span></h2><ul><li><p><span>逻辑回归中的损失函数并不能直接使用线性回归中的损失函数（误差平方），因为最终会导致损失函数图是一个波浪线，会存在许多的局部最小值，梯度下降最后也找不到全局最小值。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425155244717.png" referrerpolicy="no-referrer" alt="image-20230425155244717"></p></li><li><p><span>所以在逻辑回归中的损失函数 L(y^hat, y)是，损失函数代表了单个样本的误差</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425155926712.png" referrerpolicy="no-referrer" alt="image-20230425155926712"></p><p><span>为什么是这个函数呢？</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425161056462.png" referrerpolicy="no-referrer" alt="image-20230425161056462"></p></li><li><p><span>如果要看全部训练样本上的表现，就需要使用代价(成本)函数</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425161207405.png" referrerpolicy="no-referrer" alt="image-20230425161207405"></p><p>&nbsp;</p></li></ul><h2 id='24-梯度下降法'><span>2.4 梯度下降法</span></h2><ul><li><p><span>梯度下降法是一种可以通过最小化代价函数，从而训练出最优参数的算法</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425190318472.png" referrerpolicy="no-referrer" alt="image-20230425190318472"></p></li><li><p><span>细节：</span></p><ul><li><span>如果代价函数有2个以上的参数，那么使用偏导符号。</span></li><li><span>如果只有1个参数，使用微分符号d</span></li></ul></li></ul><h2 id='25-导数'><span>2.5 导数</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425190905623.png" referrerpolicy="no-referrer" alt="image-20230425190905623"></p><ul><li><p><span>导数，可以看做函数的斜率 slope</span></p></li><li><p><span>斜率 slope = 高（变化量） / 宽（变化量）</span></p></li><li><p><span>导数可以在这个函数中也可写为</span></p></li><li><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n154" cid="n154" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="23.15ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 10232.4 1000" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.566ex;"><defs><path id="MJX-1-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path id="MJX-1-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-1-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-1-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-1-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-1-TEX-N-2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path><path id="MJX-1-TEX-N-A0" d=""></path><path id="MJX-1-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-1-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-1-TEX-N-2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D451" xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(520,0)"><use data-c="1D453" xlink:href="#MJX-1-TEX-I-1D453"></use></g><g data-mml-node="mo" transform="translate(1070,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1459,0)"><use data-c="1D44E" xlink:href="#MJX-1-TEX-I-1D44E"></use></g><g data-mml-node="mo" transform="translate(1988,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2377,0)"><g data-mml-node="mo"><use data-c="2F" xlink:href="#MJX-1-TEX-N-2F"></use></g></g><g data-mml-node="mi" transform="translate(2877,0)"><use data-c="1D451" xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(3397,0)"><use data-c="1D44E" xlink:href="#MJX-1-TEX-I-1D44E"></use></g><g data-mml-node="mtext" transform="translate(3926,0)"><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0"></use></g><g data-mml-node="mi" transform="translate(4176,0)"><use data-c="1D45C" xlink:href="#MJX-1-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(4661,0)"><use data-c="1D45F" xlink:href="#MJX-1-TEX-I-1D45F"></use></g><g data-mml-node="mtext" transform="translate(5112,0)"><use data-c="A0" xlink:href="#MJX-1-TEX-N-A0"></use></g><g data-mml-node="mi" transform="translate(5362,0)"><use data-c="1D451" xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5882,0)"><g data-mml-node="mo"><use data-c="2F" xlink:href="#MJX-1-TEX-N-2F"></use></g></g><g data-mml-node="mi" transform="translate(6382,0)"><use data-c="1D451" xlink:href="#MJX-1-TEX-I-1D451"></use></g><g data-mml-node="mi" transform="translate(6902,0)"><use data-c="1D44E" xlink:href="#MJX-1-TEX-I-1D44E"></use></g><g data-mml-node="mo" transform="translate(7653.2,0)"><use data-c="2217" xlink:href="#MJX-1-TEX-N-2217"></use></g><g data-mml-node="mi" transform="translate(8375.4,0)"><use data-c="1D453" xlink:href="#MJX-1-TEX-I-1D453"></use></g><g data-mml-node="mo" transform="translate(8925.4,0)"><use data-c="28" xlink:href="#MJX-1-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(9314.4,0)"><use data-c="1D44E" xlink:href="#MJX-1-TEX-I-1D44E"></use></g><g data-mml-node="mo" transform="translate(9843.4,0)"><use data-c="29" xlink:href="#MJX-1-TEX-N-29"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>d</mi><mi>f</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>d</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>o</mi><mi>r</mi><mtext>&nbsp;</mtext><mi>d</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>d</mi><mi>a</mi><mo>∗</mo><mi>f</mi><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></div></div><p><span>它的含义是，在函数f(a)上，每当a移动一点，f(a)就增加了多少倍移动量</span></p><p><span>“多少倍”指的就是斜率，如果斜率为3，那么a 移动 0.001会让f(a)也就是函数值增加0.003</span></p></li></ul><h2 id='26-more-derivatives-examples-更多的导数例子'><span>2.6 more derivatives examples 更多的导数例子</span></h2><h2 id='27-计算图'><span>2.7 计算图</span></h2><ul><li><p><span>一个神经网络的计算，都是按照前向以及反向传播过程组织的</span></p><ul><li><span>前向传播可以计算出神经网络的输出</span></li><li><span>反向传播可以计算出斜率或者导数</span></li></ul></li><li><p><span>计算图解释了为什么我们会使用这两种方式来组织整个计算过程</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425194610904.png" referrerpolicy="no-referrer" alt="image-20230425194610904"></p><p><span>                 图为计算图计算出整个函数的例子，蓝色箭头</span></p></li><li><p><span>概括一下：计算图组织计算的形式是用蓝色箭头从左到右的计算</span></p></li></ul><h2 id='28-使用计算图求导'><span>2.8 使用计算图求导</span></h2><ul><li><p><span>使用计算图求导就是反向传播在做的事情</span></p></li><li><p><span>里面提到的数学知识为 链式法则</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425205544828.png" referrerpolicy="no-referrer" alt="image-20230425205544828"></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230425205706885.png" referrerpolicy="no-referrer" alt="image-20230425205706885"></p><ul><li><p><span>给了一张计算图，如果我想要求出dj / dv 怎么办？</span></p><ul><li><span>思路：增加v的值，看J增加了多少</span></li><li><span>如果v + 0.001 = 11.001，那么j = 33.003。可以知道dj/dv 也就是导数/斜率为3.</span></li></ul></li><li><p><span>想求出dj/da怎么办？</span></p><p><span>如果想知道dj / da，也就是求出当a增加，会让j产生多少增加那就得先求出dj/dv和dv/da，如果a + 0.001 = 5.001，v = 11.001，dv/da = 1</span></p><p><span>最后dj/da = dj/dv * dv / da = 3</span></p></li><li><p><span>在反向传播中，我们最后要求的就是损失函数J，它也被称为</span><strong><span>最终输出值</span></strong></p></li><li><p><span>而反向传播也就是在计算对于最终输出值的导数，比如dj / dv，dj / da，所以在python中，我们使用d_变量名来表示过程中某一个变量的对最终输出值的导数</span></p><ul><li><span>例如da = dj/da = dj / dv * dv / da</span></li><li><span>例如dv = dj / dv</span></li></ul></li><li><p><span>python中表示某一个部分的斜率就直接写da db dc du dv </span></p></li></ul><p>&nbsp;</p><h2 id='29-逻辑回归中的梯度下降法'><span>2.9 逻辑回归中的梯度下降法</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426164428191.png" referrerpolicy="no-referrer" alt="image-20230426164428191"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426164438866.png" referrerpolicy="no-referrer" alt="image-20230426164438866"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426164449246.png" referrerpolicy="no-referrer" alt="image-20230426164449246"></p><ul><li><span>这一节课就是带着你知道了梯度下降中 w =  w - αdw中的 dw是如何计算出来的</span></li></ul><h2 id='210-m个样本的梯度下降法'><span>2.10 m个样本的梯度下降法</span></h2><ul><li><p><span>这节课讲述了实现一步 关于m个样本的梯度下降法</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426184614199.png" referrerpolicy="no-referrer" alt="image-20230426184614199"></p></li><li><p><span>这里的缺点有两个</span></p><ul><li><span>使用了两个for循环，一个控制训练样本 1 &gt; m ，第二个for循环循环了所有的参数</span></li><li><span>特征如果太多，参数也会变多，导致第二个for循环中需要写的东西变多</span></li></ul></li><li><p><span>所以为了解决这两个问题 向量化是个好办法。</span></p></li></ul><h2 id='211-向量化'><span>2.11 向量化</span></h2><ul><li><p><span>实验出 相同的事情，使用np.dot来做，比单计算的速度快300倍左右</span></p></li><li><p><span>GPU CPU都可以做并行化，有一个并行化的指令就叫SIMD 单指令流多数据流 single instruction multiple data，GPU表现比CPU好</span></p></li><li><p><span>因为向量化有专门的硬件来并发执行，所以速度很快</span></p></li><li><h2 id='只要有可能就别用for用向量化'><span>只要有可能，就别用for，用向量化</span></h2></li></ul><h2 id='212-向量化的更多例子'><span>2.12 向量化的更多例子</span></h2><ul><li><p><span>如果是计算两个矩阵的点乘，使用np.dot函数即可</span></p></li><li><p><span>如果将矩阵内的所有数进行这个运算，就可以使用np.exp(v)函数</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426202319587.png" referrerpolicy="no-referrer" alt="image-20230426202319587"></p></li><li><p><span>计算对数的话就用np.log(x)</span></p></li><li><p><span>计算绝对值np.abs(x)</span></p></li><li><p><span>计算最大值np.maxinum(x,y)</span></p></li><li><p><span>A.sum(axis=0) 对 矩阵A的列求和，如果axis = 1 那就是对行求和</span></p></li><li><p><span>np.zerros(x,y)填充一个x行 y列的矩阵</span></p></li><li><p><span>如果a是一个矩阵，那a.T就是它的转置矩阵</span></p></li><li><h2 id='能不使用for就不使用for'><span>能不使用for就不使用for</span></h2></li></ul><h2 id='213-向量化逻辑回归'><span>2.13 向量化逻辑回归</span></h2><p><span>就讲了一个事情，如何使用向量化，而不是for来算出Z</span></p><p><span>原本</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426221002355.png" referrerpolicy="no-referrer" alt="image-20230426221002355"></p><p><span>向量化就是这样了</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426220855604.png" referrerpolicy="no-referrer" alt="image-20230426220855604"></p><h2 id='214-向量化-logistic-回归的梯度输出'><span>2.14 向量化 logistic 回归的梯度输出</span></h2><ul><li><p><span>这节课主要是将如何通过python实现向量化的、并行的logistic 回归的梯度输出</span></p><p>&nbsp;</p></li><li><p><span>例如在原来的梯度下降中 dz^(i) = a^(i) - y^(i)，是这样进行的，然后把它放在一个循环里。</span></p><ul><li><p><span>而向量化需要将 a 和 y 变成1 x m的矩阵或者是m维的行向量A和Y</span></p><p><span>然后进行 dZ = a - y 就可以了</span></p><p><span>其他的请看下图</span><span>	</span></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426223749772.png" referrerpolicy="no-referrer" alt="image-20230426223749772"></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426223756291.png" referrerpolicy="no-referrer" alt="image-20230426223756291"></p><p><span>上图前五行完成了向前向后传播，后面两行完成了一次梯度下降</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230426223805443.png" referrerpolicy="no-referrer" alt="image-20230426223805443"></p><h2 id='215-python中的广播-boardcasting'><span>2.15 Python中的广播 boardcasting</span></h2><ul><li><p><span>numpy 在算术运算期间采用“广播”来处理具有不同形状的 array ，</span><strong><span>即将较小的阵列在较大的阵列上“广播”，以便它们具有兼容的形状。</span></strong></p></li><li><p><span>匹配规则是：</span></p><ul><li><span>先匹配第二维，也就是列</span></li><li><span>如果其中一个列 = 1 或者 两个列值相等，就是广播兼容的，那么就会把短的那个轴拉长。</span></li><li><span>否则会报错。</span></li></ul></li><li><p><span>广播的结论是</span></p><ul><li><strong><span>任意 m x n矩阵 四则运算 常数，最后结果是m x n矩阵</span></strong></li><li><strong><span>任意 m x n 矩阵 四则运算 另一个矩阵 m x 1 / m x n ，最后是m x n 矩阵</span></strong></li></ul><p><img src="https://img-blog.csdnimg.cn/7245e83ccc8941b09ed7099458d61bbe.png" referrerpolicy="no-referrer" alt="图示"></p></li></ul><p><img src="https://img-blog.csdnimg.cn/094924143b3e46009bc73f928f7d9a07.png" referrerpolicy="no-referrer" alt="如果一维 array 元素的数量与二维 array 的数量相匹配，则添加到二维 array 的一维 array 会导致广播。"></p><p><img src="https://img-blog.csdnimg.cn/876bc92f60204790b622579b82ca5f83.png" referrerpolicy="no-referrer" alt="图示"></p><h2 id='216-numpy向量的说明'><span>2.16 numpy向量的说明</span></h2><p><span>这节课主要是在讲python做神经网络需要注意的问题</span></p><ul><li><p><span>创建矩阵的时候，要使用第二种方法，第一种方法创建出来的是一个 秩为1的数组，它和列、行向量相乘 最后得到一个数，而不是一个矩阵。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230427175025295.png" referrerpolicy="no-referrer" alt="image-20230427175025295"></p></li><li><p><span>可以使用.reshape(x,y)函数来把向量重新变化成你想要的样子</span></p></li><li><p><span>可以使用assert(a.shape == (x,y)) 来判断该矩阵是几行几列的</span></p><ul><li><p><span>assert叫断言函数，是对表达式布尔值的判断，要求表达式计算值必须为真。可用于自动调试。</span></p><p><span>如果表达式</span><strong><span>为假，触发异常</span></strong><span>；如果表达式为真，不会报错。</span></p></li></ul></li></ul><h2 id='217--jupyter-notebook的使用教学'><span>2.17  jupyter notebook的使用教学</span></h2><h2 id='218-这周的quiz有用的部分'><span>2.18 这周的quiz有用的部分</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230427181649528.png" referrerpolicy="no-referrer" alt="image-20230427181649528"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230427182346559.png" referrerpolicy="no-referrer" alt="image-20230427182346559"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230427183716474.png" referrerpolicy="no-referrer" alt="image-20230427183716474"></p><h2 id='219-第二周编程作业'><span>2.19 第二周编程作业</span></h2><p><a href='https://www.heywhale.com/mw/project/5dd236a800b0b900365eca9b'><span>吴恩达《深度学习》L1W2作业1 - Heywhale.com</span></a></p><ul><li><p><span>在做作业的时候发现了他们进行了特征缩放，这里是相关概念的解读，可以复习用，序号为阅读顺序</span></p><ol start='' ><li><a href='https://blog.csdn.net/bluishglc/article/details/128696871'><span>(24条消息) 标准化和归一化概念澄清与梳理</span><em><span>标准差归一化</span></em><span>　Laurence的博客-CSDN博客</span></a></li><li><a href='https://blog.csdn.net/bluishglc/article/details/128642156?spm=1001.2014.3001.5501'><span>(24条消息) 范数的意义与计算方法</span><em><span>范数的计算公式</span></em><span>　Laurence的博客-CSDN博客</span></a></li><li><a href='https://blog.csdn.net/bluishglc/article/details/128723321'><span>(24条消息) Sklearn标准化和归一化方法汇总(3)：范数归一化_　Laurence的博客-CSDN博客</span></a></li></ol></li><li><p><span>作业的向量化中讲了 点/外部/元素乘积之间的区别（没用向量化）</span></p><ul><li><p><span>设 x1 = (1, 2, 3)  x2 = (4, 5, 6)</span></p></li><li><p><span>点积：输出一个实数，实数为两个向量的元素依次相乘，最终求和</span></p></li><li><p><span>外积：输出一个矩阵，矩阵的维度分别为 (x1.length, x2.length)，矩阵的每一个位置的分量为 与下标对应的元素的乘积</span></p><ul><li><span>例如 矩阵A(2,3)  = x1[2] * x2[3]</span></li></ul></li><li><p><span>元素乘积：输出一个向量，向量的长度为x1 或 x2的长度，数组的每一个元素是x1 x2对应下标的乘积</span></p><ul><li><span>例如 输出向量out[2] =  x1[2] * x2[2]</span></li></ul></li><li><p><span>激活值的计算，输出一个向量</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429214446241.png" referrerpolicy="no-referrer" alt="image-20230429214446241"></p></li></ul></li><li><p><span>向量化实现</span></p><ul><li><span>设 x1 = (1, 2, 3)  x2 = (4, 5, 6)</span></li><li><span>点积： np.dot(x1,x2)</span></li><li><span>外积：np.outer(x1,x2)</span></li><li><span>元素乘积：np.multiply(x1,x2)  </span></li><li><span>激活值的计算：np.dot(W,x1)</span></li><li><span>np.multiply和 * 一样，执行的是逐个元素的相乘，而dot执行的是矩阵-矩阵乘法 / 矩阵向量乘法</span></li></ul></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429215010952.png" referrerpolicy="no-referrer" alt="image-20230429215010952"></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429215020248.png" referrerpolicy="no-referrer" alt="image-20230429215020248"></p></li><li><h3 id='预处理数据集的常见步骤是'><span>预处理数据集的常见步骤是：</span></h3><ul><li><p><span>找出数据的尺寸和维度（m_train，m_test，num_px等）</span></p></li><li><p><span>重塑数据集，以使每个示例都是大小为（num_px \ *num_px </span><span>*</span><span> 3，1）的向量</span></p></li><li><p><span>“标准化”数据</span></p><ul><li><span>如果是普通数据，那么从每个示例中减去整个numpy数组的均值，然后除以整个numpy数组的标准差</span></li><li><span>如果是图片，将数据集的每一行除以255（像素通道的最大值）</span></li></ul></li></ul></li><li><h3 id='建立神经网络的主要步骤是'><span>建立神经网络的主要步骤是：</span></h3><p><span>1.定义模型结构（例如输入特征的数量）</span>
<span>2.初始化模型的参数</span>
<span>3.循环：</span></p><ul><li><span>计算当前损失（正向传播）</span></li><li><span>计算当前梯度（向后传播）</span></li><li><span>更新参数（梯度下降）</span></li></ul><p><span>你通常会分别构建1-3，然后将它们集成到一个称为“ model（）”的函数中。</span></p></li><li><p><strong><span>此作业要记住的内容：</span></strong></p><ol start='' ><li><span>预处理数据集很重要。</span></li><li><span>如何实现每个函数：initialize（），propagation（），optimize（），并用此构建一个model（）。</span></li><li><span>调整学习速率（这是“超参数”的一个示例）可以对算法产生很大的影响。 你将在本课程的稍后部分看到更多示例！</span></li></ol></li></ul><h1 id='3-神经网络'><span>3 神经网络</span></h1><h2 id='31-神经网络概览'><span>3.1 神经网络概览</span></h2><ul><li><span>这节课讲了啥是一个神经网络</span></li></ul><p><span>andrew使用计算图来映射到一个标准结构的神经网络中</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230427190823068.png" referrerpolicy="no-referrer" alt="image-20230427190823068"></p><ul><li><p><span>计算图中，参数和特征 就是神经网络的输入x</span></p></li><li><p><span>x会有一个方括号上标，代表该输入和第几层神经网络相关</span></p></li><li><p><span>神经网络的输入层是第0层</span></p><p><span>第一层是第一个隐藏层</span></p><p><span>输出是最后一层</span></p></li><li><p><span>每个隐藏层的神经单元、神经元在计算的东西就是计算图中每一个长方形格子计算的函数</span></p><ul><li><span>例如第一层就是在做z的计算，第二层在做a的计算，输出层输出y^ 也就是 L损失函数值</span></li></ul></li><li><p><span>计算图可以前向反向传播，神经网络也可以</span></p></li></ul><p><span>## </span></p><h2 id='32-神经网络表示'><span>3.2 神经网络表示</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230427224220631.png" referrerpolicy="no-referrer" alt="image-20230427224220631"></p><ul><li><p><span>x被称为输入特征，它被堆叠起来</span></p></li><li><p><span>我们使用a^[i]标记每一层的激活值（输出值）</span></p><ul><li><p><span>（在这个例子中，我们假设这是一个逻辑回归神经网络）</span></p></li><li><p><span>输入层 a^[0] 为第0层，为神经网络的输入</span></p></li><li><p><span>中间的含有四个结点的层是隐藏层 a^[1]</span></p><ul><li><span>可以使用 a^[a]_b 来指明是第a层的第b个单元</span></li><li><span>隐藏层包含着两个参数W和B，在上图的例子中，W是一个4 x 3的矩阵（因为有3个特征值，一共有4个单元）</span></li><li><span>B是一个4x1的矩阵/4维的列向量</span></li></ul></li><li><p><span>最后的含有一个结点的层是输出层 a^[2]，这层负责输出预测值，也就是y^</span></p><ul><li><span>最后的输出也有着W和B两个参数，分别是1x4和1x1的矩阵。</span></li></ul></li><li><p><span>这个就是一个双层神经网络，我们把第一个隐藏层当成第一层，输入层是第0层</span></p></li></ul></li><li><p><span>为啥叫隐藏层呢？因为中间的准确值我们不知道</span></p><p><span>你能</span><strong><span>看见输入的值</span></strong><span>，你也能看见输出的值，但是隐藏层中的东西，在 训练集中你是无法看到的。所以这也解释了词语隐藏层，</span><strong><span>只是表示你无法在训练集中看到他们</span></strong></p></li></ul><h2 id='33-计算一个神经网络的输出'><span>3.3 计算一个神经网络的输出</span></h2><ul><li><p><span>假设是一个搞逻辑回归的双层神经网络，输入和输出层只是单纯输入和输出最终数据，那么中间隐藏层干了什么？并且如何实现一个样本的计算</span></p></li><li><p><span>我们知道计算逻辑回归需要两步，首先计算线性回归的方程，然后将其带入进激活函数sigmod函数，计算出概率。</span><strong><span>所以隐藏层的单元就是在做这个事情。它同时做了两个式子的计算</span></strong></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428150016981.png" referrerpolicy="no-referrer" alt="image-20230428150016981"></p><ul><li><span>注意，</span><strong><span>在神经元中，输入的 inputs 通过加权，求和后，还被作用了一个函数，这个函数就是激活函数。</span></strong><span>	</span></li><li><span>激活函数的主要作用就是增加模型的非线性</span></li></ul><p><span>那么四个单元的计算过程，就对应了下面的四个等式（非向量化计算），最终得到了激活值a^[1]</span><span>_</span><span>1 ~ a^[1]</span><span>_</span><span>4，四个激活值就是下一个单元的输入值。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428150433982.png" referrerpolicy="no-referrer" alt="image-20230428150433982"></p></li><li><p><span>为了提升速度就需要使用向量化计算，把这四个等式都进行向量化</span></p></li><li><p><span>如何向量化？</span></p><ul><li><span>向量化的过程是将神经网络中的一层神经元</span><strong><span>参数纵向堆积</span></strong><span>起来，例如隐藏层中的𝑤纵向堆积起来变成一个(4,3)的矩阵，用符号𝑊[1]表示。</span></li></ul></li><li><p><span>那么按照上面的思路</span></p><ul><li><span>参数w堆叠起来变成一个4x3的矩阵W</span></li><li><span>特征x堆叠起来变成一个3x1的列向量X</span></li><li><span>常量b堆叠起来变成一个4x1的列向量B</span></li><li><span>计算W * X + b 就可以得到四个值，这四个值就是z</span></li><li><span>再把z堆叠起来变成一个4x1的列向量 Z</span></li><li><span>最后将Z作为激活sigmoid函数的输入，输出a^[1]的激活值，a^[1]是一个4x1的矩阵，或者4维的列向量</span></li><li><span>这样a^[1]就求出来了</span></li><li><span>接着把将a^[1]输入第二层，</span></li><li><span>Z^[2] = W^[2] * a^[1] + b^[2]</span></li><li><span>a[2] = sigmod(Z^[2])</span></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428151223599.png" referrerpolicy="no-referrer" alt="image-20230428151223599"></p></li></ul><h2 id='34-多样本的向量化'><span>3.4 多样本的向量化</span></h2><ul><li><p><span>a^[2]</span><span>(</span><span>i)的含义，[2]指的是第二层的神经元/单元/结点 (i)指的是第几个训练样本</span></p></li><li><p><span>如何实现多样本的向量化呢？     </span><strong><span>列向量堆叠</span></strong></p><p><span>例如 特征有x1 x2 x3， 把他们竖着堆叠起来，放入一个矩阵X里，每一个训练样本一列，这就构成了一个 n_x,m的特征值矩阵，这就是向量化</span></p></li><li><p><span>上述讲单个样本的计算过程时候，还有z[1] a[1] z[2] a[2]四个值产生，把他们向量化的思路和之前向量化X的一样</span></p></li><li><p><span>把z^[1]</span><span>(</span><span>1) ~ z^[1]</span><span>(</span><span>m)以列向量的形式放在Z^[1]中就完成了向量化</span></p></li><li><p><span>A^[1] Z^[2] A^[2]一样</span></p><p>&nbsp;</p><p><span>也就如下图所示</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428153558672.png" referrerpolicy="no-referrer" alt="image-20230428153558672"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428231928643.png" referrerpolicy="no-referrer" alt="image-20230428231928643"></p><p><span>向量化前需要for来循环遍历所有的样本，以此计算出对应的z[1] a[1] z[2] a[2]，过程比较慢，所以向量化的用处就体现出来了</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428153522393.png" referrerpolicy="no-referrer" alt="image-20230428153522393"></p></li></ul><p>&nbsp;</p><h2 id='35-向-量-化-实-现-的-解-释'><span>3.5 向 量 化 实 现 的 解 释</span></h2><ul><li><span>就是讲为哈上面的做法是对的</span></li></ul><h2 id='36-激活函数'><span>3.6 激活函数</span></h2><ul><li><p><strong><span>创建神经网络的时候可以选择隐层使用的激活函数</span></strong></p></li><li><p><span>之前的例子里我们一直使用的sigmoid函数，有时候选择别的函数效果会更好（因为σ函数的值域是0 ~ 1 ）</span></p></li><li><p><span>我们</span><strong><span>使用g^(Z[i])来代表非线性函数</span></strong><span>，不一定是σ（sigma）函数</span></p></li><li><h2 id='tanh函数'><span>tanh函数</span></h2><ul><li><span>有一个函数的表现总会比σ函数好，它是</span><strong><span>tanh函数 双曲正切函数，值域为 -1 ~ 1</span></strong><span>，可以看做σ平移后的版本</span></li></ul><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n544" cid="n544" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="20.613ex" height="5.021ex" role="img" focusable="false" viewBox="0 -1451.2 9110.7 2219.2" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.738ex;"><defs><path id="MJX-2-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-2-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-2-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-2-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path id="MJX-2-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-2-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-2-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-2-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-2-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-2-TEX-N-A0" d=""></path><path id="MJX-2-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-2-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D461" xlink:href="#MJX-2-TEX-I-1D461"></use></g><g data-mml-node="mi" transform="translate(361,0)"><use data-c="1D44E" xlink:href="#MJX-2-TEX-I-1D44E"></use></g><g data-mml-node="mi" transform="translate(890,0)"><use data-c="1D45B" xlink:href="#MJX-2-TEX-I-1D45B"></use></g><g data-mml-node="mi" transform="translate(1490,0)"><use data-c="210E" xlink:href="#MJX-2-TEX-I-210E"></use></g><g data-mml-node="mo" transform="translate(2066,0)"><use data-c="28" xlink:href="#MJX-2-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(2455,0)"><use data-c="1D467" xlink:href="#MJX-2-TEX-I-1D467"></use></g><g data-mml-node="mo" transform="translate(2920,0)"><use data-c="29" xlink:href="#MJX-2-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3586.8,0)"><use data-c="3D" xlink:href="#MJX-2-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4642.6,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(499,363) scale(0.707)"><use data-c="1D467" xlink:href="#MJX-2-TEX-I-1D467"></use></g></g><g data-mml-node="mtext" transform="translate(877.8,0)"><use data-c="A0" xlink:href="#MJX-2-TEX-N-A0"></use></g><g data-mml-node="mo" transform="translate(1350,0)"><use data-c="2212" xlink:href="#MJX-2-TEX-N-2212"></use></g><g data-mml-node="mtext" transform="translate(2350.2,0)"><use data-c="A0" xlink:href="#MJX-2-TEX-N-A0"></use></g><g data-mml-node="msup" transform="translate(2600.2,0)"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(499,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="2212" xlink:href="#MJX-2-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(778,0)"><use data-c="1D467" xlink:href="#MJX-2-TEX-I-1D467"></use></g></g></g></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="mi" transform="translate(499,289) scale(0.707)"><use data-c="1D467" xlink:href="#MJX-2-TEX-I-1D467"></use></g></g><g data-mml-node="mtext" transform="translate(877.8,0)"><use data-c="A0" xlink:href="#MJX-2-TEX-N-A0"></use></g><g data-mml-node="mo" transform="translate(1350,0)"><use data-c="2B" xlink:href="#MJX-2-TEX-N-2B"></use></g><g data-mml-node="mtext" transform="translate(2350.2,0)"><use data-c="A0" xlink:href="#MJX-2-TEX-N-A0"></use></g><g data-mml-node="msup" transform="translate(2600.2,0)"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-2-TEX-I-1D452"></use></g><g data-mml-node="TeXAtom" transform="translate(499,289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="2212" xlink:href="#MJX-2-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(778,0)"><use data-c="1D467" xlink:href="#MJX-2-TEX-I-1D467"></use></g></g></g></g><rect width="4228.2" height="60" x="120" y="220"></rect></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>z</mi></msup><mtext>&nbsp;</mtext><mo>−</mo><mtext>&nbsp;</mtext><msup><mi>e</mi><mrow data-mjx-texclass="ORD"><mo>−</mo><mi>z</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>z</mi></msup><mtext>&nbsp;</mtext><mo>+</mo><mtext>&nbsp;</mtext><msup><mi>e</mi><mrow data-mjx-texclass="ORD"><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></math></mjx-assistive-mml></mjx-container></div></div><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428233218660.png" referrerpolicy="no-referrer" alt="image-20230428233218660"></p><ul><li><p><span>特点</span></p><ul><li><p><span>它会让隐层的激活函数平均值更接近0，而不是0.5，这样给下一层学习的效果会更好，也被称为</span><strong><span>数据中心化</span></strong></p></li><li><p><span>在所有场合tanh函数表现都比σ函数好</span><strong><span>除了用于二元分类的输出层</span></strong><span>。</span></p><p><span>如果训练集的y在0~1之间，那么让y^在0~1之间更为合理，而不是-1~1，所以这里使用sigmoid函数更好</span></p></li></ul></li><li><p><strong><span>从这里开始σ函数基本不会再使用</span></strong></p></li><li><p><span>不同层的激活函数可以不同，表示不同的激活函数可以写g^[i]</span><span>(</span><span>Z^[i])</span></p></li><li><p><span>tanh函数和sigmoid函数的缺点就是如果Z的值太大或者太小，将会导致斜率接近零，进而导致梯度下降速度变满，所以解决这个问题，ReLU函数 ReLU（修复线性单元）出现了</span></p></li></ul></li><li><h2 id='relu函数-修复线性单元'><span>ReLU函数 修复线性单元</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428234150579.png" referrerpolicy="no-referrer" alt="image-20230428234150579"></p></li><li><p><strong><span>公式是 a = max (0 , z )</span></strong></p></li><li><p><strong><span>如果z是正数，那么斜率就是1  ； 如果z是负数，那么斜率就是0</span></strong></p></li><li><p><span>函数在z = 0处没有定义，但是实践中 z正好等于0的情况太少太少了，所以不用担心，编程中如果出现了这个问题，给导数赋值0 或者 1</span></p></li><li><p><strong><span>缺点：当z为负数时候，导数/斜率为0</span></strong></p></li><li><p><span>所以针对上面的问题，发展出了 </span><strong><span>带泄露的ReLU</span></strong><span> 即 </span><strong><span>a = (0.01z , z)</span></strong><span>，它在z为负数的时候斜率变得比较平缓，它比ReLU更好，但是用的更少</span><strong><span>，主要还是使用ReLU就够了</span></strong></p></li><li><p><span>也不一定使用0.01z，你当他是个参数，你可以自己调整</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230428234616698.png" referrerpolicy="no-referrer" alt="image-20230428234616698"></p></li><li><p><span>使用ReLU或者带泄露的ReLU做激活函数带来的效果就是 训练速度比tanh和sigmoid快，因为ReLU没有斜率接近0的地方，并且由于有足够多的隐藏单元让z大于0，所以对于大多数训练样本ReLU的效果都比较好，训练比较快</span></p></li><li><h2 id='选择激活函数的小贴士'><span>选择激活函数的小贴士</span></h2><ul><li><h2 id='如果你做二元分类输出为0-1那默认使用relu函数做隐层输出层sigmoid函数'><span>如果你做二元分类，输出为0 1，那默认使用ReLU函数做隐层，输出层sigmoid函数</span></h2><p><span>这个方案用的人比较多，也有些人使用tanh函数</span></p></li><li><p><span>深度学习有着很多的选择，这是它的特点。隐层的单元数、激活函数、初始化权值，都是需要选择的。而神经网络有可以做很多种的工作，有着很多不同的应用场景，很难说哪种选各种会更好。</span></p></li><li><h2 id='andrew提到的思路是如果不确定那个激活函数效果更好可以把它们都试一试然后放在验证集或者发展集上进行评价看那个效果好就用那个'><span>andrew提到的思路是：如果不确定那个激活函数效果更好，可以把它们都试一试，然后放在验证集或者发展集上进行评价，看那个效果好，就用那个。</span></h2><h3 id='这不是穷举吗'><span>这不是穷举吗？！！？！？</span></h3></li></ul><h2 id='37-为什么需要非线性激活函数'><span>3.7 为什么需要非线性激活函数？</span></h2><ul><li><h3 id='为什么神经网络需要非线性激活函数事实证明要让你的神经网络能够计算出有趣-函数你必须使用非线性激活函数'><span>为什么神经网络需要非线性激活函数？事实证明：要让你的神经网络能够计算出有趣 函数，你必须使用非线性激活函数</span></h3></li><li><p><span>线性激活函数有一个学名叫是</span><strong><span>恒等激励函数</span></strong><span>，它们就是把输入值原封不动的再输出出来。</span></p><ul><li><span>例如上面的双层神经网络，如果隐层的激活函数是线性激活函数，那么隐层的输入是 x ，输出是 z^[1] = w^T * x + b。如果再让第二层计算的话，第二层的输出 z = w^[2]T * z^[1] + b，那么这个模型最后的输出结果仅仅是关于特征x的线性组合。</span></li></ul></li><li><h3 id='将恒等激励函数作为隐层的激活函数那隐层一点意义都没有还不如去掉隐层'><span>将恒等激励函数作为隐层的激活函数，那隐层一点意义都没有，还不如去掉隐层。</span></h3></li><li><h3 id='在之前的双层神经网络中如果隐层的激活函数是恒等激励函数那整个模型的复杂度和效果和标准的逻辑回归是一样的'><span>在之前的双层神经网络中，如果隐层的激活函数是恒等激励函数，那整个模型的复杂度和效果和标准的逻辑回归是一样的</span></h3></li><li><h3 id='使用恒定激励函数作为激活函数的情况非常少只有在做回归问题的时候用在输出层'><span>使用恒定激励函数作为激活函数的情况非常少，只有在做回归问题的时候用在输出层</span></h3></li></ul><h2 id='38-激活函数的导数'><span>3.8 激活函数的导数</span></h2><ul><li><p><span>在反向传播的时候需要计算激活函数的斜率/导数，这节课介绍了四种激活函数分别介绍了他们的导数</span></p></li><li><h2 id='sigmoid函数'><span>sigmoid函数</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429002818267.png" referrerpolicy="no-referrer" alt="image-20230429002818267"></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429002853257.png" referrerpolicy="no-referrer" alt="image-20230429002853257"></p><ul><li><span>g(z)^撇是之后会使用的标记，意思是求函数g(z)对变量z的导数/斜率</span></li><li><span>知道斜率是a(1-a)，那通过计算a的值，就可以计算出导数。</span></li></ul></li><li><h2 id='tanhz函数-双曲面正切函数'><span>tanh(z)函数 双曲面正切函数</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429003024151.png" referrerpolicy="no-referrer" alt="image-20230429003024151"></p></li></ul></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429003409176.png" referrerpolicy="no-referrer" alt="image-20230429003409176"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429003414678.png" referrerpolicy="no-referrer" alt="image-20230429003414678"></p><ul><li><p><span>如果使用 a来表示函数 g(z)，那么 </span><strong><span>g(z)的导数 = 1 - a^2</span></strong></p></li><li><h2 id='relu函数和带泄露的relu函数'><span>ReLU函数和带泄露的ReLU函数</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429003530621.png" referrerpolicy="no-referrer" alt="image-20230429003530621"></p><ul><li><p><span>ReLU</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429003542653.png" referrerpolicy="no-referrer" alt="image-20230429003542653"></p><ul><li><span>可以把 z = 0的情况并入到 if z &gt; 0 中，改为 if z &gt;= 0 ， z = 1</span></li><li><span>因为z = 0的情况太少太少了，所以怎么定义都无所谓，程序都可以跑起来</span></li></ul></li><li><p><span>带泄露的ReLU</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429003723076.png" referrerpolicy="no-referrer" alt="image-20230429003723076"></p><ul><li><span>可以把 z = 0的情况并入到 if z &gt; 0 中，改为 if z &gt;= 0 ， z = 1</span></li><li><span>因为z = 0的情况太少太少了，所以怎么定义都无所谓，程序都可以跑起来</span></li></ul></li></ul></li></ul><h2 id='39-神经网络的梯度下降'><span>3.9 神经网络的梯度下降</span></h2><ul><li><p><span>这节课就是讲</span><strong><span>单隐层神经网络的反向传播或者说梯度下降算法的方程组</span></strong></p></li><li><p><span>n_x表示输入特征的个数 n_x = a^[0]， n^[1]表示隐藏单元个数，n^[2]表示输出单元个数</span></p><ul><li><span>目前我们见过的神经网络的n^[2]  = 1</span></li></ul></li><li><p><span>单隐层神经网络中会有 W^[1] ， b^[1] ，W^[2]，b^[2]这四个参数（都已经向量化）</span></p><ul><li><p><span>所以每个参数的向量化矩阵维度是</span></p><ul><li><span>W^[1]  是 (n^[1] , n_x)</span></li><li><span>b^[1] 是 (n^[1]  , 1)</span></li><li><span>W^[2] 是 (n^[2] , n^[1])</span></li><li><span>b^[2] 是 (1,1) 或者 (n^2 , 1)</span></li></ul></li></ul></li><li><p><span>如果假设这个神经网络在做二分类任务，那么成本函数和之前逻辑回归一样</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n669" cid="n669" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="44.537ex" height="4.613ex" role="img" focusable="false" viewBox="0 -1342 19685.2 2039" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.577ex;"><defs><path id="MJX-3-TEX-I-1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path><path id="MJX-3-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-3-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-3-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path id="MJX-3-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-3-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path><path id="MJX-3-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-3-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-3-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-3-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-3-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-3-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path><path id="MJX-3-TEX-I-1D6F4" d="M65 0Q58 4 58 11Q58 16 114 67Q173 119 222 164L377 304Q378 305 340 386T261 552T218 644Q217 648 219 660Q224 678 228 681Q231 683 515 683H799Q804 678 806 674Q806 667 793 559T778 448Q774 443 759 443Q747 443 743 445T739 456Q739 458 741 477T743 516Q743 552 734 574T710 609T663 627T596 635T502 637Q480 637 469 637H339Q344 627 411 486T478 341V339Q477 337 477 336L457 318Q437 300 398 265T322 196L168 57Q167 56 188 56T258 56H359Q426 56 463 58T537 69T596 97T639 146T680 225Q686 243 689 246T702 250H705Q726 250 726 239Q726 238 683 123T639 5Q637 1 610 1Q577 0 348 0H65Z"></path><path id="MJX-3-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-3-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-3-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D43D" xlink:href="#MJX-3-TEX-I-1D43D"></use></g><g data-mml-node="mo" transform="translate(633,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(1022,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g><g data-mml-node="TeXAtom" transform="translate(1136.2,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-3-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-3-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(2954.9,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="msup" transform="translate(3399.6,0)"><g data-mml-node="mi"><use data-c="1D44F" xlink:href="#MJX-3-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(462,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-3-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-3-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(4658.3,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="msup" transform="translate(5102.9,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-3-TEX-I-1D44A"></use></g><g data-mml-node="TeXAtom" transform="translate(1136.2,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-3-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="32" xlink:href="#MJX-3-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-3-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(7035.8,0)"><use data-c="2C" xlink:href="#MJX-3-TEX-N-2C"></use></g><g data-mml-node="msup" transform="translate(7480.5,0)"><g data-mml-node="mi"><use data-c="1D44F" xlink:href="#MJX-3-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(462,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-3-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="32" xlink:href="#MJX-3-TEX-N-32"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-3-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(8739.2,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(9406,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(10461.8,0)"><g data-mml-node="mn" transform="translate(409,676)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220,-686)"><use data-c="1D45A" xlink:href="#MJX-3-TEX-I-1D45A"></use></g><rect width="1078" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(12002,0)"><use data-c="2217" xlink:href="#MJX-3-TEX-N-2217"></use></g><g data-mml-node="msubsup" transform="translate(12724.2,0)"><g data-mml-node="mi"><use data-c="1D6F4" xlink:href="#MJX-3-TEX-I-1D6F4"></use></g><g data-mml-node="mi" transform="translate(890.3,413) scale(0.707)"><use data-c="1D45A" xlink:href="#MJX-3-TEX-I-1D45A"></use></g><g data-mml-node="TeXAtom" transform="translate(813,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-3-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-3-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-3-TEX-N-31"></use></g></g></g><g data-mml-node="mi" transform="translate(14734.9,0)"><use data-c="1D43F" xlink:href="#MJX-3-TEX-I-1D43F"></use></g><g data-mml-node="mo" transform="translate(15415.9,0)"><use data-c="28" xlink:href="#MJX-3-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(15804.9,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-3-TEX-I-1D466"></use></g><g data-mml-node="TeXAtom" transform="translate(523,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">帽</text></g><g data-mml-node="mtext" transform="translate(852,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">子</text></g></g></g><g data-mml-node="mo" transform="translate(17805.9,0)"><use data-c="2212" xlink:href="#MJX-3-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(18806.2,0)"><use data-c="1D466" xlink:href="#MJX-3-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(19296.2,0)"><use data-c="29" xlink:href="#MJX-3-TEX-N-29"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>J</mi><mo stretchy="false">(</mo><msup><mi>W</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>,</mo><msup><mi>b</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>,</mo><msup><mi>W</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo>,</mo><msup><mi>b</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>∗</mo><msubsup><mi>Σ</mi><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mi>L</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mtext>帽</mtext><mtext>子</mtext></mrow></msup><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></div></div></li><li><p><span>损失函数L()</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n672" cid="n672" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="50.864ex" height="2.7ex" role="img" focusable="false" viewBox="0 -943.3 22482 1193.3" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.566ex;"><defs><path id="MJX-4-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-4-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-4-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-4-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-4-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-4-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-4-TEX-N-2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path><path id="MJX-4-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-4-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-4-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-4-TEX-N-A0" d=""></path><path id="MJX-4-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-4-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D43F" xlink:href="#MJX-4-TEX-I-1D43F"></use></g><g data-mml-node="mo" transform="translate(681,0)"><use data-c="28" xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(1070,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-4-TEX-I-1D466"></use></g><g data-mml-node="TeXAtom" transform="translate(523,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">帽</text></g><g data-mml-node="mtext" transform="translate(852,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">子</text></g></g></g><g data-mml-node="mo" transform="translate(2848.9,0)"><use data-c="2C" xlink:href="#MJX-4-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(3293.5,0)"><use data-c="1D466" xlink:href="#MJX-4-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(3783.5,0)"><use data-c="29" xlink:href="#MJX-4-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(4450.3,0)"><use data-c="3D" xlink:href="#MJX-4-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(5506.1,0)"><use data-c="2212" xlink:href="#MJX-4-TEX-N-2212"></use></g><g data-mml-node="mo" transform="translate(6284.1,0)"><use data-c="28" xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(6673.1,0)"><use data-c="1D466" xlink:href="#MJX-4-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(7385.3,0)"><use data-c="2217" xlink:href="#MJX-4-TEX-N-2217"></use></g><g data-mml-node="mi" transform="translate(8107.5,0)"><use data-c="1D459" xlink:href="#MJX-4-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(8405.5,0)"><use data-c="1D45C" xlink:href="#MJX-4-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(8890.5,0)"><use data-c="1D454" xlink:href="#MJX-4-TEX-I-1D454"></use></g><g data-mml-node="mtext" transform="translate(9367.5,0)"><use data-c="A0" xlink:href="#MJX-4-TEX-N-A0"></use></g><g data-mml-node="msup" transform="translate(9617.5,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-4-TEX-I-1D466"></use></g><g data-mml-node="TeXAtom" transform="translate(523,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">帽</text></g><g data-mml-node="mtext" transform="translate(852,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">子</text></g></g></g><g data-mml-node="mo" transform="translate(11618.6,0)"><use data-c="2B" xlink:href="#MJX-4-TEX-N-2B"></use></g><g data-mml-node="mo" transform="translate(12618.8,0)"><use data-c="28" xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(13007.8,0)"><use data-c="31" xlink:href="#MJX-4-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(13730,0)"><use data-c="2212" xlink:href="#MJX-4-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(14730.3,0)"><use data-c="1D466" xlink:href="#MJX-4-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(15220.3,0)"><use data-c="29" xlink:href="#MJX-4-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(15831.5,0)"><use data-c="2217" xlink:href="#MJX-4-TEX-N-2217"></use></g><g data-mml-node="mi" transform="translate(16553.7,0)"><use data-c="1D459" xlink:href="#MJX-4-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(16851.7,0)"><use data-c="1D45C" xlink:href="#MJX-4-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(17336.7,0)"><use data-c="1D454" xlink:href="#MJX-4-TEX-I-1D454"></use></g><g data-mml-node="mo" transform="translate(17813.7,0)"><use data-c="28" xlink:href="#MJX-4-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(18202.7,0)"><use data-c="31" xlink:href="#MJX-4-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(18924.9,0)"><use data-c="2212" xlink:href="#MJX-4-TEX-N-2212"></use></g><g data-mml-node="msup" transform="translate(19925.1,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-4-TEX-I-1D466"></use></g><g data-mml-node="TeXAtom" transform="translate(523,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">帽</text></g><g data-mml-node="mtext" transform="translate(852,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">子</text></g></g></g><g data-mml-node="mo" transform="translate(21704,0)"><use data-c="29" xlink:href="#MJX-4-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(22093,0)"><use data-c="29" xlink:href="#MJX-4-TEX-N-29"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mtext>帽</mtext><mtext>子</mtext></mrow></msup><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mo stretchy="false">(</mo><mi>y</mi><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mtext>&nbsp;</mtext><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mtext>帽</mtext><mtext>子</mtext></mrow></msup><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mtext>帽</mtext><mtext>子</mtext></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></div></div></li></ul><p>&nbsp;</p><ul><li><h3 id='在训练神经网络的时候随机初始化参数很重要而不是初始化成全零'><span>在训练神经网络的时候，随机初始化参数很重要，而不是初始化成全零。</span></h3></li><li><h3 id='当你参数初始化成某些值后每次梯度下降都会循环计算以下预测值'><span>当你参数初始化成某些值后，每次梯度下降都会循环计算以下预测值</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429010134734.png" referrerpolicy="no-referrer" alt="image-20230429010134734"></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429010216901.png" referrerpolicy="no-referrer" alt="image-20230429010216901"></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230429010224610.png" referrerpolicy="no-referrer" alt="image-20230429010224610"></p><ul><li><p><span>这些都是他们求导求出来的，想要知道为什么就看3.10</span></p></li><li><h3 id='npsum中axis1是求横轴的值keepdims是保证函数输出m-x-n矩阵而不是秩为1的n-矩阵除了这种方法还可以显性的调用reshapemn函数把-npsum-输出结果写成矩阵形式'><span>np.sum中，axis=1是求横轴的值；keepdims是保证函数输出(m x n)矩阵，而不是秩为1的(n, 矩阵，除了这种方法还可以显性的调用.reshape(m,n)函数把 np.sum 输出结果写成矩阵形式</span></h3></li></ul><h2 id='310-直观理解反向传播'><span>3.10 直观理解反向传播</span></h2><p><span>放弃！有一个点还没学过——矩阵求导、矩阵求导链式法则</span></p><h2 id='311-随机初始化'><span>3.11 随机初始化</span></h2><ul><li><h3 id='在神经网络中随机初始化参数很重要如果使用标准的逻辑回归把参数初始化为0没问题但是在神经网络中将各个参数w全都初始化为0会导致梯度下降完全无效偏置项b没关系）'><span>在神经网络中随机初始化参数很重要，如果使用标准的逻辑回归，把参数初始化为0没问题。但是在神经网络中将各个参数W全都初始化为0会导致梯度下降完全无效（偏置项b没关系）</span></h3><ul><li><p><span>w^T</span><span>*</span><span>x + b，b叫做偏置项</span></p></li><li><h3 id='因为隐层的无论多少个单元他们计算的函数都是一样的前向传播一样）经过梯度下降后两个单元的函数还是一样的反向传播一样）以此往复两个单元一直是一样的即隐含单元是对称的）那隐层的多个单元就没意义了因为他们在计算一样的东西'><span>因为隐层的无论多少个单元，他们计算的函数都是一样的（前向传播一样），经过梯度下降后，两个单元的函数还是一样的（反向传播一样），以此往复两个单元一直是一样的（即隐含单元是对称的），那隐层的多个单元就没意义了，因为他们在计算一样的东西。</span></h3></li></ul></li><li><p><span>如何解决这个问题？  —— 随机初始化</span></p><p><span>W^[1] = np.random.randn(n1, n_x) * 0.01</span></p><ul><li><span>为什么这里是0.01？而不是其他常数？</span></li><li><span>因为如果在做二元分类任务，使用tanh或者sigmoid函数做激活函数的话，如果W的值太大/太小，会导致Z的值太大或者太小，最后导致函数过于饱和，梯度下降非常缓慢，所以为了避免这个问题 我们一般把特征的权重弄的很小，这样梯度下降速度是正常的</span></li><li><span>0.01在浅层的神经网络中或许还是个好选择，但是在深层神经网络或者深度学习中，这就不是一个好办法了，有其他的常数是更好的选择，之后会提到</span></li></ul><p><span>b^[1] = np.zeros(n1,1)</span></p><p><span>注意：b初始化为0没有问题，因为参数W经过了随机初始化，这会让不同的单元计算不同的函数，不会产生对称性，所以可以初始化为0</span></p><p>&nbsp;</p></li></ul><h2 id='312--第三周编程作业--用1层隐藏层的神经网络分类二维数据'><span>3.12  第三周编程作业  用1层隐藏层的神经网络分类二维数据</span></h2><ul><li><p><span>建立神经网络的一般方法是：</span></p><p><span>1.定义神经网络结构（输入单元数，隐藏单元数等）。</span></p><p><span>2.初始化模型的参数（随机初始化）</span></p><p><span>3.循环：</span></p><ul><li><span>实现前向传播</span></li><li><span>计算损失</span></li><li><span>后向传播以获得梯度</span></li><li><span>更新参数（梯度下降）</span></li></ul></li></ul><p><span>我们通常会构建辅助函数来计算第1-3步，然后将它们合并为</span><code>nn_model()</code><span>函数。一旦构建了</span><code>nn_model()</code><span>并学习了正确的参数，就可以对新数据进行预测。</span></p><h1 id='4--深层神经网络'><span>4  深层神经网络</span></h1><h2 id='41-深层神经网络'><span>4.1 深层神经网络</span></h2><ul><li><p><span>深度是神经网络层数的象征</span></p></li><li><h3 id='深层神经网络的符号'><span>深层神经网络的符号</span></h3><ul><li><p><span>L 表示神经网络的层数</span></p></li><li><p><span>n^[l] 表示第 l 层的节点数量</span></p></li><li><p><span>a^[l] 表示第 l 层的激活函数</span></p><ul><li><span>a^[l] = g^[l]</span><span>(</span><span>z^[l])</span></li></ul></li><li><p><span>w^[l] 表示在a^[l]中计算z^[l]值的权重</span></p></li><li><p><span>b^[l] 表示在a^[l]中计算z^[l]值的偏置值</span></p></li><li><p><span>x 表示输入特征，同时是第0层的激活函数 a^[0] = x </span></p></li><li><p><span>a^[L = y^，也就是预测的输出</span></p></li></ul></li></ul><h2 id='42-深层神经网络的前向反向传播'><span>4.2 深层神经网络的前向/反向传播</span></h2><ul><li><span>深层神经网络的每一层都会有前向传播步骤和相反的反向传播步骤，这节课讲讲如何实现这些步骤。</span></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501011632127.png" referrerpolicy="no-referrer" alt="image-20230501011632127"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501011636124.png" referrerpolicy="no-referrer" alt="image-20230501011636124"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501011658021.png" referrerpolicy="no-referrer" alt="image-20230501011658021"></p><h2 id='43-深层网络中的前向传播'><span>4.3 深层网络中的前向传播</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501012814618.png" referrerpolicy="no-referrer" alt="image-20230501012814618"></p><ul><li><p><span>深层神经网络的前向传播和单隐层的神经网络的前向传播没有什么区别，只是多重复了几遍</span></p></li><li><h3 id='减少深层神经网络bug的方法就是仔细检查和思考每一个矩阵的维数下节课讲'><span>减少深层神经网络BUG的方法就是仔细检查和思考每一个矩阵的维数，下节课讲</span></h3></li></ul><h2 id='44-核对矩阵的维数'><span>4.4 核对矩阵的维数</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501154324901.png" referrerpolicy="no-referrer" alt="image-20230501154324901"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501154342572.png" referrerpolicy="no-referrer" alt="image-20230501154342572"></p><h2 id='45-为什么使用深层表示'><span>4.5 为什么使用深层表示？</span></h2><ul><li><p><span>这节课就是在探讨为什么使用深度学习网络效果会比较好，其实对于一些任务我们并不需要一个很巨大的神经网络，但是需要深度，需要隐层的数量比较多，来看看为什么。</span></p></li><li><h2 id='深度神经网络在做什么'><span>深度神经网络在做什么？</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501155413282.png" referrerpolicy="no-referrer" alt="image-20230501155413282"></p><p><span>例如在做人脸识别系统，输入一张图片，神经网络的第一层就会做边缘探测器的工作，去探测图片中的每一个 边缘（也就是一条直线、曲线），把它们输出。</span></p><p><span>然后在第二层将第一层的输出尝试组合在一起，组成不同的面部部位，比如眼睛鼻子之类的</span></p><p><span>第三层再把不同的部位组合在一起，形成一张人脸， 最后看看这是谁的脸</span></p><p>&nbsp;</p><h3 id='那么知觉上来理解深度神经网络的前几层在做探测工作比如第一层的边缘探测每一层的探测结果都会是下一层的输入下一层再把前一层的图像组合起来变成脸上部位的图像再下一层把部位组装变成一张人脸最后输出识别结果'><span>那么知觉上来理解，深度神经网络的前几层在做“探测工作”，比如第一层的边缘探测。每一层的探测结果都会是下一层的输入，下一层再把前一层的图像组合起来变成脸上部位的图像，再下一层把部位组装，变成一张人脸，最后输出识别结果。</span></h3><p><span>那么第一层就是在做边缘探测器，第二层在做五官探测器，最后一层在做面部探测器</span></p><ul><li><p><span>这种从简单到复杂的金字塔状表示方法或者组成方法，也可以应用在图像或者人脸识别以外的其他数据上。比如当你想要建一个语音识别系统的时候。</span></p></li><li><h3 id='深度神经网络的思路来自于-大脑运行的规律即先从比较小的细节入手比如边缘然后再一步步到更大更复杂的区域'><span>深度神经网络的思路来自于 大脑运行的规律，即先从比较小的细节入手，比如边缘，然后再一步步到更大更复杂的区域。</span></h3></li><li><h3 id='所以深度神经网络的这许多隐藏层中较早的前几层能学习一些低层次的简单特征等-到后几层就能把简单的特征结合起来去探测更加复杂的东西'><span>所以深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等 到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。</span></h3></li></ul></li><li><h2 id='为什么深度神经网络不需要广度而需要深度'><span>为什么深度神经网络不需要广度，而需要深度？</span></h2><h3 id='深层的网络隐藏单元数量相对较少隐藏层数目较多如果浅层的网络想要达到同样的-计算结果则需要指数级增长的单元数量才能达到'><span>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的 计算结果则需要指数级增长的单元数量才能达到。</span></h3><ul><li><p><span>andrew用电路理论来解释了这点</span></p><p><span>电路也就是与门、或门、非门、异或门</span></p><p><span>假设要求 x_1 ~ x_n的异或，如果你使用一些异或门来解决问题的话，那它的复杂度是O(log n)，而如果使用单隐层神经网络的话，它的复杂度是O(2n)</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501161120861.png" referrerpolicy="no-referrer" alt="image-20230501161120861"></p></li><li><h3 id='所以很多数学函数用深度网络计算比浅网络要容易得多'><span>所以，很多数学函数用深度网络计算比浅网络要容易得多</span></h3></li></ul></li><li><p><span>andrew使用深度神经网络的步骤：</span></p><ol start='' ><li><span>先用逻辑回归看看效果</span></li><li><span>试试1 - 2个隐层，也就是说 把隐层的数字当成一个参数来调整，找到合适的深度</span></li></ol></li><li><p><span>有的人喜欢堆很多很多很多的隐层，但对于有些问题这或许可行，但是有些问题，这就不是一个好的方法了。像是编程作业中也看到，20层隐层的效果并不如5层隐层的好。</span></p></li></ul><h2 id='46-搭建深层神经网络块'><span>4.6 搭建深层神经网络块</span></h2><ul><li><p><span>深度神经网络的重要组成部分：前向传播、反向传播</span></p></li><li><p><span>我们知道在监督学习中，深度神经网络/神经网络主要是由隐层的单元组成的，在深度学习中，我们会对每一个单元会做两件事前向传播和反向传播</span></p></li><li><h2 id='前向传播过程计算预测值）'><span>前向传播过程（计算预测值）</span></h2><ul><li><p><span>步骤：</span></p><ul><li><span>给一层输入 a^[ l - 1 ]，输出a^[ l ]，保存cache( z^ [ l ] )</span></li><li><span>其中会用到参数 W^[ l ] 和 b^[ l ]</span></li><li><span>前向传播一直循环两个步骤，直到计算出 a ^ [L]，也就是 y^为止</span></li><li><span>过程图如下所示。</span></li></ul></li><li><p><span>正向函数，也就是计算z^[ l ]和 a ^[ l ]的函数</span></p><ul><li><span>z ^ [ l ] = W^[ l ]  * x + b ^ [ l ]</span></li><li><span>a ^ [ l ] = g^[ l ] ( z ^ [ l ])</span></li></ul></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501163234842.png" referrerpolicy="no-referrer" alt="image-20230501163234842"></p></li><li><h2 id='反向传播过程计算全部导数为梯度下降做准备）'><span>反向传播过程（计算全部导数，为梯度下降做准备）</span></h2><ul><li><p><span>步骤</span></p><ul><li><p><span>给最后一层输入 da^[L]，输出da^[L - 1]，保存dW^[L]和db^[L]</span></p></li><li><p><span>其中会用到cache中的z^[L]以及W^[L]和b^[L]</span></p></li><li><p><span>反向传播一直循环两个步骤，直到计算出da^[2]和da^[1]为止（da^[0]可以计算，但是没有意义，因为特征不是我们需要梯度下降的对象）</span></p></li><li><p><span>过程如下所示</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501164004584.png" referrerpolicy="no-referrer" alt="image-20230501164004584"></p></li><li><p><span>反向函数，也就是反向传播中使用到计算dz^[ l ]和 dz^[ l - 1 ]的函数</span></p><ul><li><span>dz^[ l ] = da^[ l ] * dg^[ l ]</span><span>(</span><span>z ^ [ l ])</span></li><li><span>dw^[ l ] = dz^[ l ] * a[ l - 1 ]</span></li><li><span>db^[ l ] = dz^[ l ]</span></li><li><span>da^[ l - 1 ]  = dz^[ l ] * W^[ l ]T</span></li></ul></li></ul></li></ul></li><li><h3 id='反向传播结束之后进行梯度下降最后这么一轮下来前向传播反向传播梯度下降）就是神经网络的一次梯度下降循环'><span>反向传播结束之后进行梯度下降，最后这么一轮下来（前向传播、反向传播、梯度下降）就是神经网络的一次梯度下降循环</span></h3></li><li><h2 id='过程中为什么保存了cache了呢'><span>过程中为什么保存了cache了呢？</span></h2><ul><li><span>把正向函数计算出来的𝑧 缓存下来，可以迅速得到 𝑊[𝑙]和𝑏 [𝑙]的值，在编程练习中你缓存了𝑧，还有𝑊和𝑏对吧？从实现角度上看，我认为是一个很方便的方法，可以将参数复制到你在计算反向传播时所需要的地方。 </span></li></ul></li></ul><h2 id='47-参数vs-超参数'><span>4.7 参数VS 超参数</span></h2><ul><li><p><span>在神经网络中你会有一些参数来完成前向传播和反向传播</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501165148969.png" referrerpolicy="no-referrer" alt="image-20230501165148969"></p><p><span>除了参数之外，还需要另一些参数来需要设置，比如学习率、隐层数、隐层单元数、梯度下降法循环的数量（iterations）、激活函数使用那个（sigmoid、ReLU、tanh），这些就是超参数</span></p></li><li><p><span>这些超参数实际上会影响最后W和b的值，也就是控制了W和b的值的参数，所以就叫超参数</span></p></li><li><h3 id='如何取得最后的超参数值'><span>如何取得最后的超参数值？</span></h3><p><strong><span>自己尝试</span></strong><span>，就像是下图的过程</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230501191128696.png" referrerpolicy="no-referrer" alt="image-20230501191128696"></p></li><li><p><span>要注意的是，超参数可能是会变的，可能几个月 可能一年，那么你就需要多多尝试超参数，检验结果，看看有没有更好的超参数。</span></p></li><li><p><span>尝试也是深度学习的一部分</span></p></li></ul><h2 id='48-这和大脑有什么关系'><span>4.8 这和大脑有什么关系？</span></h2><p><span>没关系</span></p><h1 id='5-改善深层神经网络超参数调试-正-则-化-以-及-优-化'><span>5 改善深层神经网络：超参数调试、 正 则 化 以 及 优 化</span></h1><h1 id='第一周深度学习的实践层面'><span>第一周：深度学习的实践层面</span></h1><h2 id='11-训练验证测试集train--dev--test-sets）'><span>1.1 训练，验证，测试集（Train / Dev / Test sets）</span></h2><ul><li><p><span>在配置训练、验证和测试数据集的过程中做出正确决策会在很大程度上帮助大家创建高 效的神经网络。</span></p></li><li><p><span>应用型机器学习是一个高度迭代的过程</span></p><ul><li><span>训练神经网络的时候我们需要做出很多决策（调整超参数），每一个隐层多少个单元，学习的速率是多少，用那个激活函数等  </span></li><li><span>不管是从业了多久的人，创建新应用的过程中，也近乎不可能从一开始就准确预测出这些信息和其他超级参数，因此就需要去迭代，去实验</span></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502162446375.png" referrerpolicy="no-referrer" alt="image-20230502162446375"></li><li><span>想法、编码、实验、反馈结果、再提出想法、编码.. 这就是神经网络的迭代过程，经过了如此往复的迭代才能为应用程序找到一个称心的神经网络</span></li></ul></li><li><h3 id='其中创建高质量的训练数据集验证集和测试集也有助于提高循环效-率'><span>其中创建高质量的训练数据集，验证集和测试集也有助于提高循环效 率</span></h3></li><li><p><span>数据会分为训练集、 验证集、测试集</span></p><ul><li><span>训练集训练算法</span></li><li><span>验证集的目的就是验证不同的算法，检验哪种算法更有效</span></li><li><span>一旦算法选择完成，训练完成，使用测试集测试</span></li></ul></li><li><p><span>以前数据量小的时候 100 - 1w条，三个集会 7/3分，或者是 6/2/2分</span></p></li><li><p><span>大数据时代数据量是百万级别，以前的方法就不适用了</span></p><ul><li><span>总体来看 验证集和测试集的大小占总数据量的比例会比较小，基本是20% - 10%以内</span></li><li><span>验证集需要足够大，来分别测试不同算法的表现，如果有100万条数据，那么验证集1万个数据基本就可以了</span></li><li><span>测试集是测试最终分类器的性能（也就是输出层单元），如果有100万条数据，那么测试集1000数据基本就可以了</span></li></ul></li><li><p><span>如果数据量是一百万</span></p><ul><li><span>可以训练集98%，测试集和验证集各1%</span></li><li><span>过百万的应用，训练集99.5%，测试和验证各0.25% / 或验证集占 0.4%，测试 集占 0.1%</span></li></ul></li><li><p><span>现代深度学习的另一个趋势是越来越多的人</span><strong><span>在训练和测试集分布不匹配的情况下进行训练</span></strong></p><ul><li><span>意思是训练集和测试集、验证集的样本质量不同，例如如果是看一张图是不是猫图，可能训练集的图片都很清楚 很专业，但验证集和测试集的图片比较一般</span><strong><span>，这种情况尽量避免，</span></strong><span>最好让三个集的质量类似只要遵循这个经验法则，你就会发现机器学习算法会变得更快</span></li></ul></li><li><p><strong><span>没有测试集也可以</span></strong><span>，测试集的目的就是对最终的神经网络系统做出无偏估计，如果不需要无偏估计，也就不需要测试集。那你最后就是使用训练集来训练，验证集来验证那个算法最好，验证集中已经涵盖了一些测试集的数据，其不再提供无偏性能评估。</span></p><p><strong><span>上面的这种没有测试集的情况里，人们一般会将验证集叫做测试集，</span></strong><span>不过在实际应用中，人们只是 把测试集当成简单交叉验证集使用，并没有完全实现该术语的功能，因为他们把验证集数据 过度拟合到了测试集中。</span><strong><span>改称其为“训练验证集”，而不是“训练测试集”，可能不太容易。即便 我认为“训练验证集“在专业用词上更准确。</span></strong></p></li></ul><h2 id='12-偏差方差bias-variance）'><span>1.2 偏差，方差（Bias /Variance）</span></h2><ul><li><p><span>关于深度学习的误差问题，另一个趋势是对偏差和方差的权衡研究甚浅，你可能听说过这两 个概念，但深度学习的误差很少权衡二者，我们总是分别考虑偏差和方差，却很少谈及偏差 和方差的权衡问题，</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502173123807.png" referrerpolicy="no-referrer" alt="image-20230502173123807"></p><p><span>高偏差high bias（左一），欠拟合 </span></p><p><span>正好（中间），适度拟合</span></p><p><span>高方差high variance（右一），过拟合</span></p></li><li><p><span>理解偏差和方差的两个关键指标：训练集误差和验证集误差</span></p><p><span>偏差 bias：预测值和真实值的偏差，如果训练集误差大，偏差也大</span></p><p><span>方差 variance：训练集和开发集（验证集）的数据分布是否均匀，这会导致模型在这两个集上的差别，如果验证集误差比训练集误差大太多，那么方差大</span></p><ul><li><p><span>高偏差和高方差在训练集误差和验证集误差中如何体现？</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502173751354.png" referrerpolicy="no-referrer" alt="image-20230502173751354"></p><h3 id='假设我们在做二元分类看图片里有没有猫'><span>假设我们在做二元分类，看图片里有没有猫</span></h3><h3 id='假设人眼的误差是0最优误差也叫贝叶斯误差）是本次分析的基准误差）那么我们分析'><span>假设人眼的误差是0%（最优误差，也叫贝叶斯误差）（是本次分析的基准误差），那么我们分析：</span></h3><p><span>左1 ：训练集误差小，说明训练集设置的好。但是验证集误差比较大，说明存在过拟合训练集，也就是高方差</span></p><p><span>左2：训练集和验证集都训练的不好，也就是存在训练集欠拟合的情况，但是验证集的输出是正常的，只多了1%的误差，因此是高偏差</span></p><p><span>左3：训练集和验证集都训练的不好，并且验证集表现的过于差，也就是说在算法欠拟合的基础上，又有算法过拟合情况，也就是高偏差和高方差都有</span></p><p><span>4：最理想的情况（是优质的分类器），训练集误差较小，验证集误差也很小，也就是适度拟合，低偏差 低方差</span></p></li><li><h3 id='上面的分析都是以人类的误差最优误差）为基准的如果人眼在识别猫猫图片上的误差是15那么左2的情况就是低方差和低偏差了是适度拟合了'><span>上面的分析都是以人类的误差（最优误差）为基准的，如果人眼在识别猫猫图片上的误差是15%，那么左2的情况就是低方差和低偏差了，是适度拟合了。</span></h3></li><li><p><span>如果没有分类器可以来识别这些图片，比如一张模糊的照片，那人和机器就都不好认出图片的内容了，最后导致最优误差也很高，那么分析偏差和方差的分析过程就会改变，之后再讨论。</span></p></li><li><h3 id='以上分析的前提都是假设基本误差很小训练集和验证集数据来自相同分布如果没有-这些假设作为前提分析过程更加复杂我们将会在稍后课程里讨论'><span>以上分析的前提都是假设基本误差很小，训练集和验证集数据来自相同分布，如果没有 这些假设作为前提，分析过程更加复杂，我们将会在稍后课程里讨论。</span></h3></li></ul></li><li><p><span>高偏差 和 高方差的例子</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502175319307.png" referrerpolicy="no-referrer" alt="image-20230502175319307"></p><ul><li><p><span>直线并没有很好的拟合数据，所以是欠拟合；并且中间的曲线并不自然，过度的拟合了两个样本，所以是过拟合，这就是高偏差和高方差的例子</span></p><p><span>使用直线函数去拟合数据往往是高偏差（欠拟合）</span></p><p><span>而采用曲线函数或二次元函数会产生高方差（过拟合）</span></p></li></ul></li><li><p><span>那么面对高偏差 高方差的情况我们应该怎么办？下节课。</span></p></li></ul><h2 id='13-机器学习基础basic-recipe-for-machine-learning）'><span>1.3 机器学习基础（Basic Recipe for Machine Learning）</span></h2><ul><li><p><span>训练神经网络的大概步骤</span></p><ul><li><p><span>初始模型训练完</span></p></li><li><p><span>测试初始模型的偏差</span></p><ul><li><p><span>如果偏差较高，试着评估训练集或训练数据的性能</span></p></li><li><p><span>如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络。比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法，后面我们会讲到这部分内容。</span></p></li><li><p><span>提示</span></p><ul><li><span>多去尝试不同的神经网络架构或许你能找到一个更合适解决此问题的新 的网络架构</span></li><li><span>一般来说更大的神经网络通常更有帮助，网络足够大，通常至少可以拟合或者过拟合训练集。</span></li><li><span>延长训练时间不一定有用，但一定没坏处</span></li></ul></li><li><h3 id='训练学-习算法时反复使用上面的方法直到解决掉偏差问题直到可以拟合数据为止至少能够拟合训练集'><span>训练学 习算法时，反复使用上面的方法直到解决掉偏差问题，直到可以拟合数据为止，至少能够拟合训练集</span></h3></li></ul></li><li><p><span>一旦偏差可以接受，测试初始模型的方差</span></p><ul><li><span>如果方差高，最好的办法就是采用更多数据</span></li><li><span>如果没有更多数据，那么可以通过正则化来减少过拟合</span></li></ul></li><li><h3 id='总结就是一句话反复尝试直到找到一个低偏差低方差的框架'><span>总结就是一句话，反复尝试，直到找到一个低偏差，低方差的框架。</span></h3></li></ul></li><li><p><span>注意：</span></p><ul><li><p><strong><span>高方差和高偏差是两种不同的情况，需要对症下药</span></strong><span>。例如更多的数据可以解决高方差的问题，但是不能解决高偏差。</span></p></li><li><p><span>以前在机器学习的初级接阶段，由于工具和理论没有很成熟，关于所谓的偏差方差权衡的讨论屡见不鲜，也就是在</span><strong><span>只能增加的一方同时减少另一方的值/反之亦同</span></strong><span>，到底应该如何选择，是增加方差还是增加偏差。</span></p><p><span>等到现在，调整偏差和方差的工具来了，</span><strong><span>我们可以只增加/减少其中一个的基础上，对另一个不产生影响</span></strong><span>。只要</span><strong><span>正则适度</span></strong><span>，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。这两步实际要做的工作是：</span><strong><span>训练网络，选择网络</span></strong><span>或者</span><strong><span>准备更多数据</span></strong><span>。我觉得这就是深度学习对监督式学习大有裨益的一个重要原因，也是我们不用太过关注如何平衡偏差和方差的 一个重要原因</span></p><p><span>最终，我们会得 到一个非常规范化的网络。训练一个更大的网络几乎没有任何负面影响，而训练一个大型神经网络的主要代价也只是计算时间，前提是网络是比较规范化的</span></p></li><li><p><strong><span>正则化，另一种非常实用的减少方差的方法（第一种是准备更多数据）</span></strong><span>，正则化时会出现偏差方差权衡问题，</span><strong><span>偏差可能略有增加</span></strong><span>，如果网络足够大，增幅通常不会太高</span></p></li></ul></li></ul><h2 id='14-正则化regularization）'><span>1.4 正则化（Regularization）</span></h2><ul><li><p><span>出现过拟合的时候有两种办法解决，</span><strong><span>提供更多的数据，或者正则化</span></strong></p></li><li><p><span>L2正则化</span></p><p><span>例如我们在做逻辑回归，有参数W和偏置项b，我们只需要在逻辑回归的成本函数中加入正则化，也就是</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1035" cid="n1035" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="47.419ex" height="4.676ex" role="img" focusable="false" viewBox="0 -1370 20959.4 2067" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.577ex;"><defs><path id="MJX-5-TEX-I-1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path><path id="MJX-5-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-5-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-5-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-5-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-5-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-5-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-5-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-5-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path><path id="MJX-5-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"></path><path id="MJX-5-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-5-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-5-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-5-TEX-I-1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path><path id="MJX-5-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-5-TEX-N-7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path id="MJX-5-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D43D" xlink:href="#MJX-5-TEX-I-1D43D"></use></g><g data-mml-node="mo" transform="translate(633,0)"><use data-c="28" xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1022,0)"><use data-c="1D44A" xlink:href="#MJX-5-TEX-I-1D44A"></use></g><g data-mml-node="mo" transform="translate(2070,0)"><use data-c="2C" xlink:href="#MJX-5-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(2514.7,0)"><use data-c="1D44F" xlink:href="#MJX-5-TEX-I-1D44F"></use></g><g data-mml-node="mo" transform="translate(2943.7,0)"><use data-c="29" xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(3610.4,0)"><use data-c="3D" xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(4666.2,0)"><g data-mml-node="mn" transform="translate(409,676)"><use data-c="31" xlink:href="#MJX-5-TEX-N-31"></use></g><g data-mml-node="mi" transform="translate(220,-686)"><use data-c="1D45A" xlink:href="#MJX-5-TEX-I-1D45A"></use></g><rect width="1078" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(6206.4,0)"><use data-c="2217" xlink:href="#MJX-5-TEX-N-2217"></use></g><g data-mml-node="msubsup" transform="translate(6928.7,0)"><g data-mml-node="mo"><use data-c="2211" xlink:href="#MJX-5-TEX-LO-2211"></use></g><g data-mml-node="TeXAtom" transform="translate(1477,677.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D45A" xlink:href="#MJX-5-TEX-I-1D45A"></use></g></g><g data-mml-node="TeXAtom" transform="translate(1477,-485.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-5-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-5-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-5-TEX-N-31"></use></g></g></g><g data-mml-node="mi" transform="translate(9770,0)"><use data-c="1D43F" xlink:href="#MJX-5-TEX-I-1D43F"></use></g><g data-mml-node="mo" transform="translate(10451,0)"><use data-c="28" xlink:href="#MJX-5-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(10840,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-5-TEX-I-1D466"></use></g><g data-mml-node="TeXAtom" transform="translate(523,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">帽</text></g><g data-mml-node="mtext" transform="translate(852,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">子</text></g><g data-mml-node="mo" transform="translate(1705.3,0)"><use data-c="2C" xlink:href="#MJX-5-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(1983.3,0)"><use data-c="1D456" xlink:href="#MJX-5-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(13059.3,0)"><use data-c="2C" xlink:href="#MJX-5-TEX-N-2C"></use></g><g data-mml-node="msup" transform="translate(13504,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-5-TEX-I-1D466"></use></g><g data-mml-node="TeXAtom" transform="translate(523,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-5-TEX-I-1D456"></use></g></g></g><g data-mml-node="mo" transform="translate(14321,0)"><use data-c="29" xlink:href="#MJX-5-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(14932.2,0)"><use data-c="2B" xlink:href="#MJX-5-TEX-N-2B"></use></g><g data-mml-node="mfrac" transform="translate(15932.4,0)"><g data-mml-node="mi" transform="translate(617.5,676)"><use data-c="1D706" xlink:href="#MJX-5-TEX-I-1D706"></use></g><g data-mml-node="mrow" transform="translate(220,-686)"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-5-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(500,0)"><use data-c="1D45A" xlink:href="#MJX-5-TEX-I-1D45A"></use></g></g><rect width="1578" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(17972.6,0)"><use data-c="2217" xlink:href="#MJX-5-TEX-N-2217"></use></g><g data-mml-node="mo" transform="translate(18694.9,0) translate(0 -0.5)"><use data-c="7C" xlink:href="#MJX-5-TEX-N-7C"></use></g><g data-mml-node="mo" transform="translate(18972.9,0) translate(0 -0.5)"><use data-c="7C" xlink:href="#MJX-5-TEX-N-7C"></use></g><g data-mml-node="mi" transform="translate(19250.9,0)"><use data-c="1D464" xlink:href="#MJX-5-TEX-I-1D464"></use></g><g data-mml-node="mo" transform="translate(19966.9,0) translate(0 -0.5)"><use data-c="7C" xlink:href="#MJX-5-TEX-N-7C"></use></g><g data-mml-node="msubsup" transform="translate(20244.9,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><use data-c="7C" xlink:href="#MJX-5-TEX-N-7C"></use></g><g data-mml-node="TeXAtom" transform="translate(311,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-5-TEX-N-32"></use></g></g><g data-mml-node="TeXAtom" transform="translate(311,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-5-TEX-N-32"></use></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>J</mi><mo stretchy="false">(</mo><mi>W</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mo>∗</mo><msubsup><mo>∑</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow data-mjx-texclass="ORD"><mi>m</mi></mrow></msubsup><mi>L</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mtext>帽</mtext><mtext>子</mtext><mo>,</mo><mi>i</mi></mrow></msup><mo>,</mo><msup><mi>y</mi><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msup><mo stretchy="false">)</mo><mo>+</mo><mfrac><mi>λ</mi><mrow><mn>2</mn><mi>m</mi></mrow></mfrac><mo>∗</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mi>w</mi><mo data-mjx-texclass="ORD" stretchy="false">|</mo><msubsup><mo data-mjx-texclass="ORD" stretchy="false">|</mo><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow></msubsup></math></mjx-assistive-mml></mjx-container></div></div><p><span>此方法称为𝐿2正则化，乘以2纯粹是为了求导方便。</span></p><p><span>其中向量参数W的𝐿2范数的平方如图所示</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502223905003.png" referrerpolicy="no-referrer" alt="image-20230502223905003"></p></li><li><h3 id='为什么只用正则化参数w而不加上参数b呢'><span>为什么只用正则化参数W而不加上参数b呢？</span></h3><p><span>可以加上b，但是没必要。因为高方差问题是来自于对训练数据的过拟合，而过拟合又和参数挂钩，W是几乎涵盖了所有的参数，而b只是其中的一个参数，所以正则化b的影响并不大，可有可无。</span></p></li><li><p><span>L1正则化用的很少了，很多人用的都是L2正则化。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502231231990.png" referrerpolicy="no-referrer" alt="image-20230502231231990"></p></li><li><p><span>λ是正则化参数，它是一个超参数，然后需要使用验证集来反复配置，寻找最好的选择，最终可以让参数W变得很小。</span></p></li><li><p><span>为了方便写代码，在 Python 编程语言中，𝜆是一个保留字段，编写代码时，我们删掉𝑎，写成𝑙𝑎𝑚𝑏𝑑，以免与 Python 中的保留字段冲突。</span></p></li><li><h3 id='求和公式的具体参数'><span>求和公式的具体参数</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502233200602.png" referrerpolicy="no-referrer" alt="image-20230502233200602"></p><p><span>该矩阵范数被称作“</span><strong><span>弗罗贝尼乌斯范数</span></strong><span>”，用下标𝐹标注，鉴于线性代数中一些神秘晦涩的原因，我们不称之为“矩阵𝐿2范数”，而称它为“弗罗贝尼乌斯范数”，矩阵𝐿2范数听起来更自然，但鉴于一些大家无须知道的特殊原因，按照惯例，</span><strong><span>我们称之为“弗罗贝尼乌斯范数”， 它表示一个矩阵中所有元素的平方和。</span></strong></p></li><li><h3 id='该如何使用该范数实现梯度下降呢'><span>该如何使用该范数实现梯度下降呢？</span></h3><p><span>梯度下降原本为W = W - α* dW，我们只需要给dW加上 λ/m * W[ l ] 然后计算这个更新项即可</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502234250103.png" referrerpolicy="no-referrer" alt="image-20230502234250103"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230502234253520.png" referrerpolicy="no-referrer" alt="image-20230502234253520"></p><p><span>可以看到，𝑊[𝑙]的定义被更新为𝑊[𝑙]减去学习率 𝑎 乘以 backprop 再加上𝜆 𝑚 𝑊[𝑙]。</span></p><p><span>该正则项说明，不论𝑊[𝑙]是什么，我们都试图让它变得更小，实际上，</span><strong><span>相当于我们给矩 阵 W 乘以(1 − 𝑎 𝜆 / 𝑚 )倍的权重，该系数小于 1，因此𝐿2范数正则化也被称为“权重衰减”，</span></strong><span>因为它就像一般的梯度下降， </span><strong><span>𝑊被更新为少了𝑎乘以 backprop 输出的最初梯度值，同时𝑊也乘以了这个系数，这个系数小 于 1，因此𝐿2正则化也被称为“权重衰减”。</span></strong></p></li></ul><h2 id='15-为什么正则化可以减少过拟合'><span>1.5 为什么正则化可以减少过拟合？</span></h2><ul><li><p><span>为什么会过拟合？</span></p><p><span>因为数据量少，导致参数W太过于拟合训练集，这样导致验证集、测试集的数据结果不会太好。太过于拟合训练集，也就是让一个曲线的公式变得有很多很多的参数，变得过于复杂起来。</span></p><p><span>那解决问题的办法有两种</span></p><ul><li><span>丢掉一部分参数</span></li><li><span>削减一部分参数对整个算法的影响</span></li><li><span>L2正则化使用的就是第二种方法</span></li></ul></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503000918364.png" referrerpolicy="no-referrer" alt="image-20230503000918364"></p><p><span>这是一个例子，假设成本函数为</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503000946989.png" referrerpolicy="no-referrer" alt="image-20230503000946989"></p><p><span>那么在梯度下降中 </span></p><p><span>W^[ l ] = W - α * (dW^[ l ] + λ/m * W^[ l ])，整理</span></p><p><span>W^[ l ] = W^[ l ]- α * λ / m * W^[ l ]  - α * dW^[ l ]，整理 </span></p><p><span>W^[ l ] = (1 - α * λ / m) * W^[ l ]  - α * dW^[ l ] </span></p><p><span>可以看到，如果λ的值变大，那么W矩阵整个会变小，接近0。 这样就可以做到削减一部分参数对整个算法的影响，直观的理解就是将隐层的部分单元“抹去”（虽然它们还在，但是影响已经很小），以此来解决过拟合问题</span></p></li><li><p><span>这就是L2正则化，很常用，后面还有一种dropout正则化</span></p></li></ul><h2 id='16-dropout-正则化dropout-regularization）-随机失活'><span>1.6 dropout 正则化（Dropout Regularization） 随机失活</span></h2><ul><li><p><span>假设过拟合的神经网络结构如下</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503005304156.png" referrerpolicy="no-referrer" alt="image-20230503005304156"></p><ul><li><span>dropout正则化的方法就是对于每一个样本，都会有一个精简版的神经网络与之对应，再计算。</span></li><li><span>产生精简版神经网络的过程很简单，遍历每一个层，设置每一个结点消失的概率。假如每一层的每一个结点的消失概率就是抛硬币的概率 0.5，那抽到消失的节点 它的输入和输出连接都会消失，最终得到精简版的神经网络，之后进行反向传播进行训练</span></li></ul></li><li><h3 id='那如何实施drop呢'><span>那如何实施drop呢</span></h3><ul><li><h3 id='方法1反向随机失活inverted-dropout'><span>方法1，反向随机失活inverted dropout</span></h3><p><span>假设一个三层的网络，如果要对第三层进行反向随机失活</span></p><ol start='' ><li><p><span>定义一个向量d^[l]（python写为 dl），这里定义一个d3，它是第三层的dropout向量</span></p><p><span>实现如下：d3 = np.random.randn(a3.shape[0], a3.shape[1]) &lt; keep-prob</span></p><ul><li><span>keep-prob 是一个具体的数字， 它表示保留某个隐藏单元的概率。 上面的例子中是0.5，现在改为0.8，也就是80%的几率留着节点</span></li><li><span>d3 = np.random.randn(a3.shape[0], a3.shape[1]) &lt; keep_prob  这句话的作用是将矩阵d3中，小于 keep_prob的元素设置为False，不小于的设置为True</span></li><li><span>这句话最后的执行结果是，用随机数产生了一个和a3一样维度的矩阵，矩阵中的元素如果小于0.8，那就会设置为False，如果大于就设置为True。</span></li></ul></li><li><p><span>从第三层中获取激活函数a^[3]，a3 = np.multiply(a3, d3)，这里是用的元素相乘而不是点乘，用 a3 *= d3 效果一样</span></p><ul><li><span>让a3和d3 的同位置元素相乘， 保留d3中同位置结果为True的元素</span></li><li><span>python中实现的话，d3是一个布尔型数组，值为 true 和 false，而不是 1 和 0，乘法运算依然有效，python 会把 true 和 false 翻译为 1 和 0。 </span></li></ul></li><li><p><span>最后，我们向外扩展𝑎 [3]，用它除以 0.8，或者除以 keep-prob 参数。 𝑎3/= 𝑘𝑒𝑒𝑝 − 𝑝𝑟𝑜𝑏</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503012911770.png" referrerpolicy="no-referrer" alt="image-20230503012911770"></p></li></ol></li><li><h3 id='事实证明在测试阶段当我们评估一个神经网络时也就是用绿线框标注的反向随机失活方法使测试阶段变得更容易因为它的数据扩展问题变少'><span>事实证明，在测试阶段，当我们评估一个神经网络时，也就是用绿线框标注的反向随机失活方法，使测试阶段变得更容易，因为它的数据扩展问题变少。</span></h3></li><li><h3 id='目前实施-dropout-最常用的方法就是-inverted-dropout'><span>目前实施 dropout 最常用的方法就是 Inverted dropout</span></h3></li><li><p><span>测试阶段不用dropout函数</span></p></li></ul></li></ul><h2 id='17-理解dropout'><span>1.7 理解dropout</span></h2><ul><li><p><span>dropout的思路和L2正则化的思路大概一致，只是两种实现方法</span></p><ul><li><span>假设有一个节点，有4个特征输入。由于dropout机制的存在，四个输入都有可能被随机干掉，那对于这个节点来说，给与四个输入相同的一点点的权重就可以了，因此该单元通过这种方式传播下去，通过传播所有权重，dropout 将产生收缩权重的平方范数的效果，和之前讲的𝐿2正则化类似，不过𝐿2对不同权重的衰减是不同的，它取决于激活函数倍增的大小，而这个仅仅靠随机</span></li><li><span>实施 dropout 的结果是它会压缩权重，并完成一些预防过拟合的外层正则化</span></li></ul></li><li><p><span>不同层的keepprob值可以不一样</span></p><ul><li><strong><span>一般来说容易出现过拟合的层且含有诸多参数的层</span></strong><span>，keepprob的值设置的比较小 比如0.5</span></li><li><strong><span>而其他不太容易出现过拟合的层</span></strong><span> 可以设置0.7</span></li><li><strong><span>不用担心过拟合的层</span></strong><span>可以设置为1</span></li></ul></li><li><p><span>dropout不是所有地方都使用，它主要是用于计算机视觉，因为计算机视觉的输入量非常大，输入太多像素，以至于没有足够的数据，所以 dropout 在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的 选择</span></p></li><li><p><span>但在其他应用中除非算法过拟合，不然是不会使用dropout 的</span></p></li><li><p><span>dropout的缺点就是代价函数J并不能明确定义了，我们就不能绘制这样的辅助图片来查看梯度下降效果</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503022844397.png" referrerpolicy="no-referrer" alt="image-20230503022844397"></p></li><li><p><span>所以通常会先关闭 dropout 函数，将 keepprob 的值设为 1，运行代码，确保𝐽函数单调递减。然后打开 dropout 函数，希望在 dropout 过程中，代码并未引入 bug。</span></p></li></ul><h2 id='18-其他正则化方法other-regularization-methods）'><span>1.8 其他正则化方法（Other regularization methods）</span></h2><ul><li><p><span>除了L2和dropout还有一些常用的正则化方法</span></p></li><li><h2 id='数据扩增'><span>数据扩增</span></h2><ul><li><p><span>如果做图片识别有关的问题，可以把图片水平翻转、或者还是随意裁剪，但要保证主体依旧存在。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503133147088.png" referrerpolicy="no-referrer" alt="image-20230503133147088"></p></li><li><p><span>这种方法是在给训练集增加假的训练数据，增加的数据稍微有些冗余（因为大体相同）并且无法像新的训练数据那样包含那么多的信息，但是和收集一组新的数据相比，这个很方便。</span></p></li><li><p><span>在做字体图片的数据扩增时候，不用变形的这么严重，只需要一些轻微的变形（旋转、扭曲）就可以了</span></p></li></ul></li><li><h2 id='early-stopping'><span>early stopping</span></h2><ul><li><p><span>在梯度下降中，我们可以绘制一个图，横轴代表迭代次数，然后你可以绘制两条曲线，一条是成本函数J，另一条是验证集误差</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503141503997.png" referrerpolicy="no-referrer" alt="image-20230503141503997"></p></li><li><p><span>你会发现，验证集先是下降，然后在某一点之后开始上升，开始上升就意味着可能出现了过拟合的情况，那early stopping的作用就是在过拟合之前，终止梯度下降的迭代次数。</span></p></li><li><p><span>再详细一点，在迭代的初期，参数矩阵W的值还很小，因为初始化的时候W的值都很小。随着梯度下降次数的增多，W开始拟合训练数据，它就变得比较大。当迭代次数太多的时候，W就会过拟合训练数据，变得特别大。</span></p><p><span>因此early stopping就会在W还没那么大的时候终止梯度下降迭代，我们得到一个𝑤值中等大小的弗罗贝尼乌斯范数，与𝐿2正则化相似，选择参数𝑤范数较小的神经网络，但愿你的神经网络过度拟合不严重。</span></p></li><li><h3 id='early-stopping的缺点'><span>early stopping的缺点</span></h3><ul><li><span>在做神经网络的过程中，包含了两个必要步骤：</span><strong><span>优化成本函数J以及预防过拟合</span></strong></li><li><span>两个步骤都有对应的工具来解决，比如优化成本函数J就有梯度下降 Adam 等等，预防过拟合有正则化，扩增数据等等</span></li><li><span>优化成本函数的时候，我只需要关注W和b，以及成本J，让它们最小就好</span></li><li><span>预防过拟合的时候同时还有一个任务是减小方差，这一点我们将来使用正交化来实现</span></li><li><span>我们看到，上面的两个问题有两个工具来解决</span></li><li><span>但early stopping的出现相当于终止了梯度下降迭代，停止了迭代一方面是停止了优化成本函数，让J还不是最小值，同时迭代停止导致提前停止了预防过拟合。 </span></li><li><strong><span>相当于一个办法解决了两个问题，相当于不能独立地处理这两个问题，但是解决的都并不是那么好</span></strong><span>，同时会加大你处理问题的难度，比如何时停止梯度下降、如何避免过拟合等等</span></li></ul></li><li><h3 id='early-stopping的优点'><span>early stopping的优点</span></h3><ul><li><span>相较于L2正则化需要你不断的尝试λ的值，来获得最好的效果所产生的的计算代价</span></li><li><span>early stopping只需要一次梯度下降，你就可以找出𝑤的较小值，中间值和较大值，而无需尝试𝐿2正则化超级参数𝜆的很多值</span></li></ul></li><li><h3 id='early-stopping的应用场景'><span>early stopping的应用场景</span></h3><ul><li><h3 id='虽然它只需要运行一次梯度下降就可以找到较好的w值但是这期间你需要花费额外的成本去画图去发现那个w的值是比较好的这样无疑给计算性能带来了更多的挑战所以当你的计算性能较好的时候就可以使用early-stopping效果和l2正则化差不多'><strong><span>虽然它只需要运行一次梯度下降就可以找到较好的w值，但是这期间你需要花费额外的成本去画图，去发现那个w的值是比较好的，这样无疑给计算性能带来了更多的挑战，所以当你的计算性能较好的时候，就可以使用early stopping，效果和L2正则化差不多</span></strong></h3></li></ul></li></ul></li></ul><h2 id='19-归一化输入normalizing-inputs）'><span>1.9 归一化输入（Normalizing inputs）</span></h2><ul><li><h3 id='归一化加快神经网络训练的一个方法'><span>归一化加快神经网络训练的一个方法</span></h3></li><li><h3 id='做神经网络可以默认使用归一化处理一下数据归一化后的数据会让优化成本函数的过程变快'><span>做神经网络可以默认使用归一化处理一下数据，归一化后的数据会让优化成本函数的过程变快</span></h3></li><li><p><span>归一化分为两步：零均值，归一化方差</span></p><ol start='' ><li><h3 id='零均值'><span>零均值</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503150030784.png" referrerpolicy="no-referrer" alt="image-20230503150030784"></p><ul><li><span>零均值就是通过求特征向量X的平均数，让每一个特征都减去这个平均数，从而让特征映射在一定范围内，这也叫中心化</span></li></ul></li><li><h3 id='归一化方差'><span>归一化方差</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503150424608.png" referrerpolicy="no-referrer" alt="image-20230503150424608"></p></li></ol><ul><li><h3 id='注意'><span>注意</span></h3><ul><li><h3 id='如果训练集归一化了那验证集和测试集也要用相同的μ和σ来归一化'><span>如果训练集归一化了，那验证集和测试集也要用相同的μ和σ来归一化</span></h3></li></ul></li></ul></li><li><p><span>为什么归一化有用</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503152242308.png" referrerpolicy="no-referrer" alt="image-20230503152242308"></p><p><span>因为x1和x2的值域很不一样， 比如x1 是1 - 1000，x2 是 0 - 1 ，那么他们的参数W也会很不一样，特征值越大的，参数项就会越小，最后导致w1和w2 （图上的数轴应该标记成w1 w2的）也有很大的差别，得到的图也变成一个面包一样曲折，那你后期的learning rate就得设置的很小，怕步子太大，梯度下降的过头了，最终就会导致梯度下降很慢。</span></p><p><span>如果归一化了就没这些问题，梯度下降会比较快。</span></p></li></ul><h2 id='110-梯度消失梯度爆炸vanishing--exploding-gradients）'><span>1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）</span></h2><ul><li><p><span>该问题主要集中在神经网络，尤其是深度神经网络中</span></p></li><li><p><span>这个问题是以前阻碍深度学习发展的问题</span></p></li><li><h3 id='问题就在于我们选择的参数w经过多层计算是否最终会导致y的值呈爆炸式的增长或者是减小'><span>问题就在于我们选择的参数W，经过多层计算是否最终会导致y的值呈爆炸式的增长或者是减小</span></h3></li><li><p><span>如果有一个10层的神经网络，假设使用线性激活函数 g(z) = z，并且 b为0</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503161953166.png" referrerpolicy="no-referrer" alt="image-20230503161953166"></p><ul><li><p><span>最终的y(i) = W[9] * ... W[2] * W[1] * x(i) = x(i) * W ** L - 1</span></p></li><li><p><span>如果设置W为一个对角矩阵，W[L]先不管 </span>
<span>1.5    0 </span>
<span>0       1.5</span></p><p><span>这里用到了矩阵的计算，因为是对角矩阵，所以可以提出系数 1.5  让它变成单位矩阵，那最后W = 1.5</span></p><p><span>那最终y = x(i) * 1.5 ** L - 1，也就是x会以 1.5^L的速度增长</span></p><p><strong><span>y呈爆炸上涨</span></strong><span>，这就是梯度爆炸</span></p></li><li><p><span>反之，如果W是一个 0.5的对角矩阵，W = 0.5，那最后 y会越来越小，x会以 0.5 ^L的速度下降，这就是梯度消失</span></p></li></ul></li><li><p><span>这里只是讨论了激活函数以与𝐿相关的指数级数增长或下降，但它也适用于与层数𝐿相关的导数或梯度函数，也是呈指数级增长或 呈指数递减</span></p></li><li><p><span>如果激活函数或梯度函数以与𝐿相关的指数增长或递减，它们的值将会变得极大或极小，从而导致训练难度上升，尤其是梯度指数小于𝐿时，梯度下降算法的步长会非常非常小，梯度下降算法将花费很长时间来学习</span></p></li></ul><h2 id='111-神经网络的权重初始化weight-initialization-for-deep--networks）'><span>1.11 神经网络的权重初始化（Weight Initialization for Deep  Networks）</span></h2><ul><li><p><span>为了解决上述的问题，权重的随机初始化就尤为重要</span></p></li><li><p><span>Xavier Initialization</span></p><ul><li><span>W^[ l ] = np.random.randn(shape) * np.sqrt(1 / n^[ l - 1])</span></li><li><span>适用于sigmoid和tanh函数</span></li></ul></li><li><p><span>He Initialization</span></p><ul><li><span>W^[ l ] = np.random.randn(shape) * np.sqrt(2 / n^[ l - 1])</span></li><li><span>适用于RelU和带泄露的ReLU函数</span></li></ul></li><li><p><span>原因： z = w1 * x1 + w2 * x2 ... + wn * xn，为了预防𝑧值过大或过小，所以当n大的时候，w变小，那么最合理的方法就是 wi = 1 / n ，因此有了上面的式子，</span></p></li><li><p><span>*</span><span> np.sqrt(1 / n^[ l - 1]) 和 </span><span>*</span><span>np.sqrt(2 / n^[ l - 1])是为了保证每一层的输出方差尽可能相等，从而避免梯度消失和梯度爆炸问题</span></p></li></ul><h2 id='112-梯度的数值逼近numerical-approximation-of-gradients）'><span>1.12 梯度的数值逼近（Numerical approximation of gradients）</span></h2><ul><li><h3 id='在实施-backprop-时有一个测试叫做梯度检验它的作用是确保-backprop-正确实施'><span>在实施 backprop 时，有一个测试叫做梯度检验，它的作用是确保 backprop 正确实施。</span></h3></li><li><h3 id='为了逐渐实现梯度检验我们首先说说如何计算梯度的数值逼近'><span>为了逐渐实现梯度检验，我们首先说说如何计算梯度的数值逼近</span></h3></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503193945316.png" referrerpolicy="no-referrer" alt="image-20230503193945316"></p><h2 id='113-梯度检验gradient-checking）'><span>1.13 梯度检验（Gradient checking）</span></h2><ul><li><p><span>梯度检验帮我们节省了很多时间，也多次帮我发现 backprop 实施过程中的 bug</span></p></li><li><p><span>步骤</span></p><ol start='' ><li><p><span>把所有的参数W1 b1 W2 b2 W3 b3..WL bL 分别转换为一个个向量数据，将这些向量数据做连接运算后得到巨大向量θ。这样就得到了关于θ的成本函数J（θ）</span></p></li><li><p><span>把dW1 db1 dW2 db2 ...分别转换为一个个向量数据，将这些向量数据做连接运算后得到巨大向量dθ。  dW1和W1，db1和b1都有相同的维度，dθ和θ也是相同的维度</span></p></li><li><p><span>实施梯度检验grad check</span></p><p><span>计算数值逼近</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503202155996.png" referrerpolicy="no-referrer" alt="image-20230503202155996"><span>	</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503202812510.png" referrerpolicy="no-referrer" alt="image-20230503202537780"><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503202622193.png" referrerpolicy="no-referrer" alt="image-20230503202622193"><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503202700220.png" referrerpolicy="no-referrer" alt="image-20230503202700220"><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503202714789.png" referrerpolicy="no-referrer" alt="image-20230503202714789"></p></li></ol></li></ul><h2 id='114-梯-度-检-验-应-用-的-注-意-事-项--gradient-checking--implementation-notes）'><span>1.14 梯 度 检 验 应 用 的 注 意 事 项 （ Gradient Checking  Implementation Notes）</span></h2><ol start='' ><li><span>训练中不要用它，只有出现bug了 debug再用它，因为它算的特别慢</span></li><li><span>如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出 bug，也 就是说，如果𝑑𝜃approx[𝑖]与𝑑𝜃[𝑖]的值相差很大，我们要做的就是查找不同的𝑖值，看看是哪个 导致𝑑𝜃approx[𝑖]与𝑑𝜃[𝑖]的值相差这么多。 如果检查出来某一层的相差值很大，dW[l]的值都很接近，但是db[l]的值就相差甚远，那这时候问题就出在db这里了</span></li><li><span>在实施梯度检验时，如果使用正则化，后面的代价函数中也要带着正则项</span></li><li><span>dropout和梯度检验不能一起用</span></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503203916031.png" referrerpolicy="no-referrer" alt="image-20230503203916031"></li></ol><h1 id='第二周优化算法-optimization-algorithms'><span>第二周：优化算法 (Optimization algorithms)  </span></h1><ul><li><span>这能让你的神经网络运行速度加快，帮你快速训练模型</span></li></ul><h2 id='21-mini-batch-梯度下降mini-batch-gradient-descent）'><span>2.1 Mini-batch 梯度下降（Mini-batch gradient descent）</span></h2><ul><li><p><span>问题：向量化的出现让你可以不用特别多公式，就可以更快的一起处理m个样本。但如果m个样本数量太大，处理的速度依旧缓慢，如果m的数量为500万那你每运行一次梯度下降前，就得处理一遍这500万个样本（前向传播），如果有1000次epoch，那这个过程慢的要死，怎么办？</span><strong><span>那就分割训练，先分割出来一些子集做梯度下降，然后在梯度下降的同时再分割子集，如此往复，这就是mini batch的思想。</span></strong></p></li><li><h3 id='如何划分的mini-batch'><span>如何划分的mini batch</span></h3><ul><li><p><span>首先你可以把训练集分割成小一些的子训练集，这些子训练集集取名为mini-batch。假设每个子训练集1000个样本，把x^(1) ~ x^(1000)取出来扔到第一个子集X^{1}中，接下来再取1000个样本 x^(1001) ~ x^(2000)扔到X^{2}中，一直到X^{5000}，也就是说对于500万个数据，每个mini batch的样本数是1000，就得有5000个mini batch。</span></p><ul><li><span>注意这个符号 X^{1} 就代表第一个mini batch</span></li></ul></li><li><p><span>Y也是同理，分成5000个mini-batch</span></p></li><li><p><span>所以X^{ l } , Y^{ l }就是一对对应的mini batch。 X^[ l ]的维数就是（n_x, 1000)    Y^[ l ]的维数就是（1, 1000)</span></p></li></ul></li><li><h3 id='mini-batch名字的由来'><span>mini-batch名字的由来</span></h3><p><span>batch梯度下降法指的是我们之前使用的梯度下降法，它会先同时处理所有的训练集。这个算法名字的来源就是可以同时看到所有的训练集的样本被处理</span></p><p><span>mini-batch梯度下降法就是我们刚才提出的想法，你每次同时单独处理一个 mini-batch X^{l}，Y^{l}，而不是全部的X和Y训练集</span></p></li><li><h3 id='过程'><span>过程</span></h3><ul><li><span>可以这样理解，batch是一次性处理500万条数据，做一次前向、反向、梯度下降</span></li><li><span>而mini-bath是在每一个子集上做一次batch，在每一个子集上都会有前向、反向、梯度下降</span></li><li><span>1 epoch 叫一代，指遍历了一次训练集</span></li><li><span>batch做一代训练只能有一次梯度下降，而minibatch做一代训练有5000次梯度下降</span></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503213904075.png" referrerpolicy="no-referrer" alt="image-20230503213904075"></li></ul></li><li><p><span>正常来说训练神经网络会多次遍历训练集，那就需要在for外面再加一个while，一直循环遍历训练集，直到收敛到一个合适的精度</span></p></li><li><h2 id='训练大数据集的时候基本都会用mini-batch'><span>训练大数据集的时候基本都会用mini batch</span></h2></li></ul><h2 id='22-理解-mini-batch-梯度下降法understanding-mini-batch--gradient-descent）'><span>2.2 理解 mini-batch 梯度下降法（Understanding mini-batch  gradient descent）</span></h2><ul><li><p><span>batch和mini-batch的成本图</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503214130935.png" referrerpolicy="no-referrer" alt="image-20230503214130935"></p><ul><li><span>左面是batch梯度下降的 迭代次数和成本关系图，右面是mini-batch的迭代次数和成本关系图，如果这个J没有随着迭代次数而减少，反而还增加，那它有问题</span></li><li><span>注意这里指的迭代是迭代X^{t} Y^{t}，可以看到会有很多的噪声，原因是可能X1 Y1好算，X2 Y2没那么好算，但只要总体向下即可。</span></li></ul></li><li><p><span>mini-batch的超参数——mini-batch的大小</span></p><ul><li><p><span>极端情况，如果 mini-batch = m ，也就是子集的大小等于训练集样本数量，那这就是一个普通的batch。 (X^{1}, Y^{1}) = (X,Y)</span></p></li><li><p><span>另一个极端情况，如果mini-batch =  1，也就是子集的大小等于1，这就产生了一个新的算法——随机梯度下降法，(X^{1}, Y^{1}) = (x^(1),y^(1))</span></p><p><span>那第一个mini-batch的训练样本就是训练集中的第一对样本，以此类推</span></p></li><li><h3 id='两种极端情况的成本函数优化情况'><span>两种极端情况的成本函数优化情况</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503215213646.png" referrerpolicy="no-referrer" alt="image-20230503215213646"></p><ul><li><span>蓝色是batch梯度下降，可以看到，蓝色会逐渐靠近成本函数最小值，最终收敛</span></li><li><span>紫色是mini-batch梯度下降，它会有时候会靠近最小值点，有时候会远离，但总体都是在靠近最小值点的。和batch不一样，它最后会在最小值周围转圈，而不会收敛。</span></li></ul></li><li><h3 id='也就是说mini-batch的值不能太大或者太小也就是必须在-1--m之间'><span>也就是说mini-batch的值不能太大或者太小，也就是必须在 1 &lt; m之间。</span></h3><ul><li><p><span>因为如果太大，那mini-batch = m 时和，它和batch一样了，训练一代的速度很慢，</span><strong><span>不过小样本batch运行的很好</span></strong></p></li><li><p><span>如果太小mini-batch = 1 时，或者说采用了随机梯度下降法，这样会失去所有的向量化的加速。</span></p><ul><li><h3 id='所以最好最好是选择-1--m之间的中间值mini-batch既可以使用向量化加速又可以一边训练mini-batch一边做后续工作'><span>所以最好最好是选择 1 &lt; m之间的中间值，mini-batch既可以使用向量化加速，又可以一边训练mini-batch一边做后续工作</span></h3></li></ul></li><li><h3 id='minibatch的尺寸如果选择的比较好那它会比随机梯度下降法更快的逼近最小值绿色的线）它也不一定在很小的范围内收敛或者波动如果出现了这些就通过减小学习率噪声会被改善或有所减小下节课会讲'><span>minibatch的尺寸如果选择的比较好，那它会比随机梯度下降法更快的逼近最小值（绿色的线），</span><strong><span>它也不一定在很小的范围内收敛或者波动，如果出现了这些就通过减小学习率，噪声会被改善或有所减小，下节课会讲</span></strong></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230503215640319.png" referrerpolicy="no-referrer" alt="image-20230503215640319"></p></li></ul></li><li><h3 id='那mini-batch的大小选择多少比较好'><span>那mini-batch的大小选择多少比较好？</span></h3><ul><li><p><span>如果样本数 &lt; 2000，直接用batch梯度下降</span></p></li><li><p><span>如果样本数 &gt; 2000，那么一般 mini-batch的值是 64 ~ 512</span></p></li><li><h2 id='注意mini-batch的值务必要是2的次方比如-64-128-256-512-1024少见'><span>注意：mini-batch的值务必要是2的次方，比如 64 128 256 512 1024(少见)</span></h2><h2 id='你可以多尝试几个数看看那个数跑的梯度下降最好最快'><span>你可以多尝试几个数，看看那个数跑的梯度下降最好最快。</span></h2></li></ul></li></ul></li></ul><h2 id='23-指数加权平均数exponentially-weighted-averages）'><span>2.3 指数加权平均数（Exponentially weighted averages）</span></h2><ul><li><p><span>有几种比mini batch还要快的优化算法，但是你得先掌握指数加权平均，在统计中也叫做</span><strong><span>指数加权移动平均</span></strong></p></li><li><p><span>计算方法比较简单</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504162354184.png" referrerpolicy="no-referrer" alt="image-20230504162354184"><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504162417230.png" referrerpolicy="no-referrer" alt="image-20230504162417230"></p></li><li><p><span>计算v_t的时候，可以当成它实际上是在计算前 1 / (1 - β)的平均温度</span></p><ul><li><p><span>如果 β = 0.9 ，那他计算的是前10天的平均温度  也就是说 如果 v_12 = β * θ_12 + (1 - β) * v_11，那么v12大概是 12号 - 2号的平均温度</span></p></li><li><p><span>β = 0.5， 那他计算的是前2天的平均温度， v_12 = 112号11号的平均温度</span></p></li><li><p><span>β = 0.98，那他计算的是前50天的平均温度</span></p></li><li><h3 id='为什么看下一节课'><span>为什么？看下一节课</span></h3></li></ul></li><li><h3 id='β的高低对最后数据带来的影响'><span>β的高低对最后数据带来的影响</span></h3><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504164320147.png" referrerpolicy="no-referrer" alt="image-20230504164320147"></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504164326007.png" referrerpolicy="no-referrer" alt="image-20230504164326007"></p></li><li><p><span>红色是 β = 0.9，平均10天的每日温度</span></p><p><span>绿色是 β = 0.98，平均50天的每日温度</span></p><p><span>黄色是 β = 0.5，平均2天的每日温度</span></p></li><li><p><span>可以看出，β越大，那数据越平缓（因为计算的天数比较多），数据的延迟性较大（当日的温度对整体的影响会很迟才会出现）</span></p></li><li><p><span>β越大，那数据越抖（因为计算的天数比较少），数据的延迟性较小（当日的温度对整体的影响马上会出现）</span></p></li><li><p><span>所以β的值也是在中间比较好，不要过大或者过小，它很重要</span></p></li></ul></li></ul><h2 id='24-理解指数加权平均数-understanding-exponentially--weighted-averages）'><span>2.4 理解指数加权平均数（ Understanding exponentially  weighted averages）</span></h2><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504164926893.png" referrerpolicy="no-referrer" alt="image-20230504164926893"></p></li><li><p><span>这节课就是讲出 β的值是如何影响到最后的结果的（也就是为什么β 0.9，就是计算平均10天的温度， 0.5就是 计算平均2天的温度）</span></p></li><li><p><span>假设我们让β = 0.9，计算v100的温度</span></p><p><span>有</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504165115767.png" referrerpolicy="no-referrer" alt="image-20230504165115767"></p><p><span>如果把v100的式子展开，把v99替换为 v99的式子，v98替换为v98的式子，以此类推，我们可以获得一个公式</span></p><p><span>即 v_100 = 0.1 * θ_100 + 0.1 * 0.9 * θ</span><span>_</span><span>99 + 0.1 * (0.9)^2 * θ</span><span>_</span><span>98 + 0.1 * (0.9)^3 * 0</span><span>_</span><span>97 ...</span></p><p><strong><span>也就是说，距离今天越远的温度，它的权重越小</span></strong><span>（因为0.9^x是一个单调递减函数）</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504165503414.png" referrerpolicy="no-referrer" alt="image-20230504165503414"></p><ul><li><p><span>如果我们画出两张图</span></p><p><span>第一张图横轴为天数，竖轴为对应当天的温度</span>
<span>第二张图横轴为v_1 ~ v_100，也就是某一天的平均温度，竖轴为当天温度的权重，我们就有这两张图</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504165704216.png" referrerpolicy="no-referrer" alt="image-20230504165704216"></p><ul><li><span>而v_100的值，就是两张图的同一天温度 * 同一天的权重 的和，也可见，越老的数据，权重越小</span></li><li><strong><span>所有权重的和 &lt;= 1，我们称之为偏差修正</span></strong></li></ul></li></ul></li><li><p><span>最后问题来了，那知道了这些 怎么知道是平均多少天的温度的？</span></p><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504170059299.png" referrerpolicy="no-referrer" alt="image-20230504170059299"></li></ul></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504170133597.png" referrerpolicy="no-referrer" alt="image-20230504170133597"></p><ul><li><p><span>最后就是如何实现指数加权平均数了</span></p></li><li><p><span>基本思路就是循环累加和</span></p><p><span>v = 0</span>
<span>β = 0.9</span></p><p><span>for t in range(X.shape[1]) #有多少天循环多少次</span></p><p><span>    v += β * X[i] + (1 - β) * v</span></p></li></ul><h2 id='25-指-数-加-权-平-均-的-偏-差-修-正--bias-correction-in--exponentially-weighted-averages）'><span>2.5 指 数 加 权 平 均 的 偏 差 修 正 （ Bias correction in  exponentially weighted averages）</span></h2><ul><li><p><span>偏差修正可以让平均值算的更加精准</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504170936409.png" referrerpolicy="no-referrer" alt="image-20230504170936409"></p></li><li><p><span>图中的绿色和紫色分别代表了当β = 0.98时，带偏差修正（绿色）和不带偏差修正的曲线结果</span></p></li><li><h3 id='可以看出前期计算指数加权平均时出现了错误'><span>可以看出，前期计算指数加权平均时，出现了错误</span></h3><ul><li><p><span>原因在于开始的时候，我们把 v0  = 0 ，那么v1 = 0.98 * v0 + 0.02 * θ_1，此时 v0就是0，那v1 = 0.02*θ</span><span>_</span><span>1，所以是一个非常小的值（对应紫色的开头）</span></p></li><li><p><span>也就是说刚开始的时候，v_t是从0开始的，后面才慢慢上来的，也就是说，v_t是从一个错误的平均数据开始计算的，所以前期会有问题</span></p></li><li><p><span>那怎么解决呢？ 使用偏差修正</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504171321897.png" referrerpolicy="no-referrer" alt="image-20230504171321897"></p><ul><li><span>这样就可以解决前期计算错误的问题</span></li><li><span>并且当t越来越大的时候， β^t趋向于0，也就是说，偏差修正对于后期的影响几乎没有，所以紫色的线和绿色的线重叠在了一起。</span></li></ul></li></ul></li></ul><h2 id='26-动量梯度下降法gradient-descent-with-momentum）'><span>2.6 动量梯度下降法（Gradient descent with Momentum）</span></h2><ul><li><p><span>这个就是Momentum算法，动量梯度下降法，运行速度总是快于标准的梯度下降法</span></p></li><li><p><span>基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新你的权重</span></p><ul><li><p><span>例如在这个成本函数中我要搞梯度下降，那么minibathc和batch的表现会像是这样</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504172554823.png" referrerpolicy="no-referrer" alt="image-20230504172554823"></p><p><span>会很抖动，然后慢慢的靠近中间（紫色是学习率调大的效果，可以无视），这种上下抖动无疑是梯度下降速度的大阻碍。</span></p><p><span>就像是要去中心点，曲折的线路会更费时间一样</span></p></li><li><p><span>所以为了加快去中点的速度，我就想让纵轴学习的慢一点，抖动幅度别那么大，并且想让横轴上加快学习，更快的移动到红点，那么动量梯度下降法就是好选择</span></p></li></ul></li><li><p><span>你要怎么做呢？</span></p><ul><li><p><span>在每次迭代 t 中：</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504173617297.png" referrerpolicy="no-referrer" alt="image-20230504173617297"></p></li><li><p><span>也就是dW和db每次的移动平均数，然后做参数更新，这样就可以减缓梯度下降的摆动幅度</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504174103597.png" referrerpolicy="no-referrer" alt="image-20230504174103597"></p></li><li><p><span>vdw和vdb的初始值都是0，是一个维数和W dW  / b db相同的矩阵</span></p></li><li><p><span>这样就有两个超参数 α 还有 β，β一般用0.9，不过需要自己多尝试。</span></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504173752928.png" referrerpolicy="no-referrer" alt="image-20230504173752928"></p></li></ul><h2 id='27-rmsprop'><span>2.7 RMSprop</span></h2><ul><li><p><span>除了Momentum可以加快梯度下降，还有一个叫RMSprop的算法（r oot mean square prop）也可以加快梯度下降</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504174629101.png" referrerpolicy="no-referrer" alt="image-20230504174629101"></p><ul><li><span>注意，平方是针对微分项的</span></li><li><span>为了防止 Sdw和Sdb为0，会给它们各自加上一个很小ε，ε为 10^-8</span></li><li><span>RESprop和Momentum一样，都可以消除摆动，不过RESprop允许使用更大的学习率</span></li></ul></li></ul><h2 id='28-adam-优化算法adam-optimization-algorithm'><span>2.8 Adam 优化算法(Adam optimization algorithm)</span></h2><ul><li><p><span>Adam = RESprop + Momentum</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504192833060.png" referrerpolicy="no-referrer" alt="image-20230504192833060"></p><ul><li><span>具体步骤也请看图</span></li><li><span>correct意思是使用了偏差修正</span></li></ul></li><li><p><span>它可以被用于很多神经网络结构中</span></p></li><li><p><span>超参数：</span></p><ul><li><span>学习率 α ，需要一直调试，之前说可以从0.001开始， 然后乘以三倍，再乘以三倍这样来试</span></li><li><span>β_1，Momentum的β，缺省值0.9</span></li><li><span>β_2，RESprop的β，缺省值0.999</span></li><li><span>ε，RESprop的，建议(10)^-8，不影响算法的表现</span></li><li><span>除了学习率之外，都不需要设置，使用默认值即可，β1 β2也可以你自己去试，只不过基本没人这么干</span></li></ul></li><li><p><span>为啥叫Adam算法？</span></p><ul><li><span>Adaptive Moment Estimation</span></li><li><span>β1用于计算微分dW，这叫 第一矩（the first moment）</span></li><li><span>𝛽2用来计算平方数的指数加权平均数dW^2，这叫第二矩（the second moment）</span></li></ul></li></ul><h2 id='29-学习率衰减learning-rate-decay'><span>2.9 学习率衰减(Learning rate decay)</span></h2><ul><li><p><span>加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减</span></p></li><li><p><span>方法一：</span></p><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504201019734.png" referrerpolicy="no-referrer" alt="image-20230504201019734"></li></ul></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504201025404.png" referrerpolicy="no-referrer" alt="image-20230504201025404"></p></li></ul><p><span>方法2：指数衰减</span></p><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504201057660.png" referrerpolicy="no-referrer" alt="image-20230504201057660"></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504201112140.png" referrerpolicy="no-referrer" alt="image-20230504201112140"></li></ul><p><span>方法3：离散下降</span></p><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230504201125633.png" referrerpolicy="no-referrer" alt="image-20230504201125633"></p></li><li><p><span>方法4：手动下降</span></p><ul><li><span>这个只对小模型有用</span></li></ul></li><li><h3 id='andrew的经验通常他会先把学习率设置为固定值先调整其他的超参数如果不行再来调整学习率'><span>andrew的经验：通常他会先把学习率设置为固定值，先调整其他的超参数，如果不行，再来调整学习率</span></h3></li></ul><h2 id='210-局部最优的问题the-problem-of-local-optima'><span>2.10 局部最优的问题(The problem of local optima)</span></h2><ul><li><span>局部最优问题来源于 低维度神经网络图像带来的错觉</span></li><li><span>在低纬度的参数成本图中，可以看到有很多的局部最优点，之前人们担心算法最终会下降到局部最优点，而不会下降到全局最优点</span></li><li><span>局部最优点就是该点处导数为0，并且该点的所有维度都是凸函数或者凹函数</span></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505105557078.png" referrerpolicy="no-referrer" alt="image-20230505105557078"></p><ul><li><p><span>但是后来人们发现，深度神经网络中的参数项非常非常多，也就是说深度神经网络的参数成本图不会长这样也没人知道长什么样。但是如果是一个高维度的空间，那么局部最优点就很难出现，因为假设有2万个参数，那么局部最优点就需要这两万个方向都是凹函数或者凸函数，这个概率太小了。 而高维度中更可能出现鞍点（一边是凹函数 一边是凸函数），而鞍点有着继续下降的空间，也就是说高维度的成本函数最终会收敛到一个最优点。</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505105900380.png" referrerpolicy="no-referrer" alt="image-20230505105900380"></p></li><li><h3 id='所以深度学习网络中并不存在局部最优问题'><span>所以深度学习网络中并不存在局部最优问题</span></h3><ul><li><span>并且由于鞍点附近的导数接近0，梯度下降速度很慢，由此也让我们的优化算法更能大展身手（adam RESprop Momentum）</span></li></ul></li></ul><h1 id='第三周-超-参-数-调-试--batch-正-则-化-和-程-序-框-架-hyperparameter-tuning）'><span>第三周 超 参 数 调 试 、 Batch 正 则 化 和 程 序 框 架 （Hyperparameter tuning） </span></h1><h2 id='31-调试处理tuning-process）'><span>3.1 调试处理（Tuning process）</span></h2><ul><li><p><span>神经网络的改变会涉及到许多不同超参 数的设置。现在，对于超参数而言，你要如何找到一套好的设定呢？</span></p></li><li><p><span>这节课来讲调参技巧</span></p></li><li><p><span>来看目前我们已经接触的超参数的调整优先级</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505122700122.png" referrerpolicy="no-referrer" alt="image-20230505122700122"></p><ul><li><span>红圈是学习率，是最重要的超参数</span></li><li><span>橙色是次要重要的超参数， momentum的β，隐层单元数，minbatch的大小</span></li><li><span>紫色是次次要超参数，隐层数，学习衰减率</span></li><li><span>adam的参数andrew不调</span></li></ul></li><li><h3 id='如何系统的调整超参数'><span>如何系统的调整超参数？</span></h3><ul><li><h3 id='规律选择点和随机选择点'><span>规律选择点和随机选择点</span></h3><ul><li><p><span>在早期的机器学习中，</span><strong><span>超参数比较少</span></strong><span>，我们会使用网格来系统的进行参数尝试（横轴竖轴是两个不同的超参数），一般使用5x5的网格，测试25个点就可以。</span></p><p><span>在测试过程中就可以发现：在这个应用中，到底超参1 还是超参2重要。</span><strong><span>当参数的数量相对较少时，这个方法很实用。</span></strong></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505123147497.png" referrerpolicy="no-referrer" alt="image-20230505123147497"></p></li><li><p><span>但是深度学习领域，更推荐第二种方法，也就是</span><strong><span>随机选择点</span></strong><span>，每个点是随机的，但是总数还是25个。因为搞深度学习的时候 你很难知道那个超参数更有用。</span></p></li><li><h3 id='两种方法的区别在于首先需要假设参数1是学习率参数2是adam中的ε'><span>两种方法的区别在于，首先需要假设参数1是学习率，参数2是adam中的ε。</span></h3><p><span>你使用系统选择点进行测试，虽然你有一共25种模型，但是你只能测试到5种不同的α点，最终测试出ε没啥用，这是规律选择点的问题所在，模型多，但是只能测试到一点点α。</span></p><p><span>而随机选择点，你就可以测试到不同的α和不同的ε，在测试的途中你不但可以发现ε没啥用，而且可以发现 α 变大还是变小会带来更好的结果。随机取值可以提升你的搜索效率</span></p></li></ul></li></ul></li><li><h3 id='二维是一个平面测试网格而三维三个参数）就是一个立方体测试网格'><span>二维是一个平面测试网格，而三维（三个参数）就是一个立方体测试网格</span></h3></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505124147587.png" referrerpolicy="no-referrer" alt="image-20230505124147587"></p></li><li><h3 id='由粗糙到精细'><span>由粗糙到精细</span></h3><p><span>在选择完测试点测试的过程中，你很有可能发现一块表现比较好的超参区域，比如图上画出来的小蓝色区域。</span></p><p><span>那么你要做的就是将这块区域放大，精细程度变高，然后着重测试这一块区域，再看这块区域中那个点更好。</span></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505124259491.png" referrerpolicy="no-referrer" alt="image-20230505124259491"></p><ul><li><h3 id='通过试验超参数的不同取值你可以选择对训练集目标而言的最优值或对于开发集而-言的最优值或在超参搜索过程中你最想优化的东西'><span>通过试验超参数的不同取值，你可以选择对训练集目标而言的最优值，或对于开发集而 言的最优值，或在超参搜索过程中你最想优化的东西。</span></h3></li></ul><h2 id='32-为超参数选择合适的范围using-an-appropriate-scale-to--pick-hyperparameters）'><span>3.2 为超参数选择合适的范围（Using an appropriate scale to  pick hyperparameters）</span></h2><ul><li><p><span>随机选择点并不是在任何可能的范围内做随机选择，而是在一个合理的标尺中做选择</span></p><ul><li><p><span>如果你要选择超参数  </span><strong><span>隐层单元数n^[l]</span></strong><span>，假设，你选取的取值范围是从 50 到 100 中某点，这种情况下，看到这条从 50-100 的数轴，你可以随机在其取点，这是一个搜索特定超参数的 很直观的方式。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505131829493.png" referrerpolicy="no-referrer" alt="image-20230505131829493"></p></li><li><p><span>如果你要选择</span><strong><span>隐层层数 L</span></strong><span>，你也许会选择层数为 2 到 4 中的某个值，接着顺着 2，3，4 随机均匀取样</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505131917556.png" referrerpolicy="no-referrer" alt="image-20230505131917556"></p><ul><li><span>这是在几个在你考虑范围内随机均匀取值的例子，这些取值还蛮合理的，但对某些超参数而言不适用</span></li></ul></li></ul></li><li><h3 id='学习率-α'><span>学习率 α</span></h3><ul><li><p><span>如果你要对α做选择，你可能觉得它的范围是从 0.0001 ~ 1，在这个范围内做随机选择</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505132058550.png" referrerpolicy="no-referrer" alt="image-20230505132058550"></p><p><span>所以你会画一条数轴，在这个数轴上均匀随机取值。但是这样就有一个问题，由于数量级的差异，最终90%的数，都会落在0.1 ~ 1的区间中，10%落在0.0001~0.1的区间中，这样的选择就并不均匀了，怎么办？</span></p><ul><li><h3 id='就不可以用线性数轴需要用对数标尺数轴）了'><span>就不可以用线性数轴，需要用对数标尺（数轴）了</span></h3></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505132227594.png" referrerpolicy="no-referrer" alt="image-20230505132227594"></p><p><strong><span>使用这个方法，那么0.001 - 1之间的所有区间，都可以进行均匀的取数，上面的问题迎刃而解了，在python中怎么做呢？</span></strong></p><p><span> r = -4 * np.random.rand()</span></p><p><span>α = 10 ** r，也就是 a 取 10的 r次方</span></p><p><span>为什么呢？  因为 np.random.rand函数输出 0 - 1 之间的小数， 那么 r就是 [-4, 0]之间的数了，那么 a的取值就是 0.0001 ~ 1之间了，这样的取值就均匀了，不存在数量级了。</span></p><ul><li><span>更常见的做法是，区间取 10^a ~ 10^b， r 取 [ a , b ]，在a b区间上随意取值，让a = 10^r即可</span></li></ul></li></ul></li><li><h3 id='最棘手的-β取值'><span>最棘手的 β取值</span></h3><ul><li><p><span>β是指数加权平均数，之前讲 β是一个 0.9 ~ 0.999之间的值，那如果用线性标尺来随机均匀采样的话，就会出现和上面一样的数量级差异问题，同时还会导致β的过大取值影响结果（因为β越大，相当于取的平均历史值越多）。所以我们取β的时候，不计算β本身，而是直接计算1 - β的值， 范围在0.001 ~ 0.1</span></p></li><li><p><span>同样了解决数量级的问题，我们要使用对数标尺</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505133057678.png" referrerpolicy="no-referrer" alt="image-20230505133057678"></p><p><span>数轴的上下限是 1-β的取值范围是 10^a ~ 10^b，</span><strong><span>图上画反了，不用担心</span></strong></p><p><span>r = [ -3, -1]</span></p><p><span>1-β = 10^r 即可</span></p></li></ul></li></ul><h2 id='33-超参数调试实践pandas-vs-caviarhyperparameters--tuning-in-practice-pandas-vs-caviar）'><span>3.3 超参数调试实践：Pandas VS Caviar（Hyperparameters  tuning in practice: Pandas vs. Caviar）</span></h2><ul><li><p><span>andrew讲了一个现象和两种调整模型超参数的方法</span></p></li><li><p><span>现象：</span><strong><span>不同领域的超参数或许可以通用，越来越多的人通过看不同领域的论文来找寻灵感</span></strong></p></li><li><p><span>Pandas训练方法和Caviar训练方法</span></p><ul><li><p><span>Pandas训练方法是一次训练一个模型，期间观察它成本函数的状态，然后再调整超参数</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505134049402.png" referrerpolicy="no-referrer" alt="image-20230505134049402"></p></li><li><p><span>Carviar训练方法是一次同时训练多个模型，观察它们的成本函数状态，找到表现最好的那一个，那就是你要的超参数。</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505134055835.png" referrerpolicy="no-referrer" alt="image-20230505134055835"></p></li><li><h3 id='两种训练方法仅仅是针对拥有多或者少量计算资源的场景'><span>两种训练方法仅仅是针对拥有多或者少量计算资源的场景</span></h3></li><li><p><span>如果计算资源少，只能跑得动一个模型，那就用熊猫训练（跑一个），然后自己来调整，观察超参数</span></p></li><li><p><span>如果计算资源多，毫无疑问用鱼子酱训练（同时训练多个模型，然后看那个最好，就用那个做你的超参数）</span></p></li></ul></li><li><h3 id='名字来源'><span>名字来源</span></h3></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505134219443.png" referrerpolicy="no-referrer" alt="image-20230505134219443"></p><ul><li><h3 id='所以希望你能学会如何进行超参数的搜索过程'><span>所以希望你能学会如何进行超参数的搜索过程</span></h3></li><li><h3 id='现在还有另一种技巧能使你的神经-网络变得更加坚实它并不是对所有的神经网络都适用但当适用时它可以使超参数搜索变得容易许多并加速试验过程我们在下个视频中再讲解这个技巧'><span>现在，还有另一种技巧，能使你的神经 网络变得更加坚实，它并不是对所有的神经网络都适用，但当适用时，它可以使超参数搜索变得容易许多并加速试验过程，我们在下个视频中再讲解这个技巧</span></h3></li></ul><h2 id='34-归一化网络的激活函数normalizing-activations-in-a--network）'><span>3.4 归一化网络的激活函数（Normalizing activations in a  network）</span></h2><ul><li><p><span>Batch 归一化 ：会使你的</span><strong><span>参数搜索问题变得很容易</span></strong><span>， 使神经网络对超参数的选择更加稳定，超参数的范围会更加庞大，工作效果也很好，也会让你的训练更加容易，甚至是深层网络。</span></p><ul><li><span>之前我们看到，如果你单隐层（双层）的神经网络或者逻辑回归，你可以归一化输入特征，归一化数据集，让梯度下降速度更快</span></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505134505912.png" referrerpolicy="no-referrer" alt="image-20230505134505912"></p><p><span>图中的公式就是归一化的公式</span></p><p><span>计算均差，零均值，归一化方差（看1.9节课）</span></p></li><li><p><span>那么对于深度学习网络呢？</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505135002074.png" referrerpolicy="no-referrer" alt="image-20230505135002074"></p><ul><li><span>在深度学习网络中，我们不仅有特征x，还有每一层的激活值a^[l]。如果逻辑回归中归一化有用，那是否在深层神经网络中我们归一化a^[2]，可以第三层参数𝑤[3]，𝑏 [3]训练的更好？</span></li><li><span>答案是肯定的，而且batch归一化做的就是这个事情</span></li><li><span>并且严谨来说，我们归一化的是z^[2]而不是a^[2]。这个事情在一些论文中还是存在争议，就是我们是否应该对激活函数的输入值进行归一化，这里andrew推荐并且讲解的是归一化z值 </span></li></ul></li><li><p><span>方法</span></p><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505155008953.png" referrerpolicy="no-referrer" alt="image-20230505155008953"></p><ul><li><span>这里举的例子是某一层不同样本的z值，其中z norm 的分母会有一个ε，这是防止分母为0</span></li></ul></li><li><p><span>经过了上述方法，就可以将z从随机变量标准化， 让它符合均值为0，标准差为1的标准正态分布。也就是消除了量纲</span></p></li><li><h3 id='但是在深度学习中我们有时候不需要总让z符合均值0标准差为1的标准正态分布也许隐藏单元有了不同的分布会有别的意义'><span>但是在深度学习中，我们有时候不需要总让z符合均值0，标准差为1的标准正态分布，也许隐藏单元有了不同的分布会有别的意义。</span></h3></li><li><p><span>所以我们会这样处理</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505155527183.png" referrerpolicy="no-referrer" alt="image-20230505155527183"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505161747185.png" referrerpolicy="no-referrer" alt="image-20230505161747185"></p><p><span>𝛾和β是控制新的均值和方差的，𝛾是控制方差的，β是控制均值的，通过更改这两个数，你可以让Z符合你想要的均值和方差，让他们更好的匹配一些函数</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505161917915.png" referrerpolicy="no-referrer" alt="image-20230505161917915"></p><p><span>这个就是batch归一化的完整过程，图上的𝛾和β只是个示例，让Z = Znorm。 你可以根据你的需要去更改这两个</span></p></li></ul></li></ul><h2 id='35-将-batch-norm-拟合进神经网络fitting-batch-norm-into--a-neural-network）'><span>3.5 将 Batch Norm 拟合进神经网络（Fitting Batch Norm into  a neural network）</span></h2><ul><li><p><span>Batch Norm 简称BN，Batch Norm一般和mini-batch一起用</span></p></li><li><p><span>这节课让我们看看如何把BN拟合进深度神经网络，还记得神经网络每一个隐层单元其实在做两件事情吗？ 计算输入特征的加权结果 z^[l]</span><span>(</span><span>i)，通过激活函数g( z^[l]</span><span>(</span><span>i) )来计算出该层的激活值a^[l]</span><span>(</span><span>i)</span></p></li><li><p><span>归一化是归一化输入特征，而batch 归一化是归一化z值，新的z值由γ和β两个值来控制。</span></p><ul><li><span>请注意adam、RESprop、Momentum里面都有β，归一化中的β和他们不一样，但都是β</span></li></ul></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505163939647.png" referrerpolicy="no-referrer" alt="image-20230505163939647"></p><ul><li><p><span>正向传播如上图所示。   使用batch归一化增加了两个新的参数（γ和β），同时也减少了一个参数（b）。</span></p></li><li><p><span>激活函数的输入值由原来的z^[1]，变为了</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505164501629.png" referrerpolicy="no-referrer" alt="image-20230505164501629"></p></li><li><p><span>融合了batch归一化后的参数为（b^[l]可以被当成0，或者直接去掉了）</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505164729029.png" referrerpolicy="no-referrer" alt="image-20230505164729029"></p></li><li><p><span>这个就是融合了batch归一化的正向传播</span></p></li><li><p><span>正向传播完了可以进行反向传播然后使用一种优化算法来更新参数项</span></p></li><li><p><span>优化算法可以选梯度下降、动量梯度下降Momentum、RESprop、Adam任意一种</span></p></li><li><p><span>在实践中，在深度学习框架中 比如 TensorFlow中，归一化的操作就是一句代码tf.nn.batch_normalization。</span></p></li><li><p><span>最后就是说一下β γ和维数，他们和b是一样维度的 即（n^[l] , 1)</span></p></li><li><p><span>总结使用batch归一化的过程如图</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505165335457.png" referrerpolicy="no-referrer" alt="image-20230505165335457"></p></li></ul><p><span>		</span><span>使用mini-batch梯度下降法，从子集1 开始做循环</span></p><p><span>每次循环包含：</span></p><ul><li><p><span>计算X^{t}的正向传播</span></p><ul><li><span>每一个隐藏层，使用 z~^[l]来代替原有的z^[l]</span></li></ul></li><li><p><span>进行反向传播，计算dW^[l]，db^[l]可以划掉，dβ^[l]，dγ^[l]</span></p></li><li><p><span>更新参数（使用不同的优化算法）</span></p></li></ul><h2 id='36-batch-norm-为什么奏效why-does-batch-norm-work）'><span>3.6 Batch Norm 为什么奏效？（Why does Batch Norm work?）</span></h2><p><span>三个方面</span></p><ul><li><p><span>让数据做特征缩放，可以让各个W的值之间的差距不大，加快梯度下降速度</span></p></li><li><p><span>可以让你的权重更加经受得起输入值的变化（每一层都让它相当于重新学习一遍特征）</span></p><ul><li><p><span>也就是解决Covariate shift 协变量偏移问题（训练数据和测试数据的特征分布不一样，导致策略的价值也随数据集的变化而变化，用训练数据学出来的策略就不一定是测试数据上最优的了）</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505180433432.png" referrerpolicy="no-referrer" alt="image-20230505180433432"></p><p><span>图左是训练数据，图右是测试数据，可以看到 训练数据拟合的曲线，对于测试数据来说表现就没那么好了，可能需要重新拟合</span></p></li><li><p><span>看这个例子，假如有这么一个网络</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505175025356.png" referrerpolicy="no-referrer" alt="image-20230505175025356"></p></li><li><p><span>假设我们就关注它的第三层，把左面的东西先遮住</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505175043949.png" referrerpolicy="no-referrer" alt="image-20230505175043949"></p></li><li><p><span>那么这一层的输入值a^[2]，就会影响到W^[3]和b^[3]。根据相同的逻辑W4 b4也会被影响</span></p></li><li><p><span>因为神经网络的工作就是通过输入值，去寻找让它最能拟合预测值y的参数项，所以这时候三层四层五层的参数就都被调整为适合a3的参数了</span></p></li><li><p><span>那这时候揭开左面的遮盖</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505180128345.png" referrerpolicy="no-referrer" alt="image-20230505180128345"></p></li><li><p><span>可以看到左面还有参数W1 b1 W2 b2，如果这些参数再改变，那么a2的值会改变。所以从第三层看来，这个a2的值一直是在改变的，它就有了协变量偏移问题。</span></p></li><li><p><span>那么此时Batch归一化做的就是减少了 Z的分布，让Z怎么变都会是符合我们要求的均值和方差（也就是让Z的形状基本是一样的）</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505180545014.png" referrerpolicy="no-referrer" alt="image-20230505180545014"></p></li><li><p><span>那么此时，每一层学习出来的参数就都是差不多的了，因为无论数据怎么变，它最后都是按照我们规定的形状来分布的，所以这对于后层的单元来说，这变相的削减了前层的特征变化对于自己的影响（也就是让每一层都能自己学习，不会被之前的输出值的噪声干扰）</span></p></li></ul></li><li><p><span>有轻微的正则化效果（和dropout一样，mini-batch的值越大，正则化效果越弱）</span></p><ul><li><p><span>batch归一化和minibatch梯度下降总是喜欢一起用，mini-batch计算出来的均值和方差是该minibatch子集的数据，而并不是整个训练集的数据，所以此时的均值和方差是有噪声的。</span></p><ul><li><span>那么缩放z到z~的过程中也是有噪声的（因为方差和均值是有噪声的，计算出来的z也就是有噪声的）</span></li><li><span>这点和dropout很像，dropout增加噪声的方式就是暂时删除一部分节点的输出值。而batch归一化就是让z的输出值是有噪声的，所以这两个很像。</span></li><li><span>给数据添加了噪声，就意味着让后部单元不过分依赖任何一个前部隐藏单元，减轻神经元之间的共适应现象</span></li><li><span>所以说batch归一化有轻微的正则化效果，但是不强，而且和dropout一样随着mini-batch的值越大，正则化效果越弱</span></li><li><span>所以batch归一化和dropout可以一起使用</span></li></ul></li></ul></li></ul><h2 id='37-测试时的-batch-normbatch-norm-at-test-time）'><span>3.7 测试时的 Batch Norm（Batch Norm at test time）</span></h2><ul><li><span>在训练时候，使用mini-batch梯度下降法和batch norm 正则化，会让你每一次计算出来的 γ和β都是针对于这一个mini-batch子集的，而不是针对于整个训练集的。</span></li><li><span>如果要测试训练好的模型的话，你就需要使用一个单独的𝜇和𝜎2，来完成整个测试集的前向传播，那这两个值如何计算？</span></li><li><span>训练集中会根据不同的mini-batch，比如输入X{1} X{2}... X{m}，会分别得出γ1 γ2。。。 β1 β2.。。</span></li><li><span>你需要做的就是把这些γ和β分别求他们的指数加权平均数得到γ和β</span></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505204436993.png" referrerpolicy="no-referrer" alt="image-20230505204436993"></p><h2 id='38-softmax-回归softmax-regression）'><span>3.8 Softmax 回归（Softmax regression）</span></h2><ul><li><p><span>逻辑回归可以做二元分类，如果要做多元分类的话用Softmax</span></p></li><li><p><span>符号：C 代表 分类类别数（比如你的应用要分鸡 考拉 猫和其他，那就是 C = 4），这四个类别也就是按顺序来编号 鸡 0   考拉1  猫2  其他3 </span></p></li><li><p><span>该神经网络的结构图如下</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505212806452.png" referrerpolicy="no-referrer" alt="image-20230505212806452"></p><p><span>该网络有C个输出单元，同时 n^L ，也就是输出层的单元数一般也是C</span></p><p><span>四个输出单元每一个单元输出对应一种预测种类（比如第一个单元是其他，第二个单元是鸡，第三个单元是考拉，第四个单元是猫）的预测几率。</span></p><p><span>y帽子将是一个(n^[L], 1)维度的向量，该例子中y帽子是一个（4,1）的向量。</span></p><p><span>y帽子中的四个输出值加起来应该是1</span></p></li><li><p><span>Softmax的前向传播计算步骤</span></p><ul><li><p><span>计算线性组合 z</span></p></li><li><p><span>使用Softmax激活函数</span></p><ul><li><p><span>设临时变量 t ，  t = e ^ z[ l ]</span></p><p><span>例子：假设z^[l]是一个 （4，1）向量[5, 2, -1, 3]，那么t = e^z[1] = 一个4,1的列向量 [e^5, e^2, e^-1, e^3]，那么t = [148.4, 7.4, 0.4, 20.1]</span></p></li><li><p><span>a^[l]也是一个和t同维度的列向量，这一步也是在做归一化，让a的各个分量的累加和是1</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1918" cid="n1918" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="11.979ex" height="5.712ex" role="img" focusable="false" viewBox="0 -1302 5294.9 2524.5" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -2.766ex;"><defs><path id="MJX-6-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-6-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-6-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-6-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-6-TEX-I-1D6F4" d="M65 0Q58 4 58 11Q58 16 114 67Q173 119 222 164L377 304Q378 305 340 386T261 552T218 644Q217 648 219 660Q224 678 228 681Q231 683 515 683H799Q804 678 806 674Q806 667 793 559T778 448Q774 443 759 443Q747 443 743 445T739 456Q739 458 741 477T743 516Q743 552 734 574T710 609T663 627T596 635T502 637Q480 637 469 637H339Q344 627 411 486T478 341V339Q477 337 477 336L457 318Q437 300 398 265T322 196L168 57Q167 56 188 56T258 56H359Q426 56 463 58T537 69T596 97T639 146T680 225Q686 243 689 246T702 250H705Q726 250 726 239Q726 238 683 123T639 5Q637 1 610 1Q577 0 348 0H65Z"></path><path id="MJX-6-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-6-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path id="MJX-6-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-6-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D459" xlink:href="#MJX-6-TEX-I-1D459"></use></g></g></g><g data-mml-node="mo" transform="translate(1100.5,0)"><use data-c="3D" xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2156.3,0)"><g data-mml-node="mi" transform="translate(1388.8,676)"><use data-c="1D461" xlink:href="#MJX-6-TEX-I-1D461"></use></g><g data-mml-node="mrow" transform="translate(220,-867.2)"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D6F4" xlink:href="#MJX-6-TEX-I-1D6F4"></use></g><g data-mml-node="TeXAtom" transform="translate(890.3,361.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-6-TEX-I-1D45B"></use></g><g data-mml-node="TeXAtom" transform="translate(633,289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D43F" xlink:href="#MJX-6-TEX-I-1D43F"></use></g></g></g></g><g data-mml-node="TeXAtom" transform="translate(813,-297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-6-TEX-I-1D456"></use></g><g data-mml-node="mo" transform="translate(345,0)"><use data-c="3D" xlink:href="#MJX-6-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1123,0)"><use data-c="31" xlink:href="#MJX-6-TEX-N-31"></use></g></g></g><g data-mml-node="msub" transform="translate(2010.6,0)"><g data-mml-node="mi"><use data-c="1D461" xlink:href="#MJX-6-TEX-I-1D461"></use></g><g data-mml-node="TeXAtom" transform="translate(394,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D456" xlink:href="#MJX-6-TEX-I-1D456"></use></g></g></g></g><rect width="2898.6" height="60" x="120" y="220"></rect></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msup><mi>a</mi><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow></msup><mo>=</mo><mfrac><mi>t</mi><mrow><msubsup><mi>Σ</mi><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow data-mjx-texclass="ORD"><msup><mi>n</mi><mrow data-mjx-texclass="ORD"><mi>L</mi></mrow></msup></mrow></msubsup><msub><mi>t</mi><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub></mrow></mfrac></math></mjx-assistive-mml></mjx-container></div></div><p><span>例子，如果继续使用上面的数据，那么</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1920" cid="n1920" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="19.915ex" height="4.548ex" role="img" focusable="false" viewBox="0 -1302 8802.3 2010" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.602ex;"><defs><path id="MJX-7-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-7-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-7-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-7-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path id="MJX-7-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-7-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-7-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-7-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-7-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-7-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-7-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-7-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-7-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-7-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D459" xlink:href="#MJX-7-TEX-I-1D459"></use></g></g></g><g data-mml-node="mo" transform="translate(1100.5,0)"><use data-c="3D" xlink:href="#MJX-7-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2156.3,0)"><g data-mml-node="mi" transform="translate(1178.5,676)"><use data-c="1D461" xlink:href="#MJX-7-TEX-I-1D461"></use></g><g data-mml-node="mn" transform="translate(220,-686)"><use data-c="31" xlink:href="#MJX-7-TEX-N-31"></use><use data-c="37" xlink:href="#MJX-7-TEX-N-37" transform="translate(500,0)"></use><use data-c="36" xlink:href="#MJX-7-TEX-N-36" transform="translate(1000,0)"></use><use data-c="2E" xlink:href="#MJX-7-TEX-N-2E" transform="translate(1500,0)"></use><use data-c="33" xlink:href="#MJX-7-TEX-N-33" transform="translate(1778,0)"></use></g><rect width="2478" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(4874.3,0)"><use data-c="28" xlink:href="#MJX-7-TEX-N-28"></use></g><g data-mml-node="mn" transform="translate(5263.3,0)"><use data-c="34" xlink:href="#MJX-7-TEX-N-34"></use></g><g data-mml-node="mo" transform="translate(5763.3,0)"><use data-c="2C" xlink:href="#MJX-7-TEX-N-2C"></use></g><g data-mml-node="mn" transform="translate(6207.9,0)"><use data-c="31" xlink:href="#MJX-7-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(6707.9,0)"><use data-c="29" xlink:href="#MJX-7-TEX-N-29"></use></g><g data-mml-node="mtext" transform="translate(7096.9,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">向</text></g><g data-mml-node="mtext" transform="translate(7948.9,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">量</text></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msup><mi>a</mi><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow></msup><mo>=</mo><mfrac><mi>t</mi><mn>176.3</mn></mfrac><mo stretchy="false">(</mo><mn>4</mn><mo>,</mo><mn>1</mn><mo stretchy="false">)</mo><mtext>向</mtext><mtext>量</mtext></math></mjx-assistive-mml></mjx-container></div></div><p><span>那么它的输出为</span></p><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1922" cid="n1922" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="19.522ex" height="5.018ex" role="img" focusable="false" viewBox="0 -1509.9 8628.7 2217.9" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.602ex;"><defs><path id="MJX-8-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-8-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-8-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-8-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-8-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-8-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path id="MJX-8-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-8-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-8-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-8-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-8-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-8-TEX-N-38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path><path id="MJX-8-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-8-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-8-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D459" xlink:href="#MJX-8-TEX-I-1D459"></use></g></g><g data-mml-node="TeXAtom" transform="translate(562,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="31" xlink:href="#MJX-8-TEX-N-31"></use></g></g></g><g data-mml-node="mo" transform="translate(1243.3,0)"><use data-c="3D" xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2299.1,0)"><g data-mml-node="msup" transform="translate(907.7,676)"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-8-TEX-I-1D452"></use></g><g data-mml-node="mn" transform="translate(499,363) scale(0.707)"><use data-c="35" xlink:href="#MJX-8-TEX-N-35"></use></g></g><g data-mml-node="mn" transform="translate(220,-686)"><use data-c="31" xlink:href="#MJX-8-TEX-N-31"></use><use data-c="37" xlink:href="#MJX-8-TEX-N-37" transform="translate(500,0)"></use><use data-c="36" xlink:href="#MJX-8-TEX-N-36" transform="translate(1000,0)"></use><use data-c="2E" xlink:href="#MJX-8-TEX-N-2E" transform="translate(1500,0)"></use><use data-c="33" xlink:href="#MJX-8-TEX-N-33" transform="translate(1778,0)"></use></g><rect width="2478" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(5294.9,0)"><use data-c="3D" xlink:href="#MJX-8-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(6350.7,0)"><use data-c="30" xlink:href="#MJX-8-TEX-N-30"></use><use data-c="2E" xlink:href="#MJX-8-TEX-N-2E" transform="translate(500,0)"></use><use data-c="38" xlink:href="#MJX-8-TEX-N-38" transform="translate(778,0)"></use><use data-c="34" xlink:href="#MJX-8-TEX-N-34" transform="translate(1278,0)"></use><use data-c="32" xlink:href="#MJX-8-TEX-N-32" transform="translate(1778,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>a</mi><mrow data-mjx-texclass="ORD"><mn>1</mn></mrow><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow></msubsup><mo>=</mo><mfrac><msup><mi>e</mi><mn>5</mn></msup><mn>176.3</mn></mfrac><mo>=</mo><mn>0.842</mn></math></mjx-assistive-mml></mjx-container></div></div><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1923" cid="n1923" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="19.522ex" height="5.018ex" role="img" focusable="false" viewBox="0 -1509.9 8628.7 2217.9" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.602ex;"><defs><path id="MJX-9-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-9-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-9-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-9-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-9-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-9-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-9-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-9-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-9-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-9-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-9-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-9-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-9-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D459" xlink:href="#MJX-9-TEX-I-1D459"></use></g></g><g data-mml-node="TeXAtom" transform="translate(562,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="32" xlink:href="#MJX-9-TEX-N-32"></use></g></g></g><g data-mml-node="mo" transform="translate(1243.3,0)"><use data-c="3D" xlink:href="#MJX-9-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2299.1,0)"><g data-mml-node="msup" transform="translate(907.7,676)"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-9-TEX-I-1D452"></use></g><g data-mml-node="mn" transform="translate(499,363) scale(0.707)"><use data-c="32" xlink:href="#MJX-9-TEX-N-32"></use></g></g><g data-mml-node="mn" transform="translate(220,-686)"><use data-c="31" xlink:href="#MJX-9-TEX-N-31"></use><use data-c="37" xlink:href="#MJX-9-TEX-N-37" transform="translate(500,0)"></use><use data-c="36" xlink:href="#MJX-9-TEX-N-36" transform="translate(1000,0)"></use><use data-c="2E" xlink:href="#MJX-9-TEX-N-2E" transform="translate(1500,0)"></use><use data-c="33" xlink:href="#MJX-9-TEX-N-33" transform="translate(1778,0)"></use></g><rect width="2478" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(5294.9,0)"><use data-c="3D" xlink:href="#MJX-9-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(6350.7,0)"><use data-c="30" xlink:href="#MJX-9-TEX-N-30"></use><use data-c="2E" xlink:href="#MJX-9-TEX-N-2E" transform="translate(500,0)"></use><use data-c="30" xlink:href="#MJX-9-TEX-N-30" transform="translate(778,0)"></use><use data-c="34" xlink:href="#MJX-9-TEX-N-34" transform="translate(1278,0)"></use><use data-c="32" xlink:href="#MJX-9-TEX-N-32" transform="translate(1778,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>a</mi><mrow data-mjx-texclass="ORD"><mn>2</mn></mrow><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow></msubsup><mo>=</mo><mfrac><msup><mi>e</mi><mn>2</mn></msup><mn>176.3</mn></mfrac><mo>=</mo><mn>0.042</mn></math></mjx-assistive-mml></mjx-container></div></div><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1924" cid="n1924" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="19.522ex" height="4.885ex" role="img" focusable="false" viewBox="0 -1451.2 8628.7 2159.2" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.602ex;"><defs><path id="MJX-10-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-10-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-10-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-10-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-10-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-10-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-10-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-10-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-10-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-10-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-10-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-10-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-10-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D459" xlink:href="#MJX-10-TEX-I-1D459"></use></g></g><g data-mml-node="TeXAtom" transform="translate(562,-247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="33" xlink:href="#MJX-10-TEX-N-33"></use></g></g></g><g data-mml-node="mo" transform="translate(1243.3,0)"><use data-c="3D" xlink:href="#MJX-10-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2299.1,0)"><g data-mml-node="mrow" transform="translate(559.4,676)"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-10-TEX-I-1D452"></use></g><g data-mml-node="mo" transform="translate(499,363) scale(0.707)"><use data-c="2212" xlink:href="#MJX-10-TEX-N-2212"></use></g></g><g data-mml-node="mn" transform="translate(1099.1,0)"><use data-c="31" xlink:href="#MJX-10-TEX-N-31"></use></g></g><g data-mml-node="mn" transform="translate(220,-686)"><use data-c="31" xlink:href="#MJX-10-TEX-N-31"></use><use data-c="37" xlink:href="#MJX-10-TEX-N-37" transform="translate(500,0)"></use><use data-c="36" xlink:href="#MJX-10-TEX-N-36" transform="translate(1000,0)"></use><use data-c="2E" xlink:href="#MJX-10-TEX-N-2E" transform="translate(1500,0)"></use><use data-c="33" xlink:href="#MJX-10-TEX-N-33" transform="translate(1778,0)"></use></g><rect width="2478" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(5294.9,0)"><use data-c="3D" xlink:href="#MJX-10-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(6350.7,0)"><use data-c="30" xlink:href="#MJX-10-TEX-N-30"></use><use data-c="2E" xlink:href="#MJX-10-TEX-N-2E" transform="translate(500,0)"></use><use data-c="30" xlink:href="#MJX-10-TEX-N-30" transform="translate(778,0)"></use><use data-c="30" xlink:href="#MJX-10-TEX-N-30" transform="translate(1278,0)"></use><use data-c="32" xlink:href="#MJX-10-TEX-N-32" transform="translate(1778,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>a</mi><mrow data-mjx-texclass="ORD"><mn>3</mn></mrow><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow></msubsup><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mo>−</mo></msup><mn>1</mn></mrow><mn>176.3</mn></mfrac><mo>=</mo><mn>0.002</mn></math></mjx-assistive-mml></mjx-container></div></div><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1925" cid="n1925" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="19.522ex" height="5.016ex" role="img" focusable="false" viewBox="0 -1509.2 8628.7 2217.2" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.602ex;"><defs><path id="MJX-11-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-11-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-11-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path><path id="MJX-11-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-11-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path id="MJX-11-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path id="MJX-11-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-11-TEX-N-37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path id="MJX-11-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path id="MJX-11-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path id="MJX-11-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-11-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D459" xlink:href="#MJX-11-TEX-I-1D459"></use></g></g><g data-mml-node="TeXAtom" transform="translate(562,-253.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use data-c="34" xlink:href="#MJX-11-TEX-N-34"></use></g></g></g><g data-mml-node="mo" transform="translate(1243.3,0)"><use data-c="3D" xlink:href="#MJX-11-TEX-N-3D"></use></g><g data-mml-node="mfrac" transform="translate(2299.1,0)"><g data-mml-node="msup" transform="translate(907.7,676)"><g data-mml-node="mi"><use data-c="1D452" xlink:href="#MJX-11-TEX-I-1D452"></use></g><g data-mml-node="mn" transform="translate(499,363) scale(0.707)"><use data-c="33" xlink:href="#MJX-11-TEX-N-33"></use></g></g><g data-mml-node="mn" transform="translate(220,-686)"><use data-c="31" xlink:href="#MJX-11-TEX-N-31"></use><use data-c="37" xlink:href="#MJX-11-TEX-N-37" transform="translate(500,0)"></use><use data-c="36" xlink:href="#MJX-11-TEX-N-36" transform="translate(1000,0)"></use><use data-c="2E" xlink:href="#MJX-11-TEX-N-2E" transform="translate(1500,0)"></use><use data-c="33" xlink:href="#MJX-11-TEX-N-33" transform="translate(1778,0)"></use></g><rect width="2478" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(5294.9,0)"><use data-c="3D" xlink:href="#MJX-11-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(6350.7,0)"><use data-c="30" xlink:href="#MJX-11-TEX-N-30"></use><use data-c="2E" xlink:href="#MJX-11-TEX-N-2E" transform="translate(500,0)"></use><use data-c="31" xlink:href="#MJX-11-TEX-N-31" transform="translate(778,0)"></use><use data-c="31" xlink:href="#MJX-11-TEX-N-31" transform="translate(1278,0)"></use><use data-c="34" xlink:href="#MJX-11-TEX-N-34" transform="translate(1778,0)"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msubsup><mi>a</mi><mrow data-mjx-texclass="ORD"><mn>4</mn></mrow><mrow data-mjx-texclass="ORD"><mi>l</mi></mrow></msubsup><mo>=</mo><mfrac><msup><mi>e</mi><mn>3</mn></msup><mn>176.3</mn></mfrac><mo>=</mo><mn>0.114</mn></math></mjx-assistive-mml></mjx-container></div></div><ul><li><span>四个输出就对应了类别0 1 2 3 的可能性，最后的输出结果就是可能性最大的那个</span></li></ul></li><li><p><span>如果这个计算的是a^[L]，也就是输出层的激活值，那么a^[L] = y帽子，这个y帽子也是一个和a^[L]一样维度的列向量</span></p></li><li><h3 id='softmax和逻辑回归的区别就在于softmax的输入值是一个列向量而逻辑回归的输入值是单值'><span>softmax和逻辑回归的区别就在于，softmax的输入值是一个列向量，而逻辑回归的输入值是单值</span></h3></li></ul></li><li><p><span>没有隐藏层的softmax函数的表现</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505220332499.png" referrerpolicy="no-referrer" alt="image-20230505220332499"></p><p><span>它做出的决策边界都是线性的，等有隐藏层了就可以做非线性的决策边界了。</span></p></li></ul></li></ul><h2 id='39-训练一个-softmax-分类器training-a-softmax-classifier）'><span>3.9 训练一个 Softmax 分类器（Training a Softmax classifier）</span></h2><ul><li><p><span>softmax函数的名字来源于另一个hardmax函数，hardmax函数是输入一个列向量，把最大的那个分量标为1，其他的分量标0，然后输出一个相同大小的列向量</span></p></li><li><p><span>softmax就温和一些，输出的是概率</span></p></li><li><p><span>softmax可以看做一个可以分C类的逻辑回归，如果C =2 ，那softmax也是一个逻辑回归</span></p></li><li><h2 id='loss-function'><span>loss function</span></h2><ul><li><p><span>如果真实值 y = [0, 1, 0, 0]，也就是输出类别1。</span></p></li><li><p><span>而y帽子 = {0.3, 0.2, 0.1, 0.4}，预计输出类比1的概率是20%，这样误差太大了，所以</span></p></li><li><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n1955" cid="n1955" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="34.898ex" height="2.962ex" role="img" focusable="false" viewBox="0 -911.5 15425 1309.2" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.9ex;"><defs><path id="MJX-12-TEX-I-1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path id="MJX-12-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-12-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-12-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-12-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path id="MJX-12-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-12-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-12-TEX-I-1D6F4" d="M65 0Q58 4 58 11Q58 16 114 67Q173 119 222 164L377 304Q378 305 340 386T261 552T218 644Q217 648 219 660Q224 678 228 681Q231 683 515 683H799Q804 678 806 674Q806 667 793 559T778 448Q774 443 759 443Q747 443 743 445T739 456Q739 458 741 477T743 516Q743 552 734 574T710 609T663 627T596 635T502 637Q480 637 469 637H339Q344 627 411 486T478 341V339Q477 337 477 336L457 318Q437 300 398 265T322 196L168 57Q167 56 188 56T258 56H359Q426 56 463 58T537 69T596 97T639 146T680 225Q686 243 689 246T702 250H705Q726 250 726 239Q726 238 683 123T639 5Q637 1 610 1Q577 0 348 0H65Z"></path><path id="MJX-12-TEX-I-1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path><path id="MJX-12-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path id="MJX-12-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-12-TEX-N-A0" d=""></path><path id="MJX-12-TEX-N-2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path><path id="MJX-12-TEX-I-1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path id="MJX-12-TEX-I-1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path id="MJX-12-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="1D43F" xlink:href="#MJX-12-TEX-I-1D43F"></use></g><g data-mml-node="mo" transform="translate(681,0)"><use data-c="28" xlink:href="#MJX-12-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(1070,0)"><use data-c="1D466" xlink:href="#MJX-12-TEX-I-1D466"></use></g><g data-mml-node="mo" transform="translate(1560,0)"><use data-c="2C" xlink:href="#MJX-12-TEX-N-2C"></use></g><g data-mml-node="mi" transform="translate(2004.7,0)"><use data-c="1D466" xlink:href="#MJX-12-TEX-I-1D466"></use></g><g data-mml-node="mtext" transform="translate(2494.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">帽</text></g><g data-mml-node="mtext" transform="translate(3346.7,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">子</text></g><g data-mml-node="mo" transform="translate(4200,0)"><use data-c="29" xlink:href="#MJX-12-TEX-N-29"></use></g><g data-mml-node="mo" transform="translate(4866.8,0)"><use data-c="3D" xlink:href="#MJX-12-TEX-N-3D"></use></g><g data-mml-node="mo" transform="translate(5922.6,0)"><use data-c="2212" xlink:href="#MJX-12-TEX-N-2212"></use></g><g data-mml-node="msubsup" transform="translate(6700.6,0)"><g data-mml-node="mi"><use data-c="1D6F4" xlink:href="#MJX-12-TEX-I-1D6F4"></use></g><g data-mml-node="TeXAtom" transform="translate(890.3,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D436" xlink:href="#MJX-12-TEX-I-1D436"></use></g></g><g data-mml-node="TeXAtom" transform="translate(813,-253.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D457" xlink:href="#MJX-12-TEX-I-1D457"></use></g><g data-mml-node="mo" transform="translate(412,0)"><use data-c="3D" xlink:href="#MJX-12-TEX-N-3D"></use></g><g data-mml-node="mn" transform="translate(1190,0)"><use data-c="31" xlink:href="#MJX-12-TEX-N-31"></use></g></g></g><g data-mml-node="mtext" transform="translate(8758.6,0)"><use data-c="A0" xlink:href="#MJX-12-TEX-N-A0"></use></g><g data-mml-node="msub" transform="translate(9008.6,0)"><g data-mml-node="mi"><use data-c="1D466" xlink:href="#MJX-12-TEX-I-1D466"></use></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D457" xlink:href="#MJX-12-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(10095.1,0)"><use data-c="2217" xlink:href="#MJX-12-TEX-N-2217"></use></g><g data-mml-node="mi" transform="translate(10817.3,0)"><use data-c="1D459" xlink:href="#MJX-12-TEX-I-1D459"></use></g><g data-mml-node="mi" transform="translate(11115.3,0)"><use data-c="1D45C" xlink:href="#MJX-12-TEX-I-1D45C"></use></g><g data-mml-node="mi" transform="translate(11600.3,0)"><use data-c="1D454" xlink:href="#MJX-12-TEX-I-1D454"></use></g><g data-mml-node="mo" transform="translate(12077.3,0)"><use data-c="28" xlink:href="#MJX-12-TEX-N-28"></use></g><g data-mml-node="mi" transform="translate(12466.3,0)"><use data-c="1D466" xlink:href="#MJX-12-TEX-I-1D466"></use></g><g data-mml-node="mtext" transform="translate(12956.3,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">帽</text></g><g data-mml-node="msub" transform="translate(13808.3,0)"><g data-mml-node="mtext"><text data-variant="normal" transform="scale(1,-1)" font-size="852px" font-family="serif">子</text></g><g data-mml-node="TeXAtom" transform="translate(886.3,-235.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use data-c="1D457" xlink:href="#MJX-12-TEX-I-1D457"></use></g></g></g><g data-mml-node="mo" transform="translate(15036,0)"><use data-c="29" xlink:href="#MJX-12-TEX-N-29"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mo stretchy="false">(</mo><mi>y</mi><mo>,</mo><mi>y</mi><mtext>帽</mtext><mtext>子</mtext><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msubsup><mi>Σ</mi><mrow data-mjx-texclass="ORD"><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi></mrow></msubsup><mtext>&nbsp;</mtext><msub><mi>y</mi><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>y</mi><mtext>帽</mtext><msub><mtext>子</mtext><mrow data-mjx-texclass="ORD"><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></div></div><ul><li><span>啥原理呢？ 求损失函数的时候，y1 = y3 = y4 = 0，那么损失函数就等于 y_2 * log (y帽子_2) ，也就是 L = -1 * log(0.2)，那想要让L变小，就得让log0.2变大，那就得让log里面的指数（0.2）变大（即几率）。</span></li></ul></li></ul></li><li><h2 id='cost-funciton'><span>Cost funciton</span></h2><ul><li><p><span>刚才是对应一个样本的，那对于整个训练集呢？</span></p><ul><li><span>J(W1,b1,W2,b2....WL, bL) = 1 / m * Σ i = 1, m L(y帽子^(i) , y^(i))</span></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505223107524.png" referrerpolicy="no-referrer" alt="image-20230505223107524"></li><li><span>Y和Y帽子的列向量维数基本是（C , m）的</span></li></ul></li></ul></li><li><p><span>反向传播</span></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505224825301.png" referrerpolicy="no-referrer" alt="image-20230505224825301"></p><ul><li><span>反向传播NG主要是说后面的深度学习框架根据你的前向传播自动帮你计算反向传播和相关的导数。</span></li></ul><p>&nbsp;</p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230505224925915.png" referrerpolicy="no-referrer" alt="image-20230505224925915"></p><h2 id='310-深度学习框架deep-learning-frameworks）'><span>3.10 深度学习框架（Deep Learning frameworks）</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506104706574.png" referrerpolicy="no-referrer" alt="image-20230506104706574"></p><ul><li><h3 id='andrew选择框架的标准1便于编程2运行速度特别是大数据集）3是否真的开放是长期开源而不是短期）'><span>andrew选择框架的标准：1便于编程，2运行速度（特别是大数据集），3是否真的开放（是长期开源，而不是短期）</span></h3></li><li><h3 id='深度学习框架可以帮助你开发深度机器学习应用时更加高效'><span>深度学习框架可以帮助你开发深度机器学习应用时更加高效</span></h3></li></ul><h2 id='311-tensorflow'><span>3.11 TensorFlow</span></h2><p><span>可以看笔记复习怎么用TensorFlow</span></p><h1 id='第三门课-结构化机器学习项目structuring--machine-learning-projects）'><span>第三门课 结构化机器学习项目（Structuring  Machine Learning Projects） </span></h1><h2 id='第一周-机器学习ml）策略1）ml-strategy1））'><span>第一周 机器学习（ML）策略（1）（ML strategy（1））</span></h2><h3 id='11-为什么是-ml-策略why-ml-strategy）'><span>1.1 为什么是 ML 策略？（Why ML Strategy?）</span></h3><ul><li><span>如何构建你的机器学习项目也就是说机器学习的策略。我希望通过这门课程你们能够学到如何更快速高效地优化你的机器学习系统。</span></li><li><span>这节课主要是在告诉你，如果你的神经网络的准确率想要再进一步，那么你有很多选择（增加训练集集大小、增加反例集大小、用更多时间训练梯度下降、更换优化算法等等），</span><strong><span>在众多的选择之中如何在短时间如果有快速有效的方法能够判断哪些想法是靠谱的，或者甚至提出新的想法， 判断哪些是值得一试的想法，哪些是可以放心舍弃的。</span></strong></li></ul><h2 id='12-正交化orthogonalization）'><span>1.2 正交化（Orthogonalization）</span></h2><ul><li><p><span>搭建建立机器学习系统的挑战之一是，你可以尝试和改变的东西太多太多了，比如有那么多的超参数可以调，我留意到，那些效率很高的机器学习专家有个特点，他们思维清晰，对于要调整什么来达到某个效果，非常清楚，这个步骤我们称之为正交化，让我们来看看是什么</span></p></li><li><p><span>正交化是什么意思：要针对性的解决问题，每次调整只调整一个性质（就像之前的电视例子）。</span></p></li><li><p><span>建立一个可用的神经网络往往要经历四个步骤</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506132700566.png" referrerpolicy="no-referrer" alt="image-20230506132700566"></p><ol start='' ><li><p><span>算法很好的拟合训练集</span></p><ul><li><p><span>如果表现不好那就需要调整（欠拟合)</span></p><ul><li><span>可能可以训练更大的网络</span></li><li><span>切换到更好的优化算法，比如 Adam 优化算法</span></li></ul></li></ul></li><li><p><span>算法很好的拟合验证集(开发集)</span></p><ul><li><p><span>如果表现不好那就需要调整（可能对训练集有过拟合）</span></p><ul><li><span>正则化</span></li><li><span>增加训练集数据量</span></li></ul></li></ul></li><li><p><span>算法很好的拟合测试集</span></p><ul><li><p><span>如果表现不好那就需要调整（可能对验证集过拟合）</span></p><ul><li><span>增加开发集训练量</span></li></ul></li></ul></li><li><p><span>在现实世界表现好</span></p><ul><li><p><span>如果表现不好那就需要调整（可能有过拟合）</span></p><ul><li><span>改变开发集或成本函数</span></li><li><span>你的开发集分布设置不正确</span></li><li><span>要么你的成本函数测量的指标不对</span></li></ul></li></ul></li></ol></li><li><p><span>也就是说每个步骤可能遇到的问题和解决的方法都是相互独立的，遇到一个问题 针对性的解决即可，后面会更加详细的说明</span></p></li><li><p><span>NG说early stopping就是一个不是那么相对正交化的方法，因为它会导致训练集的成本函数拟合，进而影响下面开发集的</span></p></li></ul><h2 id='13-单一数字评估指标single-number-evaluation-metric）'><span>1.3 单一数字评估指标（Single number evaluation metric）</span></h2><ul><li><p><span>如何诊断系统的瓶颈在那里</span></p></li><li><h3 id='它可以快速的告诉你新尝试的手段比之前的手段好还是坏'><span>它可以快速的告诉你新尝试的手段比之前的手段好还是坏</span></h3></li><li><p><span>在开始深度学习之前，推荐给你的问题设置一个单实数评价指标</span></p></li><li><p><span>例子：比如训练一个二元分类（逻辑回归）</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506140829570.png" referrerpolicy="no-referrer" alt="image-20230506140829570"></p><ul><li><h3 id='分类器你应该关注的是它的查准率precision）和查全率recall）'><span>分类器你应该关注的是它的查准率（precision）和查全率（recall）</span></h3><ul><li><h3 id='查准率在你的分类器标记为true的例子中有多少真的true'><span>查准率：在你的分类器标记为True的例子中，有多少真的True。</span></h3></li><li><h3 id='查全率训练集的所有为true的数据你的分类器标记出来了多少'><span>查全率：训练集的所有为True的数据，你的分类器标记出来了多少</span></h3></li><li><p><span>这两个值都要顾忌到，越大越好</span></p></li></ul></li><li><p><span>可以看到A是95 90，改进后的B是98和85</span></p><ul><li><span>意思是如果分类器𝐴有 95%的查准率，这意味着你的分类器说这图有猫的时候，有 95%的机会真的是猫</span></li><li><span>和如果分类器𝐴查全率是 90%，这意味着对于所有的图像， 比如说你的开发集都是真的猫图，分类器𝐴准确地分辨出了其中的 90%。</span></li><li><span>所以用查准率和查全率来评估分类器是比较合理的。</span></li></ul></li></ul></li><li><p><span>问题：</span></p><ul><li><p><span>但有个问题看例子中的A和B，A在查全率上好，B是在查准率上好，那这两个我如何选择呢？</span></p></li><li><p><span>并且如果我要批量训练，从中选优的话，如果有两个评估指标，就很难 去快速地二中选一或者十中选一</span></p></li><li><h3 id='所以我们需要一个新的指标来帮助我们选优'><span>所以我们需要一个新的指标来帮助我们选优</span></h3></li></ul></li><li><h3 id='f1分数'><span>F1分数</span></h3><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506144904485.png" referrerpolicy="no-referrer" alt="image-20230506144904485"></li></ul></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506144937737.png" referrerpolicy="no-referrer" alt="image-20230506144937737"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506145034935.png" referrerpolicy="no-referrer" alt="image-20230506145034935"></p><ul><li><p><span>这样我们很快就能知道谁的表现更好了</span></p></li><li><p><span>所以这节课提到了两个思想</span></p><ul><li><h3 id='不同的应用有不同的衡量指标但是每一个应用都需要有一个自己的f1分数'><span>不同的应用有不同的衡量指标，但是每一个应用都需要有一个自己的F1分数</span></h3></li></ul></li></ul><h2 id='14-满足和优化指标satisficing-and-optimizing-metrics）'><span>1.4 满足和优化指标（Satisficing and optimizing metrics）</span></h2><ul><li><p><strong><span>要把你顾及到的所有事情组合成单实数评估指标有时并不容易</span></strong></p></li><li><p><span>在那些情况里，</span><strong><span>我发现有时候设立满足和优化指标是很重要的</span></strong></p></li><li><p><span>一个神经网络你应该对它设立一个优化指标和一个满足指标</span></p><ul><li><span>优化指标：你的神经网络所追求</span><strong><span>最佳化</span></strong><span>的某一方面</span></li><li><span>满足指标：你的神经网络应该</span><strong><span>满足</span></strong><span>的某一方面</span></li></ul></li><li><p><span>比如在刚才的分类器中，除了查准率和查全率组成的准确率，还有一个因素是图片的分类时间</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506155956338.png" referrerpolicy="no-referrer" alt="image-20230506155956338"></p><ul><li><h3 id='你想要去评估那个分类器比较好你除了可以去用一个公式去衡量他们'><span>你想要去评估那个分类器比较好，你除了可以去用一个公式去衡量他们</span></h3></li><li><h3 id='比如-cost--accurary---05-running-time'><span>比如 cost = Accurary - 0.5* Running Time</span></h3></li><li><h3 id='还可以通过设立一个优化指标和满足指标来看哪个分类器更好'><span>还可以通过设立一个优化指标和满足指标来看哪个分类器更好</span></h3><ul><li><span>比如，我想让我的</span><strong><span>分类器追求准确率</span></strong><span>，</span><strong><span>运行时间在100ms以内就好了</span></strong><span>（因为100ms以内人基本感知不到区别了）</span></li><li><span>那在这和个例子中，准确率就是优化指标，运行时间就是满足指标。</span></li><li><span>那综合来看，B就是我要的分类器，它满足我的满足指标，同时有着最好的优化指标</span></li></ul></li></ul></li><li><h3 id='总结一下'><span>总结一下</span></h3><ul><li><h3 id='你可以设立一个优化指标还有一个或多个满足指标'><span>你可以设立一个优化指标，还有一个或多个满足指标</span></h3></li><li><h3 id='这样你就有一个全自动-的方法在观察多个成本大小时选出最好的那个'><span>这样你就有一个全自动 的方法，在观察多个成本大小时，选出&quot;最好的&quot;那个。</span></h3></li><li><h3 id='这些评估指标必须是在训练集或-开发集或测试集上计算或求出来的'><span>这些评估指标必须是在训练集或 开发集或测试集上计算或求出来的。</span></h3></li></ul></li></ul><p>&nbsp;</p><h2 id='15-训练开发测试集划分traindevtest-distributions）'><span>1.5 训练/开发/测试集划分（Train/dev/test distributions）</span></h2><ul><li><h3 id='设立训练集开发集和测试集的方式大大影响了你或者你的团队在建立机器学习应用方-面取得进展的速度这节课主要是来讨论后两个'><span>设立训练集，开发集和测试集的方式大大影响了你或者你的团队在建立机器学习应用方 面取得进展的速度，这节课主要是来讨论后两个</span></h3></li><li><p><span>一般创建神经网络的步骤</span></p><ul><li><span>你尝试很多思路，用</span><strong><span>训练集训练不同的模型</span></strong></li><li><strong><span>使用开发集来评估不同的思路</span></strong><span>，然后选择一个表现最好的。不断迭代去改善开发集的性能，直到最后你可以得到一个令你满意的成本</span></li><li><span>再用</span><strong><span>测试集去评估</span></strong></li></ul></li><li><p><span>例子：</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506162713274.png" referrerpolicy="no-referrer" alt="image-20230506162713274"></p></li><li><p><span>做一个分类，有8个国家的数据，你会怎么分？是4个分到开发集 4个分到测试集？</span></p><ul><li><p><span>如果这样的话，你会发现，开发集训练的再好，模型很有可能在测试集中表现不佳，</span><strong><span>因为你的开发集数据和测试集数据分布不均</span></strong><span>，模型根本没见过测试集的这种数据，没法搞</span></p></li><li><h3 id='所以正确的做法是吧8个国家的所有数据随机抽取放到两个集中'><span>所以正确的做法是吧8个国家的所有数据，随机抽取放到两个集中</span></h3></li></ul></li><li><h3 id='开发集和测试集的数据一定是要能涵盖你未来接触的所有场景的数据一定要分布相同'><span>开发集和测试集的数据一定是要能涵盖你未来接触的所有场景的数据，一定要分布相同</span></h3></li></ul><h2 id='16-开发集和测试集的大小size-of-dev-and-test-sets）'><span>1.6 开发集和测试集的大小（Size of dev and test sets）</span></h2><ul><li><p><span>早期机器学习的时候，数据可能是几千-几万条，当时可以使用 70/30分训练集和测试集 或者 60/20/20训开测</span></p></li><li><h3 id='深度学习数据量大了起来所以以前的分法就不好了基本上9811就可以了'><span>深度学习数据量大了起来，所以以前的分法就不好了，基本上98/1/1就可以了</span></h3></li><li><h3 id='测试集'><span>测试集</span></h3><ul><li><span>这里重点提一下它，它是最终帮你测试系统实际部署之后性能的。</span></li><li><span>一般情况下测试集1万个数据就可以评估你的系统了</span></li><li><span>除非你需要对最终投产系统有一个很精确的指标，那你才需要很大的测试集，1W - 10W个数据就够了一般来说是不大于30%的</span></li></ul></li><li><p><span>有的时候你不需要对系统性能做一个置信度很高的评估，那你都不用有测试集，训练和开发就可以了。</span></p><ul><li><span>注意：有的人就叫训练集和测试集，但实际上测试集发挥着开发集的作用（调试模型），这种情况下，叫开发集是最好的</span></li><li><span>NG推荐一定要有测试集</span></li></ul></li></ul><h2 id='17-什么时候该改变开发测试集和指标when-to-change--devtest-sets-and-metrics）'><span>1.7 什么时候该改变开发/测试集和指标？（When to change  dev/test sets and metrics）</span></h2><ul><li><p><span>这次课讲的是什么情况下你需要修改你的评估指标，如果你做的模型带给了你不想要的东西，那你就需要修改你的评估指标了</span></p></li><li><p><span>比如课上的例子，有个分类器A和分类器B，分类器A的错误率低，但是输出色情图片；分类器B的错误率比A高一些，但是不会输出色情图。</span></p><ul><li><span>这时候A已经不满足你的要求了，但是你之前指定的准确率评估指标依旧会向你推荐A。</span></li><li><span>这时候的问题就是准确率评估指标需要增加新的内容，阻止你不想要的东西出现</span></li><li><span>除了更新你的评估指标，你还可以改变开发集或者测试集</span></li></ul></li><li><p><span>该例子中你用的分类错误率指标可以写成这样，mdev是开发集的样本数量，y pred是预测值（0或者1），函数 l 是统计表达式为真的样本数</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506191627374.png" referrerpolicy="no-referrer" alt="image-20230506191627374"></p><ul><li><p><span>但是它的问题并没有对色情图片做出处理，因此在函数前加上一个权重w</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506191821106.png" referrerpolicy="no-referrer" alt="image-20230506191821106"></p><ul><li><p><span>如果该样本是色情图，那w的值就是10；如果不是色情图那它的值就是1。</span></p></li><li><p><span>用这种方式来10倍惩罚把色图分类成猫猫图的分类器</span></p></li><li><p><span>最后归一化（让error映射在0~1之间），公式为</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506192047646.png" referrerpolicy="no-referrer" alt="image-20230506192047646"></p></li></ul></li><li><h3 id='注意如果你要使用这种加权你就需要自己去开发集和测试集中把色情图片标记出来这样你才能使用这个加权函数'><span>注意，如果你要使用这种加权，你就需要自己去开发集和测试集中把色情图片标记出来，这样你才能使用这个加权函数</span></h3></li><li><p><span>上述的加权只是一个例子，具体的问题你得自己来定义你的error函数</span></p></li></ul></li><li><h3 id='这是一个正交化的例子'><span>这是一个正交化的例子</span></h3><ul><li><p><span>刚才我们讨论的就是，如何确定一个指标去评估分类器，然后让我们的指标变得越来越好，达到我们的要求，这个实际上就是正交化的例子</span></p></li><li><h3 id='处理机器学习问题的时候可以分成两步来看'><span>处理机器学习问题的时候可以分成两步来看</span></h3><ul><li><h3 id='定义一个指标来衡量来表示你想要的东西明确目标）'><span>定义一个指标来衡量来表示你想要的东西（明确目标）</span></h3></li><li><h3 id='然后考虑如何改善系统的指标表现优化模型）'><span>然后考虑如何改善系统的指标表现（优化模型）</span></h3></li></ul></li></ul></li><li><h3 id='总结-1'><span>总结</span></h3></li><li><h3 id='如果你当前的指标和当前用来评估的数据和你真正关心必须做好的事情关系不大那就应该更改你的指标或者你的开发测试集让它们能更够好地反映你的算法需要处理好的数据'><span>如果你当前的指标和当前用来评估的数据和你真正关心必须做好的事情关系不大，那就应该更改你的指标或者你的开发测试集，让它们能更够好地反映你的算法需要处理好的数据。</span></h3></li><li><h3 id='没有一个完美的指标也没关系但是一定要设立最好不要在没有指标以及开发集时跑的太久'><span>没有一个完美的指标也没关系，但是一定要设立；最好不要在没有指标以及开发集时跑的太久。</span></h3></li></ul><h2 id='18-为什么是人的表现why-human-level-performance）'><span>1.8 为什么是人的表现？（Why human-level performance?）</span></h2><ul><li><p><span>过去几年，机器学习团队一直在讨论如何比较机器学习和人的表现，为啥呢？</span></p></li><li><p><span>两个主要原因：</span></p><ul><li><span>随着深度学习的进步，机器学习算法突然变的好了，已经好到有些地方可以威胁到人类的表现了</span></li><li><span>让深度学习做人类能做的事情的时候，可以通过精心设计机器学习系统的工作流程，让工作流程效率更高</span></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506200324204.png" referrerpolicy="no-referrer" alt="image-20230506200324204"></p></li><li><p><span>这张图表示的是随着时间的增多，机器的准确率会越来越高</span></p><ul><li><span>但是无论做多长时间的训练，也高不过一个理论上限（贝叶斯最优错误率Bayes optimal error）</span></li><li><span>比如语音识别，遇到一些模糊的声音，那它的准确率就不会是100%</span></li><li><span>比如一张模糊的图片，让你看是不是猫猫图，那它的准确率也不会是100%</span></li></ul></li><li><p><span>不过事实证明：机器学习的进展往往相当快，直到超越人类的表现之前一直很快，当超越人类的表现时，有时进展会变慢</span></p><ul><li><p><span>这有两个原因：</span></p><ul><li><span>人类已经做的足够好，再往上没有多少空间了（人类已经很接近贝叶斯最优错误率了）</span></li><li><span>只要你的表现比人类的表现更差，那么实际上可以 使用某些工具来提高性能。一旦你超越了人类的表现，这些工具就没那么好用了</span></li></ul></li></ul></li><li><p><span>工具指的是三个：</span></p><ul><li><span>让人类分类数据，让机器学</span></li><li><span>人工分析错误率高的原因（误差分析）</span></li><li><span>更好地分析偏差和方差</span></li></ul></li><li><p><span>但是机器超过人类的表现的时候，这三个工具就不好用了</span></p></li></ul><h2 id='19-可避免偏差avoidable-bias）'><span>1.9 可避免偏差（Avoidable bias）</span></h2><ul><li><p><span>这节课是讲如何通过贝叶斯最优错误率来调整你的策略</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230506212011329.png" referrerpolicy="no-referrer" alt="image-20230506212011329"></p><ul><li><p><span>例子：</span></p><ul><li><p><span>假设在判断图片中是否有猫的问题中，人类的错误率是1%，训练集错误率是8%，开发集是10%</span></p></li><li><p><span>如果图片模糊一点，人类的错误率是7.5%，训练集8%，开发集10%</span></p></li><li><p><span>之前我们说过贝叶斯最优错误率的问题，那是一个理论错误率的天花板，任何机器机器学习算法都无法超过的理论天花板。</span></p></li><li><p><span>在图像辨别任务中，人类的表现已经非常接近该应用的贝叶斯最优错误率，所以此时以人类的错误率作为贝叶斯最优错误率是一个非常好的选择</span></p></li><li><p><span>那再看左面的数据，此时 训练集的误差距离人类的表现还有一定的上升空间，也就是说我们还可以降低训练集的错误率。 贝叶斯最优错误率和训练集的错误率之间的差值叫做可避免偏差。</span></p></li><li><p><span>如果可避免偏差比较高（左面的例子），那我们应该着重于降低偏差，来让模型的性能更优，从而解决开发集的高错误率。</span></p><ul><li><p><span>那如何降低偏差呢？</span></p><ul><li><span>加长梯度下降时间</span></li><li><span>跑一个更大的神经网络</span></li></ul></li></ul></li><li><p><span>那再看右面的数据，此时开发集的误差距离训练集的误差还有一定的上升空间，这段空间叫方差，有2%。对比可避免偏差的上升空间0.5%，优化起来的空间就很少了。所以此时我们集中资源去减少方差会更有效，所以如果是右面的情况，你的策略应该就是去减少方差。</span></p><ul><li><p><span>减少方差方法：</span></p><ul><li><span>正则化</span></li><li><span>增加训练数据</span></li></ul></li></ul></li></ul></li></ul></li><li><p><span>总结：</span><strong><span>当你理解人类水平错误率，理解你对贝叶斯错误率的估计，你就可以在不同的场景中专注于不同的策略，使用避免偏差策略还是避免方差策略。</span></strong></p></li></ul><h2 id='110-理解人的表现understanding-human-level-performance）'><span>1.10 理解人的表现（Understanding human-level performance）</span></h2><ul><li><p><span>到底什么是人的表现呢？ 这节课来讲清楚</span></p></li><li><p><span>上节课我们使用人类表现来估计贝叶斯误差（理论最低的错误率）</span></p></li><li><p><span>但是抛出了一个问题：人类的表现在不同的群体之间也是有差别的。比如看一个有关手骨骨折的X光片，普通人的误诊率可能是1%，普通医生的误诊率可能1%，有经验的医生的误诊率可能是7%，有经验的骨科医生的误诊率可能是0.5%。</span></p></li><li><p><span>那你应该选择哪一类人群作为你系统的人类表现（贝叶斯误差）的代表呢？</span></p><ul><li><span>答：你要明白你的目标所在。你自己想要让系统达到一个什么样的高度，你就用那一群人作为贝叶斯误差。</span></li><li><span>如果只是让系统可用，那你可以用普通医生作为贝叶斯误差</span></li><li><span>如果你想让系统有非常大的价值，那你可以用表现最好的人类作为贝叶斯误差</span></li></ul></li><li><h3 id='通过了解贝叶斯误差训练集误差开发集误差我们就能从中得到应该优化可避免偏差还是优化方差的策略这一套优化策略在系统性能还未达到贝叶斯误差时很好用'><span>通过了解贝叶斯误差、训练集误差、开发集误差，我们就能从中得到应该优化可避免偏差还是优化方差的策略。这一套优化策略在系统性能还未达到贝叶斯误差时很好用。</span></h3></li><li><p><span>贝叶斯误差的选择还会产生别的影响：如果系统已经非常接近贝叶斯误差了（比如人类误差0.7%，训练集误差0.8% 开发集误差0.9）</span></p></li><li><p><span>那你此时你还可以分析出来有可避免偏差和方差可以优化，你可以选择继续优化这两项</span></p></li><li><p><span>但系统的误差达到0.7之后，可避免偏差理论上已经不存在了，那你应该朝着那个方向继续前进呢？到底这时候应该继续优化偏差呢还是方差呢？</span></p></li><li><h3 id='这就是这套优化方法的瓶颈假设系统已经达到人类表现贝叶斯误差）再优化的方向就比较迷茫了不知道优化偏差还是方差了）除非你去特别小心的再估计贝叶斯误差才能知道系统还有多少提升空间'><span>这就是这套优化方法的瓶颈，假设系统已经达到人类表现（贝叶斯误差），再优化的方向就比较迷茫了（不知道优化偏差还是方差了）。除非你去特别小心的再估计贝叶斯误差，才能知道系统还有多少提升空间。</span></h3></li><li><h3 id='同时还有一个问题如果你的训练集误差和开发集误差已经超过了贝叶斯误差那你可能也要考虑是否存在过拟合开发集误差和训练集误差不大的话可能就是真的没问题了）'><span>同时还有一个问题，如果你的训练集误差和开发集误差已经超过了贝叶斯误差，那你可能也要考虑是否存在过拟合。（开发集误差和训练集误差不大的话，可能就是真的没问题了）</span></h3></li><li><h3 id='而实际上遇到这类问题时候你应该试一试能不能在训练集上做的更好'><span>而实际上遇到这类问题时候你应该试一试能不能在训练集上做的更好</span></h3><ul><li><h3 id='上述问题也说明了为什么机器学习越接近人类水准往后越难提升因为系统的性能已经达到理论值了不知道还能不能往上提升以及如何往上提升了'><span>上述问题也说明了为什么机器学习越接近人类水准往后越难提升，因为系统的性能已经达到理论值了，不知道还能不能往上提升以及如何往上提升了。</span></h3></li></ul></li></ul><h2 id='111-超过人的表现surpassing-human--level-performance）'><span>1.11 超过人的表现（Surpassing human- level performance）</span></h2><ul><li><span>来看一些超越了贝叶斯误差的系统吧</span></li></ul><h2 id='112-改-善-你-的-模-型-的-表-现--improving-your-model--performance）'><span>1.12 改 善 你 的 模 型 的 表 现 （ Improving your model  performance）</span></h2><ul><li><p><span>这节课是对这一周的总结了</span></p></li><li><h3 id='如何提高学习算法性能的指导方针'><span>如何提高学习算法性能的指导方针</span></h3><ul><li><p><span>如果你想让一个监督学习算法变得实用，那你需要做好一下两个步骤</span></p><ul><li><span>减小可避免偏差</span></li><li><span>减小方差</span></li></ul></li><li><p><span>如何做好这两个步骤呢？你看这里有两个旋钮</span></p><ul><li><p><span>减小可避免偏差的旋钮</span></p><ul><li><span>训练一个更大的神经网络</span></li><li><span>使用更好的优化算法（Momentum、RESprop、Adam），进行更长时间的优化算法（梯度下降）</span></li><li><span>更换神经网络结构（CNN RNN）、找到一组更好的超参数（超参数搜索）</span></li></ul></li><li><p><span>减小方差的旋钮</span></p><ul><li><span>正则化（L2、Dropout）</span></li><li><span>增加更多的数据（数据增强）</span></li><li><span>更换神经网络结构（CNN RNN）、找到一组更好的超参数（超参数搜索）</span></li></ul></li></ul></li></ul></li></ul><h1 id='第二周机器学习策略2）ml-strategy-2'><span>第二周：机器学习策略（2）(ML Strategy (2))  </span></h1><h2 id='21-进行错误分析carrying-out-error-analysis）通常处理训练集的错误和开发集的错误'><span>2.1 进行错误分析（Carrying out error analysis）（通常处理训练集的错误和开发集的错误</span></h2><ul><li><p><span>比如你一个分类器，分类猫。它的准确率已经达到90%，错误率10%，你看了看错误图片里有狗狗，那你现在应不应该花时间去收集很多的狗狗图片，给分类器去训练呢？不，不应该着急，应该去评估一下如果改善了狗狗问题，它能给你带来多大的提升。这就用到了</span><strong><span>错误分析</span></strong></p></li><li><p><span>错误分析Error analysis：</span></p><ul><li><span>挑选100张错误图</span></li><li><span>看看狗狗的图片的占比</span></li></ul></li><li><p><span>如果狗狗图片只有5张，那它的占比是5%，也就是说你就算解决了狗狗图片问题，他给你的提升也只是0.5%，那这可能不是一个很好的方向。不太值得解决</span></p></li><li><p><span>如果狗狗图片有50张，占比50%，那你解决狗狗图片问题能给你带来5%的提升， 那这就是一个很好的方向。值得解决</span></p></li><li><p><span>具体来说要怎么做错误分析呢？</span></p><ul><li><p><span>从开发集或者测试集中选一些分类器的错误数据（100个就足以）</span></p></li><li><p><span>同时评估几个分类器性能改进的想法</span></p><ul><li><span>比如说 ：狗狗图片误识别、大型猫科动物容易被误识别、模糊图片不好识别、</span></li><li><span>那上面的三个想法就是分类器的改进想法，那么你现在需要对这三个想法进行评估分析，看哪一个的理论上升空间更高</span></li></ul></li><li><p><span>画一张表格（电子的就行）</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230507145409629.png" referrerpolicy="no-referrer" alt="image-20230507145409629"></p><ul><li><span>第一列是图片编号，第二列是狗狗图片，第三列是大型猫科动物图片，第四列是模糊图片，第五列是备注（标注图片具体的错误点）</span></li><li><span>你要做的就是，把所有的图片看一遍，看看出错的图片属于哪个类型的，比如是狗狗图片，就可以在狗狗图片上打勾，写上注释“比特犬”，以此类推，把所有图片看完，你就可以计算三类图片占总错误类型的百分比了</span></li><li><span>如果你在看图片的过程中发现 一些社交软件的滤镜 也出现在了错误图片中，那你就需要在表格上加上这一可优化项”滤镜图片识别“，然后继续浏览图片、标记的过程</span></li><li><span>最后你会得到不同分类图片的占比，这样你就能知道优化哪种问题会给你的系统带来较大的性能提升（比如这个例子里模糊图片和大猫图片的占比分别是61%和43%，那优化他们俩的话对分类器的性能提升比较大，很值得优化）</span></li></ul></li></ul></li><li><h3 id='在做错误分析的时候有时你会注意到开发集里有些样本被错误标-记了这时应该怎么做呢我们下一个视频来讨论'><span>在做错误分析的时候，有时你会注意到开发集里有些样本被错误标 记了，这时应该怎么做呢？我们下一个视频来讨论</span></h3></li></ul><h2 id='22-清除标注错误的数据cleaning-up-incorrectly-labeled--data）'><span>2.2 清除标注错误的数据（Cleaning up Incorrectly labeled  data）</span></h2><ul><li><p><span>监督学习问题的数据由输入特征x和输出标签y组成。如果你查看了数据发现有的数据标签有问题，那么你是否应该修正它的？</span></p></li><li><h3 id='深度学习算法对随机误差很健壮'><span>深度学习算法对随机误差很健壮</span></h3><ul><li><span>如果是随机的、少量的</span><strong><span>标记错误的样本</span></strong><span>，那不会对深度学习产生什么影响，不管。</span></li></ul></li><li><p><span>比如不小心把一只白色的小狗标记成了小猫，这没什么问题。但是如果把所有的白色小狗都标记成了小猫，这就是是系统性的错误</span></p></li><li><h3 id='深度学习算法对系统性的错误就没那么健壮'><span>深度学习算法对系统性的错误就没那么健壮</span></h3><ul><li><p><span>那你此时应该去分析一下 标记错误的样本 是否会对分类结果产生严重的影响</span></p></li><li><p><span>分析的方法依旧是在错误分析中（Error analysis）在表格的右侧再增加一列，列名为“标记错误的样本”，然后等错误分析完成，你就可以得到标记错误样本在总错误样本中的占比了。</span></p></li><li><h3 id='然后你就需要通过三个指标来判断你是否需要修正这些数据'><span>然后你就需要通过三个指标来判断你是否需要修正这些数据</span></h3><ul><li><span>总错误率</span></li><li><span>标记错误的样本产生的错误率</span></li><li><span>其他错误产生的错误率</span></li></ul></li><li><p><span>例如：总错误率10%，在错误分析中分析到标记错误的样本占6%，经过计算 10% x 6% = 0.6%，也就是说总错误率的0.6%都是因为标记错误的样本，其他问题占总错误率的10% - 0.6% = 9.4%，所以 标签错误占 0.6% / 10% = 6%，其他错误占 9.4% / 10% = 94%，那优化其他错误是更好的选择，标记错误可以放一放。</span></p></li><li><p><span>再例如总错误率2%，标记错误占0.6%，其他错误占1.4%，计算得出 标记错误占总错误的0.6%/2% = 30%，其他错误占总错误的1.4%/2% = 70%，那么此时解决标记错误就是该做的事情了</span></p></li></ul></li><li><h3 id='有关修正不正确的开发测试集样本的三点建议'><span>有关修正不正确的开发/测试集样本的三点建议</span></h3><ul><li><p><span>如果你要去修正开发集上的数据，那也请对测试集做同样的修正，使它们的数据来自同一个分布</span></p></li><li><p><span>要考虑同时检验算法判断正确和判断错误的样本</span></p><ul><li><span>这一步很少人做，因为太麻烦了。查看一遍出错的样本很容易，因为量比较少。但如果查看判断正确的样本，工作量就太大了。</span></li><li><span>查看正确样本的目的是看里面是不是有判断错的</span></li></ul></li><li><p><span>训练集和开发/测试集的数据分布稍微不同是正常的，后面就讲如何处理</span></p></li></ul></li><li><p><span>NG的建议</span></p><ul><li><span>构建深度学习系统的时候需要人来帮它排查问题、错误，需要人去用自己的见解构建这个系统</span></li><li><span>去看看正确和错误的样本，虽然比较耗时间，但是很有用。它真的可以帮你找到 需要优先处理的任务</span></li></ul></li></ul><h2 id='23-快速搭建你的第一个系统并进行迭代build-your-first--system-quickly-then-iterate）'><span>2.3 快速搭建你的第一个系统，并进行迭代（Build your first  system quickly, then iterate）</span></h2><ul><li><p><span>这节课的标题就是这节课的核心内容</span></p></li><li><p><span>NG鼓励新手，在构建一个领域的新系统时，先去做一个初始系统，然后迭代。</span></p></li><li><h3 id='具体来说构建一个新系统时候的步骤）'><span>具体来说（构建一个新系统时候的步骤）</span></h3><ul><li><h3 id='快速设立开发集和测试集还有评估指标意义先设立目标）'><span>快速设立开发集和测试集还有评估指标（意义：先设立目标）</span></h3></li><li><h3 id='快速做一个初始系统用训练集训练一下意义可以看到你距离目标还差多少）然后再用开发集和测试集来评估当前系统的指标'><span>快速做一个初始系统，用训练集训练一下（意义：可以看到你距离目标还差多少），然后再用开发集和测试集来评估当前系统的指标</span></h3></li><li><p>&nbsp;</p></li><li><h3 id='使用偏差方差分析和错误分析意义看看具体是哪块需要优化和解决也就是确定你下一步要做什么）'><span>使用偏差、方差分析和错误分析（意义：看看具体是哪块需要优化和解决，也就是确定你下一步要做什么）</span></h3></li></ul></li></ul><h2 id='24-使用来自不同分布的数据进行训练和测试training-and--testing-on-different-distributions）'><span>2.4 使用来自不同分布的数据进行训练和测试（Training and  testing on different distributions）</span></h2><ul><li><p><span>深度学习的胃口很大，如果要训练他，就需要很多很多的数据。</span></p></li><li><p><span>一般来说训练集越大，网络的效果越好。这也就导致有的团队收集到了大量的数据，然后把它们堆在了训练集中，让训练的数据量更大，即使有些数据，甚至是大部分数据都来自和开发集、测试集不同的分布。</span></p></li><li><h3 id='总结-2'><span>总结：</span></h3><ul><li><h3 id='训练集的数据分布不管如何一定要含有开发集和测试集相同的数据分布'><span>训练集的数据分布不管如何，一定要含有开发集和测试集相同的数据分布</span></h3><ul><li><span>比如要做猫猫分类器，训练集的数据都是猫猫高清大图，但是开发集和测试集使用的图片是非专业人士拍摄的模糊的猫猫图片，如果这样做的话，因为分类器完全没见过模糊的猫猫图片，所以分类器的效果不会好。正确做法是，将开发集和测试集的数据也给训练集一些。</span></li><li><span>如开发集和测试集有2w张模糊猫猫图片，训练集是50万张清晰猫猫图片，由于你的系统以后大部分处理的图片都是模糊猫猫图片，所以你需要给训练集中加上模糊猫猫图片。所以应该给训练集分1万张模糊的图片，这样训练集是51万条数据，开发和测试各5000条</span></li></ul></li><li><h3 id='开发集和测试集的数据分布必须和你系统未来要面对的数据一样'><span>开发集和测试集的数据分布必须和你系统未来要面对的数据一样</span></h3><ul><li><span>系统最后要面对何种输入，就是开发集和测试集应该长得样子。</span></li><li><span>比如分类器最后主要是是来识别用户上传的模糊猫猫图片，那开发集和测试集应该全部都是这类图片。</span></li></ul></li></ul></li></ul><h2 id='25-数据分布不匹配时的偏差与方差的分析bias-and--variance-with-mismatched-data-distributions）'><span>2.5 数据分布不匹配时的偏差与方差的分析（Bias and  Variance with mismatched data distributions）</span></h2><ul><li><p><span>还记得偏差和方差分析不？</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230507224929780.png" referrerpolicy="no-referrer" alt="image-20230507224929780"></p></li><li><p><span>假设贝叶斯偏差约为0%，训练集1%，开发集10%，你可能会说这是高方差，存在过拟合。</span><strong><span>但这个判断方法的前提是训练集和开发集的数据分布相同</span></strong></p></li><li><p><span>而如果</span><strong><span>数据分布不同</span></strong><span>，就会导致两个可能的问题：</span></p><ol start='' ><li><p><span>算法在开发集上的表现本身不错，但是因为开发集的图片比训练集的图片更加的模糊、难以识别，那就会造成这样一种”高方差“的现象（开发集的数据分布和训练集不同）</span></p></li><li><p><span>训练集的数据特征和开发集的数据特征完全不一样，算法根本没见过开发集的这种数据，所以方差很大</span></p></li><li><h3 id='那开发集中增加的9的错误率你就很难判断出来究竟是因为开发集的数据分布和训练集不同导致的方差较大还是因为只是方差较大所以我们需要新的方法来分辨'><span>那开发集中增加的9%的错误率，你就很难判断出来究竟是因为开发集的数据分布和训练集不同导致的方差较大，还是因为只是方差较大，所以我们需要新的方法来分辨</span></h3></li></ol></li><li><h3 id='训练-开发集'><span>训练-开发集</span></h3><ul><li><span>为了解决上面的问题，分辨出到底是因为何种原因导致的方差较高，我们需要设立一个</span><strong><span>训练-开发集</span></strong></li><li><span>训练-开发集的数据从训练集中随机获取，也就是说训练-开发集的数据分布和训练集相同；开发集和测试集的数据分布相同</span></li><li><span>训练-开发集不参与学习算法，也就是不做反向传播</span></li></ul></li><li><h3 id='方法'><span>方法</span></h3><ul><li><p><span>为了找到究竟是何种原因导致的方差大，我们需要在训练集上训练数据，然后计算训练-开发集和开发集的误差</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508104356195.png" referrerpolicy="no-referrer" alt="image-20230508104356195"></p><ul><li><span>比如这个例子中，三者的误差分别是  1  9  10， 由于训练集和训练开发集来自同一个数据分布，出现这种问题，那就只有一个可能，尽管你的神经网络在训练集中表现良好，但无法泛化到来自相同分布的训练开发集里，它无法很好地泛化推广到来自同一分布，所以它</span><strong><span>存在过拟合问题，导致方差变大</span></strong><span>。</span></li><li><span>而如果三者误差是 1 1.5 10，这就说明可能不存在方差问题，而出现了</span><strong><span>数据分布不匹配</span></strong><span>的问题</span></li></ul><p><span>  </span></p><p><span>  </span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508104653003.png" referrerpolicy="no-referrer" alt="image-20230508104653003"></p><ul><li><span>这个例子中，贝叶斯偏差是0%，其他数据是10 11 12，很容易能看出来，可避免偏差太高，是一个</span><strong><span>欠拟合</span></strong><span>问题</span></li><li><span>如果数据是10 11 20，贝叶斯偏差还是0，那就说明存在 </span><strong><span>可避免偏差高和数据不匹配问题</span></strong></li></ul></li></ul></li></ul><ul><li><h3 id='现在有5个指标可以分析了'><span>现在有5个指标可以分析了</span></h3><h3><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508104952458.png" referrerpolicy="no-referrer" alt="image-20230508104952458"><span> </span></h3><p>&nbsp;</p><figure><table><thead><tr><th><span>对比组</span></th><th style='text-align:center;' ><span>可以计算</span></th></tr></thead><tbody><tr><td><span>贝叶斯偏差 - 训练集误差</span></td><td style='text-align:center;' ><span>可避免偏差</span></td></tr><tr><td><span>训练集误差 - 训练开发集误差</span></td><td style='text-align:center;' ><span>方差</span></td></tr><tr><td><span>训练开发集误差 - 开发集误差</span></td><td style='text-align:center;' ><span>数据不匹配</span></td></tr><tr><td><span>开发集误差 - 测试集误差</span></td><td style='text-align:center;' ><span>（开发集的）方差</span></td></tr></tbody></table></figure></li><li><h3 id='特例'><span>特例</span></h3><ul><li><p><span>如果你见到你的几个指标是这样的情况</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508105804979.png" referrerpolicy="no-referrer" alt="image-20230508105804979"></p><p><span>这就说明你的训练集处理的数据，可能比你开发集处理的数据要难，因为7 10这两个数据是对训练集和训练开发集的误差它俩来自同一个数据分布，而6 6两个数据来自开发集和测试集的误差它俩是一个数据分布。那么要如何分析呢？</span></p><p><span>有一个更加通用的办法 —— 画表</span></p></li></ul></li><li><p><span>通用分析方法</span></p></li><li><p>&nbsp;</p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508110050225.png" referrerpolicy="no-referrer" alt="image-20230508110050225"></p><ul><li><p><span>比如，你最终要做后视镜的语音识别，你的训练集数据是普通语音样本，开发集和训练集数据是后视镜语音样本，你需要这样做分析</span></p></li><li><p><span>表头一行，写上两种数据分布</span></p></li><li><p><span>表头一列分别写上：人类误差、在训练过的数据上的误差，在没训练过的数据上的误差</span></p><figure><table><thead><tr><th>&nbsp;</th><th><span>普通语音样本</span></th><th><span>后视镜语音样本</span></th></tr></thead><tbody><tr><td><span>人类误差</span></td><td><span>人类误差 4%</span></td><td><span>人类误差 6%</span></td></tr><tr><td><span>在训练过的数据上的误差</span></td><td><span>训练集误差 7%</span></td><td><span>包含了开发集数据分布的训练集误差6%</span></td></tr><tr><td><span>在没训练过的数据上的误差</span></td><td><span>训练-开发集误差 10%</span></td><td><span>开发集误差  6% 测试集误差 6%</span></td></tr></tbody></table></figure><ul><li><span>那么你从人类-训练集误差中能分析可避免偏差</span></li><li><span>从训练集-训练开发集误差中能分析方差</span></li><li><span>从训练开发集-开发集和测试集误差能分析数据不匹配问题</span></li></ul></li><li><p><span>通过这个表格你就能把神经网络中潜在的三个问题都分析出来了，不过NG直言，他基本上使用画红圈的部分就足以得出优化方向了</span></p></li></ul></li><li><p><span>方差和偏差知道怎么解决了，那数据不匹配问题怎么解决呢？很不幸，目前没有系统的方法去解决这个问题，但是有一些方法可以尝试，下节课讲</span></p></li></ul><p>&nbsp;</p><h2 id='26-处理数据不匹配问题addressing-data-mismatch）'><span>2.6 处理数据不匹配问题（Addressing data mismatch）</span></h2><ul><li><h2 id='可以尝试的事情'><span>可以尝试的事情</span></h2><ul><li><span>如果发现严重的数据不匹配问题》做错误分析》了解训练集和开发测试集具体的差异，同时技术上为了避免对测试集过拟合，应该针对开发集做错误分析，而不是测试集</span></li></ul></li><li><h3 id='几个建议'><span>几个建议</span></h3><ul><li><h3 id='自己做错误分析搞清楚训练和开发测试集的差异'><span>自己做错误分析，搞清楚训练和开发测试集的差异</span></h3><ul><li><span>比如训练集是清楚图片，开发测试集是模糊图片，这就是两者的不同</span></li><li><span>为了解决这个问题</span></li></ul></li><li><h3 id='让训练集的数据更加像开发集一些或者更多的收集类似开发集和测试集的数据'><span>让训练集的数据更加像开发集一些，或者更多的收集类似开发集和测试集的数据</span></h3><ul><li><span>手段：人工合成数据</span></li></ul></li></ul></li><li><h3 id='人工合成数据的优缺点'><span>人工合成数据的优缺点</span></h3><ul><li><span>优点：对比获取新的数据，合成数据更好得到</span></li><li><span>缺点：使得算法比较容易对合成数据过拟合</span></li></ul></li></ul><h2 id='27-迁移学习transfer-learning）'><span>2.7 迁移学习（Transfer learning）</span></h2><ul><li><h3 id='深度学习最强大的一个理念之一就是可以通过任务a学到一些知识并且将这些知识应用到任务b中比如学习如何识别猫就可以用于识别x光片这就是迁移学习'><span>深度学习最强大的一个理念之一就是可以通过任务A学到一些知识，并且将这些知识应用到任务B中，比如学习如何识别猫，就可以用于识别X光片，这就是迁移学习</span></h3></li><li><h3 id='怎么实现迁移学习'><span>怎么实现迁移学习？</span></h3><ul><li><span>比如做X光识别，那你可以先训练一个识别猫猫图的神经网络。</span></li><li><span>然后将它的输出层剔除，重新加入一个输出层，随机初始化该层的参数；再把数据源换成X光片，重新训练输出层就可以。</span></li></ul></li><li><h3 id='经验法则'><span>经验法则</span></h3><ul><li><span>如果你的数据集比较小，那你就训练输出层，或者输出+前一层</span></li><li><span>如果你的数据及比较大，可以训练所有的层</span></li></ul></li><li><p><span>预训练pre-training：先训练一个相同用途的神经网络</span></p></li><li><p><span>微调 fine tuning：然后更新一部分或者所有权重，使用需求数据来训练</span></p></li><li><h3 id='为什么迁移学习有用或者说为什么神经网络学到的一部分知识可以用在别的地方'><span>为什么迁移学习有用？或者说为什么神经网络学到的一部分知识可以用在别的地方？</span></h3><ul><li><span>比如神经网络之前学习了如何分辨猫猫图，那它就会对边缘检测、曲线检测有过一些了解，也就是说它学习到了一些低维度的特征，在之后训练其他应用时，这些就能派上用场</span></li><li><span>所以它就可以用之前学过的知识，让你使用一个比较小的数据集也能完成任务</span></li></ul></li><li><h3 id='迁移学习什么时候有用'><span>迁移学习什么时候有用？</span></h3><ul><li><p><span>任务A和任务B的输入都是一种东西，比如图像 、音频</span></p></li><li><p><span>任务B的训练集数据太少，而任务A的训练集数据多</span></p></li><li><p><span>任务A从训练集学习到的低维度特征可以用于任务B</span></p></li><li><h3 id='满足以上的三个条件迁移学习就可以提升神经网络的训练速度以及达到你要的效果但是反之就没啥增益了'><span>满足以上的三个条件迁移学习就可以提升神经网络的训练速度，以及达到你要的效果，但是反之就没啥增益了</span></h3></li></ul></li></ul><h2 id='28-多任务学习multi-task-learning）'><span>2.8 多任务学习（Multi-task learning）</span></h2><ul><li><p><span>迁移学习中进行的是串行学习，将任务A的知识应用于任务B</span></p></li><li><p><span>而多任务学习是并行的，所有的任务同时开始，然后希望每个任务都能帮到其他所有任务</span></p></li><li><h3 id='多任务学习案例------无人驾驶车'><span>多任务学习案例——无人驾驶车</span></h3><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508142836467.png" referrerpolicy="no-referrer" alt="image-20230508142836467"></p></li><li><p><span>比如你要做无人驾驶车，这里是一张图片，需要建立神经网络识别图像里是否有 车 人 交通标志 红绿灯，和之前的神经网络不同，之前的神经网络的输出标签是一个标量，而多任务学习的输出标签是一个向量，在这个例子中标签是一个(4,1)的列向量，每一个位置对应我们识别的东西。 比如 y^帽子 = [0 , 1 , 1 , 0]，第一个分量代表是否有车，第二个分量代表是否有人等。</span></p></li><li><h3 id='多任务学习和softmax分类器的不同点在于'><span>多任务学习和softmax分类器的不同点在于</span></h3><ul><li><span>softmax分类器它的输入图片大多是只有某一种分类标签在其中，然后让softmax找出它可能是哪一类的</span></li><li><span>多任务学习是同时在识别一张图片中是否有上述提到的四个元素，并且在输出中给出对应的回答（1或者0）</span></li></ul></li></ul></li><li><h3 id='多任务学习神经网络结构'><span>多任务学习神经网络结构</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508143534737.png" referrerpolicy="no-referrer" alt="image-20230508143534737"></p></li><li><h3 id='多任务学习神经网络损失函数'><span>多任务学习神经网络损失函数</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508143601951.png" referrerpolicy="no-referrer" alt="image-20230508143601951"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508143659719.png" referrerpolicy="no-referrer" alt="image-20230508143659719"></p><ul><li><span>和逻辑回归相比只有一个不同，就是y不是标量，而是一个列向量，因此要求每一个分量的误差</span></li></ul></li><li><h3 id='多任务学习的优点'><span>多任务学习的优点</span></h3><ul><li><h3 id='相同的任务一个比四个强'><span>相同的任务，一个比四个强</span></h3><ul><li><span>无人车的问题可以拆解为四个问题，你可以交给四个单独的神经网络去做。但是多任务神经网络的性能会比四个单独的神经网络好，因为多任务神经网络在训练过程中在共享图像识别的知识。</span></li></ul></li><li><h3 id='标记数据不全也能用'><span>标记数据不全也能用</span></h3><ul><li><span>哪怕某一张图片，人工并没有标记出所有的元素，比如一个图片中有车和人，  但是人工只标记出来了车、交通标志、信号灯，并没有标记图片中是否有人（数据标签Y中标记为问号的位置），这时候的多任务学习依旧有效，它计算损失函数时遇到没有标记的数据就会略过（只计算标签为0和 1的分量）</span></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508144114282.png" referrerpolicy="no-referrer" alt="image-20230508144114282"></li></ul></li></ul></li><li><h3 id='多任务学习啥时候有意义'><span>多任务学习啥时候有意义？</span></h3><ul><li><h3 id='几个任务之间有着类似的低维度特征比如输入都是图片都是音频之类的）'><span>几个任务之间有着类似的低维度特征（比如输入都是图片、都是音频之类的）</span></h3></li><li><h3 id='可能也就是并不准确）如果你想使用多任务学习增强某一个任务的性能那几个任务之间的数据量要基本一致'><span>可能（也就是并不准确）：如果你想使用多任务学习增强某一个任务的性能，那几个任务之间的数据量要基本一致</span></h3><ul><li><span>比如有100个任务，你想加强处理任务100的性能，那任务1 ~ 任务99的数据集的规模一定要比任务100的数据集大</span></li></ul></li><li><h3 id='你有足够的计算资源搞很大的神经网络'><span>你有足够的计算资源搞很大的神经网络</span></h3><ul><li><span>多任务学习只有在神经网络不够大的时候性能不好</span></li></ul></li></ul></li><li><h3 id='多任务学习没有迁移学习用的多并且多任务学习基本只用在计算机视觉中的物体识别'><span>多任务学习没有迁移学习用的多，并且多任务学习基本只用在计算机视觉中的物体识别</span></h3></li></ul><h2 id='29-什么是端到端的深度学习what-is-end-to-end-deep--learning）'><span>2.9 什么是端到端的深度学习？（What is end-to-end deep  learning?）</span></h2><ul><li><p><span>比如在一些任务中，你需要几个步骤处理输入特征x，才能映射为输出y。</span></p><ul><li><p><span>传统神经网络的语音识别转文字中：输入音频》手动建立声音特征》找音位》将音位组合在一起 构成单独的词》输出文本</span></p></li><li><p><span>端到端：输入音频》输出文本</span></p></li><li><h3 id='一句话端到端去掉了中间所有的步骤直接输入后输出'><span>一句话：端到端去掉了中间所有的步骤，直接输入后输出。</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508145947289.png" referrerpolicy="no-referrer" alt="image-20230508145947289"></p></li></ul></li><li><h3 id='ai-的其中一个有趣的社会学效应'><span>AI 的其中一个有趣的社会学效应</span></h3><ul><li><span>随着端到端学习的性能变好，一些投入了巨大时间和精力的设计中间流程步骤的研究人员相当难以接受，他们无法接受这样构建 AI 系统，因为有些情况，端到端方法完全取代了旧系统，某些投入了多年研究的中间组件也许已经过时了。</span></li></ul></li><li><h3 id='端对端学习的难点------大量数据'><span>端对端学习的难点——大量数据</span></h3><ul><li><p><span>如果你有3000小时的数据做语音识别，那传统的神经网络效果往往会更好</span></p></li><li><p><span>如果你的数据量中等，那你可以选择在神经网络中跳过一些步骤。比如低维度特征识别，而直接转向音节的识别，这在一定程度上也是在靠近端到端</span></p></li><li><p><span>如果你有1万小时，10万小时的数据做语音识别，那端对端的深度学习效果一下子就起来了</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508150246059.png" referrerpolicy="no-referrer" alt="image-20230508150246059"></p></li></ul></li><li><h3 id='不过今天大部分的应用还是无法使用端到端的深度学习来实现因为数据量不够'><span>不过今天大部分的应用还是无法使用端到端的深度学习来实现，因为数据量不够</span></h3><ul><li><h3 id='比如一个刷脸门禁'><span>比如一个刷脸门禁</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508150915038.png" referrerpolicy="no-referrer" alt="image-20230508150915038"></p><ul><li><p><span>这个任务是使用端到端的深度学习吗？今天的实现可能还不是，因为在刷脸的时候，人们的脸可能在不同的画面位置中，并且可能有大有小，收集这样的数据集做端到端的深度学习有点难。</span></p></li><li><p><span>所以实际上的实现是把任务拆解成了两个子任务，然后分别来实现的</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508151105117.png" referrerpolicy="no-referrer" alt="image-20230508151105117"></p><ul><li><span>首先在画面中识别那个是人脸，然后把人脸放大 裁剪，最后再比对。</span></li><li><span>也就是分成了识别人脸，和对人脸处理比对 两个任务</span></li><li><span>之所以分成两个任务主要是因为，使用端对端的话，数据量不够，而拆解成两个子任务的数据量是够用的，所以现在是这样做。</span></li></ul></li><li><p><span>端到端目前主要还是用在机器翻译中比较多，因为数据量足够，端到端的表现很好</span></p></li><li><h3 id='所以端到端深度学习系统是可行的它表现可以很好也可以简化系统架构让你不需要搭建那么多手工设计的单独组件但它也不是灵丹妙药并不是每次都能成功'><span>所以端到端深度学习系统是可行的，它表现可以很好，也可以简化系统架构，让你不需要搭建那么多手工设计的单独组件，但它也不是灵丹妙药，并不是每次都能成功</span></h3></li></ul></li></ul></li></ul><h2 id='210-是否要使用端到端的深度学习whether-to-use-end-to-end-learning）'><span>2.10 是否要使用端到端的深度学习？（Whether to use end-to-end learning?）</span></h2><ul><li><h3 id='端到端的深度学习的优缺点'><span>端到端的深度学习的优缺点</span></h3><ul><li><p><span>优点</span></p><ul><li><p><span>让数据说话</span></p><ul><li><span>因为给大量的数据让神经网络自己去学习其中的内涵</span></li></ul></li><li><p><span>手工设计的步骤更少，简化神经网络工作流程</span></p></li></ul></li><li><p><span>缺点</span></p><ul><li><p><span>需要大量的数据来确保神经网络能构建一个复杂的函数</span></p></li><li><p><span>它排除了可能有用的手工设计组件</span></p><ul><li><p><span>有一些研究员非常鄙视手工设计的东西，但是实际应用来看，在数据量不大的时候，使用手工设计的步骤可以给算法注入人类的理解，帮助它在数据量少的时候也有很好的效果</span></p></li><li><p><span>NG觉得这并不总是一件坏事，他觉得神经网络的学习来源就俩东西：数据和手工设计的任何东西。数据量比较小的时候，神经网络可能无法洞察数据之间的内涵，但是人为的去帮助它确实可以给它带来比较好的效果</span></p><ul><li><span>数据量大的时候就无所谓了</span></li></ul></li></ul></li></ul></li></ul></li></ul><p>&nbsp;</p><h1 id='第四门课-卷积神经网络convolutional--neural-networks）'><span>第四门课 卷积神经网络（Convolutional  Neural Networks） </span></h1><h1 id='第一周-卷积神经网络foundations-of-convolutional-neural--networks）'><span>第一周 卷积神经网络（Foundations of Convolutional Neural  Networks） </span></h1><h2 id='11-计算机视觉computer-vision）'><span>1.1 计算机视觉（Computer vision）</span></h2><ul><li><h3 id='计算机视觉的应用'><span>计算机视觉的应用：</span></h3><ul><li><p><span>图像识别</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508161827525.png" referrerpolicy="no-referrer" alt="image-20230508161827525"></p></li><li><p><span>物体检测</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508161833818.png" referrerpolicy="no-referrer" alt="image-20230508161833818"></p></li><li><p><span>图片风格迁移</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508161900836.png" referrerpolicy="no-referrer" alt="image-20230508161900836"></p></li></ul></li><li><h3 id='计算机视觉的其中一个挑战是数据的输入可能会非常大'><span>计算机视觉的其中一个挑战：是数据的输入可能会非常大</span></h3><ul><li><span>计算机视觉中，你肯定不希望系统只能识别小的图片，而是希望它可以识别很大的图片，那大的图片就代表着矩阵的维度也会很大，比如一张64x64的彩色图片，它使用了64 x 64 x 3 = 12288 个数据来存储，如果是1000x1000的图片呢？那就要用300百万个数据来存储</span></li><li><span>将这样的图片转换为输入特征向量的话那它就是一个3000000,1的列向量。那第一个隐层（假设n_1 = 1000）对应的参数W^[1]的维度就是(1000, 3m)，但是直接这样计算的话机器基本受不了，没法开展工作，所以卷积神经网络的卷积计算就出现了</span></li></ul></li></ul><h2 id='12-边缘检测示例edge-detection-example）'><span>1.2 边缘检测示例（Edge detection example）</span></h2><ul><li><h3 id='卷积计算是卷积神经网络最基本的组成部分'><span>卷积计算是卷积神经网络最基本的组成部分</span></h3></li><li><p><span>这节课使用一个垂直线的边缘检测来说明卷积是如何计算的</span></p></li><li><p><span>之前课看到一个人脸识别的过程</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508191515981.png" referrerpolicy="no-referrer" alt="image-20230508191515981"></p><ul><li><span>先是边缘，然后是部位，最后是一张张人脸</span></li><li><span>所以计算机视觉是从边缘开始的</span></li></ul></li><li><h3 id='从边缘开始'><span>从边缘开始</span></h3><p><span>如果我们给电脑一张图片，让它识别这是什么，那第一步就是提取照片的边缘</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508191635123.png" referrerpolicy="no-referrer" alt="image-20230508191635123"></p><ul><li><span>可以提取它的</span><strong><span>垂直边缘和水平边缘</span></strong><span>，那我们是如何检测的呢？</span><strong><span>卷积计算</span></strong></li></ul></li><li><h3 id='卷积运算'><span>卷积运算</span></h3><ul><li><p><span>比如有一张6x6的灰度图像，我们要检测它的垂直边缘，那我们让它和一个3x3大小的过滤器（也叫“核”）做卷积运算。</span></p></li><li><p><span>*就是卷积的标志，不过</span><span>*</span><span>是一个重载符号，它可以表示乘法或者元素乘法，后面表示卷积的话，它会被注明</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508192031087.png" referrerpolicy="no-referrer" alt="image-20230508192031087"></p></li><li><p><span>两个矩阵做卷积运算会产生一个新的矩阵，大小为4x4。</span></p></li><li><p><span>新的矩阵的每一个元素是图片和过滤器做卷积计算后得到的</span></p></li><li><p><span>卷积计算就是将图片上一块和过滤器大小一样的区域做元素乘积运算，结果放入新的矩阵中</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508192507196.png" referrerpolicy="no-referrer" alt="image-20230508192507196"></p><p><span>新矩阵第一个格子 = 3 x 1 + 1 x 1 + 2 x 1 + 0 x 0 + 0 x 5 + 0 x 7 + 1 x -1 + 8 x -1 + 2 x -1 = -5，结果填入格子中</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508192813063.png" referrerpolicy="no-referrer"></p><p><span>过滤器右移一格，继续做卷积运算，0 x 1 + 5 x 1 + 7 x 1 + 1 x 0 + 8 x 0 + 2 x 0 + 2 x -1 + 9 x -1 + 5 x -1 = - 4</span></p><p><span>继续过滤器右移一格</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508192843802.png" referrerpolicy="no-referrer" alt="image-20230508192843802"></p><p><span>继续计算，直到把整个新矩阵填满，最后得到</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508192914117.png" referrerpolicy="no-referrer" alt="image-20230508192914117"></p><h3 id='卷积计算完成'><span>卷积计算完成</span></h3></li></ul></li><li><h3 id='编程中实现卷积计算'><span>编程中实现卷积计算</span></h3><ul><li><span>在编程练习中，你会使用一个叫 conv_forward 的 函数</span></li><li><span>如果在 tensorflow 下，这个函数叫 tf.conv2d</span></li><li><span>在其他深度学习框架 Keras 中，在这个框架下用 Conv2D 实现卷积运算</span></li><li><span>所有的编程框架 都有一些函数来实现卷积运算</span></li></ul></li><li><h3 id='这些矩阵实际代表的东西'><span>这些矩阵实际代表的东西</span></h3><ul><li><span>那这些矩阵实际代表着什么呢？</span></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508194511614.png" referrerpolicy="no-referrer" alt="image-20230508194511614"></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508200151761.png" referrerpolicy="no-referrer" alt="image-20230508200151761"></li></ul></li></ul><h2 id='13-更多边缘检测内容more-edge-detection）'><span>1.3 更多边缘检测内容（More edge detection） </span></h2><ul><li><p><span>你已经见识到用卷积运算实现垂直边缘检测，本节课你将学习如何区分正边负边</span></p><ul><li><span>正边：由亮过度到暗的边</span></li><li><span>负边：由暗过度到亮的边</span></li></ul></li><li><h3 id='正边负边'><span>正边负边</span></h3><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230508222230055.png" referrerpolicy="no-referrer" alt="image-20230508222230055"></p><ul><li><span>这是两张图片，他们的区别在于亮边和暗边的位置调换了一下，使用相同的垂直边缘检测滤波器进行卷积运后，得到两张结果图，两张结果图表明了有亮到暗，由暗到亮两种情况对于垂直边缘检测滤波器的影响。</span></li><li><span>如果你不在乎这种亮、暗的区别，你可以对结果矩阵取绝对值，虽然这个滤波器可以给我们区分亮和暗</span></li></ul></li></ul></li><li><h3 id='垂直边缘检测过滤器和水平边缘检测过滤器'><span>垂直边缘检测过滤器和水平边缘检测过滤器</span></h3><ul><li><p><span>我们已经见过这个3x3的垂直边缘检测器了，同样，水平边缘检测器如下（是垂直边缘检测器的转置矩阵）</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509152359477.png" referrerpolicy="no-referrer" alt="image-20230509152359477"></p></li></ul></li><li><h3 id='一个更加复杂的例子'><span>一个更加复杂的例子</span></h3><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509152547255.png" referrerpolicy="no-referrer" alt="image-20230509152547255"></p><ul><li><p><span>矩阵中画绿圈的30，就是一条正边，它的上面比较亮，下面比较暗</span></p></li><li><p><span>画紫色的-30，就是一条负边，它的上面比较暗，下面比较亮</span></p></li><li><p><span>而中间画圈的10则是，左面两列是正边，右面一列是负边，正负边相加在一起得到了一个中间值10,。</span></p><ul><li><span>现在我们的图片比较小，才会得到亮度为10的过渡带，如果是1000x1000的图，这些中间值就会变的很小了</span></li></ul></li></ul></li></ul></li><li><h3 id='不同的过滤器'><span>不同的过滤器</span></h3><ul><li><p><span>之前计算机视觉的文献中探讨过，究竟怎样的数字组合才是最好的，有以下几个代表</span></p></li><li><h3 id='sobel滤波器'><span>Sobel滤波器</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509153341962.png" referrerpolicy="no-referrer" alt="image-20230509153341962"></p><ul><li><span>优点：增加了中间一行元素的权重，使结果的鲁棒性更好</span></li></ul></li><li><h3 id='scharr滤波器'><span>Scharr滤波器</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509153513680.png" referrerpolicy="no-referrer" alt="image-20230509153513680"></p></li><li><p><span>上述这两种滤波器都可以通过转置来实现水平边缘检测</span></p></li></ul></li><li><h3 id='时代变了'><span>时代变了</span></h3><ul><li><p><span>不过现在随着深度学习的发展，更加好的方法是，你把滤波器的9个分量都设置为参数，然后做反向传播。</span></p></li><li><p><span>神经网络就可以学习你的图片，检测出更加复杂的边缘，它可以检测出45°或70°或73°，甚至是任何角度的边缘，这种滤波器对于数据的捕捉能力甚至可以胜过任何之前这些手写的过滤器</span></p></li><li><h3 id='所以这种将这-9-个数字当成参数的思想已经成为计算机视觉中最为有效的思想之一'><span>所以这种将这 9 个数字当成参数的思想，已经成为计算机视觉中最为有效的思想之一</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509160334352.png" referrerpolicy="no-referrer" alt="image-20230509160334352"></p></li></ul></li></ul><h2 id='14-padding'><span>1.4 Padding</span></h2><ul><li><h3 id='为了构建深度神经网络你需要学会使用的一个基本的卷积操作就是-padding让我们-来看看它是如何工作的'><span>为了构建深度神经网络，你需要学会使用的一个基本的卷积操作就是 padding，让我们 来看看它是如何工作的。</span></h3></li><li><h3 id='在之前的视频中我们看到一个6x6和3x3的图像做卷积计算最后会得到一个4x4的图像这个结果是怎么来的'><span>在之前的视频中我们看到，一个6x6和3x3的图像做卷积计算，最后会得到一个4x4的图像。这个结果是怎么来的？</span></h3><ul><li><span>如果图像是 (n x n) 大小，过滤器是 (f x f)大小，那最后结果为 (n - f + 1 , n - f + 1)</span></li></ul></li><li><h3 id='之前的计算方法面临着两个缺点'><span>之前的计算方法面临着两个缺点</span></h3><ol start='' ><li><p><span>每一次做卷积操作，图像都会越来越小</span></p><ul><li><p><span>比如一个100层网络，经过这么多步卷积计算，最后会变成一个特别特别小的图片</span></p><p>&nbsp;</p></li></ul></li><li><p><span>图片边缘的像素利用的还是太少，这样会导致你丢失图片边缘位置的许多信息</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509165018378.png" referrerpolicy="no-referrer" alt="image-20230509165018378"></p><ul><li><span>绿色的部分总共会被使用一次，而中间红色的部分会使用很多次</span></li></ul></li></ol></li><li><h3 id='padding'><span>Padding</span></h3><ul><li><p><span>为了解决上述两个问题：图片越来越小，边缘没怎么被使用</span></p></li><li><p><span>我们使用padding来解决</span></p></li><li><h3 id='所谓padding就是在卷积之前给原始图像的周围填补n个像素用符号-p来表示'><span>所谓padding就是在卷积之前，给原始图像的周围填补n个像素，用符号 p来表示</span></h3></li><li><h3 id='p--1就是给图像周围补一圈元素结果如下图'><span>p = 1，就是给图像周围补一圈元素，结果如下图</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509182248886.png" referrerpolicy="no-referrer" alt="image-20230509182248886"></p></li><li><p><span>这样做之后，如果使用3x3的滤波器进行卷积之后，就会输出一个6x6的图像，而且边缘数据也被较好的利用起来了</span></p></li><li><h3 id='计算公式为-n--2---f--1-n--2p---f--1最后结果是66'><span>计算公式为 (n + 2 - f + 1, n + 2p - f + 1)，最后结果是(6,6)</span></h3></li><li><h3 id='一般来说填充数字0'><span>一般来说，填充数字0</span></h3></li><li><p><span>刚才的例子p=1，如果p=2那就是填充两圈了</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509182523761.png" referrerpolicy="no-referrer" alt="image-20230509182523761"></p></li></ul></li><li><h3 id='填补多少元素的选择------valid卷积和same卷积'><span>填补多少元素的选择——Valid卷积和Same卷积</span></h3><ul><li><p><span>Valid卷积（不填充）</span></p><ul><li><span>给一个 (n,n)的图像，使用 (f, f)的过滤器做卷积，得到(n - f + 1, n - f + 1)结果，就像是之前的那样</span></li></ul></li><li><p><span>Same卷积（填充）</span></p><ul><li><p><span>给一个(n,n)的图像，你想让输出和输入图像的大小一样，那你使用padding，通过(f,f)的过滤器做卷积，最后获得(n + 2p - f + 1, n + 2p - f + 1)大小的图像</span></p></li><li><p><span>那么填充多少你就可以算出来</span></p></li><li><p><span>n + 2p - f + 1 = n</span>
<span>p = (f - 1) / 2</span></p></li><li><h3 id='f基本是奇数很少见偶数的'><span>f基本是奇数，很少见偶数的</span></h3><ul><li><p><span>两个原因：</span></p><ul><li><span>如果 f 是偶数的，那我们很难找到过滤器的中心点</span></li><li><span>并且我们不能采用对称填充（上下左右都填充元素），只能通过非对称填充（比如就填充左面，不填充右面）</span></li></ul></li></ul></li></ul></li></ul><h2 id='15-卷积步长strided-convolutions）'><span>1.5 卷积步长（Strided convolutions）</span></h2><ul><li><p><span>卷积中的</span><strong><span>步幅</span></strong><span>是另一个构建卷积神经网络的基本操作，让我向你展示一个例子。</span></p></li><li><p><span>步幅是指每次过滤器移动的距离，用s表示</span></p></li><li><p><span>之前s=1，假如 s = 2，那就是这样移动</span></p><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509183424612.png" referrerpolicy="no-referrer" alt="image-20230509183424612"></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509183434209.png" referrerpolicy="no-referrer" alt="image-20230509183434209"></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509183441378.png" referrerpolicy="no-referrer" alt="image-20230509183441378"></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509183450677.png" referrerpolicy="no-referrer" alt="image-20230509183450677"></li></ul></li></ul></li><li><h3 id='如果步幅不是1那我们之前求结果维度的公式也需要改变'><span>如果步幅不是1，那我们之前求结果维度的公式也需要改变</span></h3><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n3106" cid="n3106" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="34.487ex" height="4.699ex" role="img" focusable="false" viewBox="0 -1381 15243.3 2077" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -1.575ex;"><defs><path id="MJX-13-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-13-TEX-I-1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path id="MJX-13-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-13-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-13-TEX-I-1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path><path id="MJX-13-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-13-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path id="MJX-13-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path id="MJX-13-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-13-TEX-N-A0" d=""></path><path id="MJX-13-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-13-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><use data-c="28" xlink:href="#MJX-13-TEX-N-28"></use></g><g data-mml-node="mfrac" transform="translate(389,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-13-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(822.2,0)"><use data-c="2B" xlink:href="#MJX-13-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1822.4,0)"><use data-c="32" xlink:href="#MJX-13-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(2322.4,0)"><use data-c="1D45D" xlink:href="#MJX-13-TEX-I-1D45D"></use></g><g data-mml-node="mo" transform="translate(3047.7,0)"><use data-c="2212" xlink:href="#MJX-13-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(4047.9,0)"><use data-c="1D453" xlink:href="#MJX-13-TEX-I-1D453"></use></g></g><g data-mml-node="mi" transform="translate(2284.4,-686)"><use data-c="1D460" xlink:href="#MJX-13-TEX-I-1D460"></use></g><rect width="4797.9" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(5649.1,0)"><use data-c="2B" xlink:href="#MJX-13-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(6649.3,0)"><use data-c="31" xlink:href="#MJX-13-TEX-N-31"></use></g><g data-mml-node="mtext" transform="translate(7149.3,0)"><use data-c="A0" xlink:href="#MJX-13-TEX-N-A0"></use></g><g data-mml-node="mo" transform="translate(7399.3,0)"><use data-c="2C" xlink:href="#MJX-13-TEX-N-2C"></use></g><g data-mml-node="mtext" transform="translate(7844,0)"><use data-c="A0" xlink:href="#MJX-13-TEX-N-A0"></use></g><g data-mml-node="mfrac" transform="translate(8094,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><use data-c="1D45B" xlink:href="#MJX-13-TEX-I-1D45B"></use></g><g data-mml-node="mo" transform="translate(822.2,0)"><use data-c="2B" xlink:href="#MJX-13-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(1822.4,0)"><use data-c="32" xlink:href="#MJX-13-TEX-N-32"></use></g><g data-mml-node="mi" transform="translate(2322.4,0)"><use data-c="1D45D" xlink:href="#MJX-13-TEX-I-1D45D"></use></g><g data-mml-node="mo" transform="translate(3047.7,0)"><use data-c="2212" xlink:href="#MJX-13-TEX-N-2212"></use></g><g data-mml-node="mi" transform="translate(4047.9,0)"><use data-c="1D453" xlink:href="#MJX-13-TEX-I-1D453"></use></g></g><g data-mml-node="mi" transform="translate(2284.4,-686)"><use data-c="1D460" xlink:href="#MJX-13-TEX-I-1D460"></use></g><rect width="4797.9" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(13354.1,0)"><use data-c="2B" xlink:href="#MJX-13-TEX-N-2B"></use></g><g data-mml-node="mn" transform="translate(14354.3,0)"><use data-c="31" xlink:href="#MJX-13-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(14854.3,0)"><use data-c="29" xlink:href="#MJX-13-TEX-N-29"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo stretchy="false">(</mo><mfrac><mrow><mi>n</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>f</mi></mrow><mi>s</mi></mfrac><mo>+</mo><mn>1</mn><mtext>&nbsp;</mtext><mo>,</mo><mtext>&nbsp;</mtext><mfrac><mrow><mi>n</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>f</mi></mrow><mi>s</mi></mfrac><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container></div></div><p>&nbsp;</p></li></ul><p><span>所以例子中，如果 图像是 7 x 7的，过滤器是3 x 3的，使用valid卷积</span></p><p><span>那结果图像的维度应该是( 7 + 0 - 3) / 2 + 1 = 3</span></p><ul><li><h3 id='如果商并不是整数怎么办'><span>如果商并不是整数怎么办？</span></h3><ul><li><p><span>如果刚才的公式算出来不是整数，那么向下取整</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509183755946.png" referrerpolicy="no-referrer" alt="image-20230509183755946"></p></li><li><p><span>表示为</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509183830856.png" referrerpolicy="no-referrer" alt="image-20230509183830856"></p></li><li><h3 id='这样做的原因是卷积运算只会对过滤器可以完全包含的图像内容无论是原始的还是填充过的）进行如果无法做到这点那就不会进行卷积运算'><span>这样做的原因是：卷积运算只会对过滤器可以完全包含的图像内容（无论是原始的还是填充过的）进行。如果无法做到这点，那就不会进行卷积运算。</span></h3><ul><li><span>这种情况就不会进行卷积</span><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509184006478.png" referrerpolicy="no-referrer" alt="image-20230509184006478"></li></ul></li></ul></li><li><h3 id='这里有一个关于互相关和卷积的技术性建议'><span>这里有一个关于互相关和卷积的技术性建议</span></h3><ul><li><h3 id='卷积在数学教材中的定义是矩阵a和b做卷积运算先对矩阵b进行顺时针90度旋转再做水平旋转得到新的矩阵b然后再做两个矩阵之间的元素乘积求和'><span>卷积在数学教材中的定义是：矩阵A和B做卷积运算，先对矩阵B进行顺时针90度旋转，再做水平旋转，得到新的矩阵B，然后再做两个矩阵之间的元素乘积求和</span></h3></li><li><h3 id='而在深度学习中我们做的实际上叫做互相关cross-correlation）不叫卷积也就是我们之前的做法直接两个矩阵元素乘积求和但是在深度学习中由于这个翻转操作在深度学习中用处不大并且还要多代码因此省略了这个双重镜像操作简化了代码并使神经网络也能正常工作因此我们根据惯例我们大多数人都叫它卷积尽管数学家们更喜欢称之为互相关但这不会影响到你在编程练习中要实现的任何东西也不会影响你阅读和理解深度学习文献'><span>而在深度学习中，我们做的实际上叫做</span><strong><span>互相关（cross-correlation）</span></strong><span>不叫卷积。也就是我们之前的做法，直接两个矩阵元素乘积求和。但是在深度学习中，由于这个翻转操作，在深度学习中用处不大，并且还要多代码，因此省略了这个双重镜像操作，简化了代码，并使神经网络也能正常工作。</span><strong><span>因此我们根据惯例，我们大多数人都叫它卷积，尽管数学家们更喜欢称之为互相关</span></strong><span>，但这不会影响到你在编程练习中要实现的任何东西，也不会影响你阅读和理解深度学习文献</span></h3></li></ul></li><li><p><span>现在我们已经解决在一个矩阵中做卷积操作，下一节课我们来看如何对一个立方体做卷积操作</span></p></li></ul><h2 id='16-三维卷积convolutions-over-volumes）'><span>1.6 三维卷积（Convolutions over volumes）</span></h2><p><span>你已经知道如何对二维图像（灰度图像）做卷积了，现在看看如何执行卷积不仅仅在二维图像上，而 是三维立体（彩色图）上。</span></p><ul><li><h3 id='立方体之间的卷积'><span>立方体之间的卷积</span></h3><ul><li><span>如果做立方体，也就是彩色图的卷积，那么我们不能直接像之前一样。</span></li></ul></li></ul><ul><li><span>如果彩色图是6 x 6 x 3的，那么三个数分别对应</span><strong><span>高 宽 通道数</span></strong></li></ul><ul><li><p><span>因此，我们使用的过滤器也需要变成一个立方体， 这个例子中我们使用一个 3 x 3  x 3的过滤器。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509190105520.png" referrerpolicy="no-referrer" alt="image-20230509190105520"></p></li></ul><ul><li><h3 id='注意图像和过滤器的通道数必须一样过滤器的高和宽可以和图像不一样'><span>注意：图像和过滤器的通道数必须一样，过滤器的高和宽可以和图像不一样</span></h3></li><li><h3 id='最后的输出是一个--4x4的图像我们来了解一下细节'><span>最后的输出是一个  4x4的图像，我们来了解一下细节</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509190149531.png" referrerpolicy="no-referrer" alt="image-20230509190149531"></p></li><li><h3 id='立方体的卷积'><span>立方体的卷积</span></h3><ul><li><p><span>为了简化表示，我们将这3个过滤器，变成一个立方体</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509190609315.png" referrerpolicy="no-referrer" alt="image-20230509190609315"></p><ul><li><p><span>立方体的卷积过程和平面图的其实一样</span></p><ol start='' ><li><p><span>首先我们取过滤器中的27个数（3 x 3 x 3)，把它放在图像的左上角</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509190720369.png" referrerpolicy="no-referrer" alt="image-20230509190720369"></p></li><li><p><span>让我们过滤器第一个层的9个数，和图像中红色通道的9个数做卷积运算，再第二个层的9个数和绿色通道，再第三个层的9个数和蓝色通道。依次对这27对数做卷积运算（元素相乘再求和），最后就可以得到结果矩阵中的第一个数</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509191001096.png" referrerpolicy="no-referrer" alt="image-20230509191001096"></p></li><li><p><span>由于步长是1，s = 1，所以过滤器往右动一格子，再重复2的操作</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509191113698.png" referrerpolicy="no-referrer" alt="image-20230509191113698"></p></li><li><p><span>一直重复2 3的操作，直到计算完成</span></p></li><li><p><span>得到结果矩阵 4 x 4 </span></p></li></ol></li></ul></li></ul></li><li><h3 id='那这个过滤器能干什么用呢'><span>那这个过滤器能干什么用呢？</span></h3><ul><li><h3 id='如果你只是想求红色通道中的垂直边缘那么你把过滤器的第一层设置为'><span>如果你只是想求红色通道中的垂直边缘，那么你把过滤器的第一层设置为</span></h3><h3 id='其他两层设置为全0那么你就得到了一个只检测红色通道垂直边缘的过滤器'><span>其他两层设置为全0，那么你就得到了一个只检测红色通道垂直边缘的过滤器</span></h3></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509191320917.png" referrerpolicy="no-referrer" alt="image-20230509191320917"></p></li><li><h3 id='如果你不关心垂直边缘在那个通道里那就把三个过滤器都设置为图上的样子这样你就得到了对任意颜色通道检测垂直边缘的过滤器'><span>如果你不关心垂直边缘在那个通道里，那就把三个过滤器都设置为图上的样子，这样你就得到了对任意颜色通道检测垂直边缘的过滤器</span></h3></li></ul></li><li><h3 id='使用多个过滤器的例子'><span>使用多个过滤器的例子</span></h3><ul><li><p><span>如果对一个彩色图像，我又想做垂直边缘检测，又想做水平边缘检测怎么办呢？</span></p></li><li><p><span>那就可以对同一个彩色图像使用多个过滤器，比如我们使用一个垂直边缘检测滤波器，使用一个水平边缘检测滤波器</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509192056922.png" referrerpolicy="no-referrer" alt="image-20230509192056922"></p></li><li><p><span>这样就会得到两个结果矩阵，最后将这两个结果矩阵也叠在一起，就变成了 4 x  4 x 2的立方体，这里的2并不是通道数，</span><strong><span>而是过滤器数</span></strong></p></li></ul></li><li><h3 id='计算公式'><span>计算公式</span></h3><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509192809305.png" referrerpolicy="no-referrer" alt="image-20230509192809305"></p></li><li><h3 id='如果有-n-x-n-x-n𝑐-的图像这里n𝑐代表通道数一些文献叫深度）然后卷积上一个-f-x-f-x-n𝑐的过滤器你就可以得到一个-n---f--1-x-n---f--1-x-n𝑐-的立方体n𝑐--就是下一层的通道数也就是你用的过滤器的个数'><span>如果有 n x n x n𝑐 的图像，这里n𝑐代表通道数（一些文献叫深度），然后卷积上一个 f x f x n𝑐的过滤器，你就可以得到一个 (n - f + 1) x (n - f + 1) x n𝑐′ 的立方体。n𝑐 ′ 就是下一层的通道数，也就是你用的过滤器的个数。</span></h3><ul><li><h3 id='nc必须相同'><span>n_c必须相同</span></h3></li><li><p><span>注意，这个例子中并没有padding，而且步幅 s = 1，如果用了不同的步幅或者padding，这个 n - f + 1数值会变化，比如变成 n + 2p - f + 1，或者是 (n + 2p - f + 1) / s </span></p></li></ul></li></ul></li><li><p><span>对立方体卷积的概念非常有用，你现在学会了如何用立方体做卷积</span></p></li><li><p><span>更重要的是，你可以检测多个特征，比如上面的例子检测了2个特征，垂直边缘和水平边缘。之后你还可以检测10个，上百个特征，输出的通道数就等于你要检测的特征数（过滤器数）</span></p></li></ul><h2 id='17-单层卷积网络one-layer-of-a-convolutional-network）'><span>1.7 单层卷积网络（One layer of a convolutional network）</span></h2><ul><li><p><span>今天我们要讲的是如何构建卷积神经网络的卷积层，下面来看个例子。</span></p></li><li><p><span>在标准的神经网络中，一个隐层的前向传播是不是分为两步？</span></p></li><li><div contenteditable="false" spellcheck="false" class="mathjax-block md-end-block md-math-block md-rawblock" id="mathjax-n3224" cid="n3224" mdtype="math_block" data-math-tag-before="0" data-math-tag-after="0" data-math-labels="[]"><div class="md-rawblock-container md-math-container" tabindex="-1"><mjx-container class="MathJax" jax="SVG" display="true" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="21.144ex" height="9.925ex" role="img" focusable="false" viewBox="0 -2443.3 9345.5 4386.7" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -4.397ex;"><defs><path id="MJX-14-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path id="MJX-14-TEX-N-5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path id="MJX-14-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-14-TEX-N-5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path><path id="MJX-14-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path id="MJX-14-TEX-I-1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path id="MJX-14-TEX-N-2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path><path id="MJX-14-TEX-I-1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path id="MJX-14-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path id="MJX-14-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path id="MJX-14-TEX-I-1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path id="MJX-14-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path><path id="MJX-14-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-14-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtable"><g data-mml-node="mtr" transform="translate(0,1500)"><g data-mml-node="mtd"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-14-TEX-I-1D467"></use></g><g data-mml-node="TeXAtom" transform="translate(498,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-14-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="31" xlink:href="#MJX-14-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-14-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(1572.5,0)"><use data-c="3D" xlink:href="#MJX-14-TEX-N-3D"></use></g><g data-mml-node="msup" transform="translate(2628.3,0)"><g data-mml-node="mi"><use data-c="1D44A" xlink:href="#MJX-14-TEX-I-1D44A"></use></g><g data-mml-node="TeXAtom" transform="translate(1136.2,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-14-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="31" xlink:href="#MJX-14-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-14-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(4783.4,0)"><use data-c="2217" xlink:href="#MJX-14-TEX-N-2217"></use></g><g data-mml-node="msup" transform="translate(5505.6,0)"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-14-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-14-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="30" xlink:href="#MJX-14-TEX-N-30"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-14-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(7086.5,0)"><use data-c="2B" xlink:href="#MJX-14-TEX-N-2B"></use></g><g data-mml-node="msup" transform="translate(8086.8,0)"><g data-mml-node="mi"><use data-c="1D44F" xlink:href="#MJX-14-TEX-I-1D44F"></use></g><g data-mml-node="TeXAtom" transform="translate(462,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-14-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="31" xlink:href="#MJX-14-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-14-TEX-N-5D"></use></g></g></g></g></g><g data-mml-node="mtr" transform="translate(0,0)"><g data-mml-node="mtd" transform="translate(4672.7,0)"></g></g><g data-mml-node="mtr" transform="translate(0,-1693.3)"><g data-mml-node="mtd" transform="translate(1636.9,0)"><g data-mml-node="msup"><g data-mml-node="mi"><use data-c="1D44E" xlink:href="#MJX-14-TEX-I-1D44E"></use></g><g data-mml-node="TeXAtom" transform="translate(562,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-14-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="31" xlink:href="#MJX-14-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-14-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(1636.5,0)"><use data-c="3D" xlink:href="#MJX-14-TEX-N-3D"></use></g><g data-mml-node="msup" transform="translate(2692.3,0)"><g data-mml-node="mi"><use data-c="1D454" xlink:href="#MJX-14-TEX-I-1D454"></use></g><g data-mml-node="TeXAtom" transform="translate(510,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-14-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="31" xlink:href="#MJX-14-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-14-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(3999,0)"><use data-c="28" xlink:href="#MJX-14-TEX-N-28"></use></g><g data-mml-node="msup" transform="translate(4388,0)"><g data-mml-node="mi"><use data-c="1D467" xlink:href="#MJX-14-TEX-I-1D467"></use></g><g data-mml-node="TeXAtom" transform="translate(498,413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use data-c="5B" xlink:href="#MJX-14-TEX-N-5B"></use></g><g data-mml-node="mn" transform="translate(278,0)"><use data-c="31" xlink:href="#MJX-14-TEX-N-31"></use></g><g data-mml-node="mo" transform="translate(778,0)"><use data-c="5D" xlink:href="#MJX-14-TEX-N-5D"></use></g></g></g><g data-mml-node="mo" transform="translate(5682.7,0)"><use data-c="29" xlink:href="#MJX-14-TEX-N-29"></use></g></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable rowspacing=".5em" columnspacing="1em" displaystyle="true"><mtr><mtd><msup><mi>z</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>W</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>∗</mo><msup><mi>a</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></msup><mo>+</mo><msup><mi>b</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup></mtd></mtr><mtr><mtd></mtd></mtr><mtr><mtd><msup><mi>a</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><msup><mi>g</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">(</mo><msup><mi>z</mi><mrow data-mjx-texclass="ORD"><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msup><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mjx-assistive-mml></mjx-container></div></div></li><li><p><span>那么在卷积神经网络中，其实做的事情是一样的</span></p><h3 id='这个是a0'><span>这个是a[0]</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509194018087.png" referrerpolicy="no-referrer" alt="image-20230509194018087"></p><h3 id='这个是w1'><span>这个是W^[1]</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509194033559.png" referrerpolicy="no-referrer" alt="image-20230509194033559"></p><h3 id='两个做卷积就相当于'><span>两个做卷积就相当于</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509194102915.png" referrerpolicy="no-referrer" alt="image-20230509194102915"></p><h3 id='之后结果矩阵加上一个偏移量'><span>之后结果矩阵加上一个偏移量</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509194201572.png" referrerpolicy="no-referrer" alt="image-20230509194201572"></p><p><span>就实现了上面的公式，那下一步就是把它输入进激活函数g^[1]里即可</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509210051212.png" referrerpolicy="no-referrer" alt="image-20230509210051212"></p><p><span>卷积神经网络中使用ReLU作为激活函数，输出一个同样大小的矩阵 a^[1]。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509210133523.png" referrerpolicy="no-referrer" alt="image-20230509210133523"></p><ul><li><h3 id='细节'><span>细节</span></h3><ul><li><span>一个过滤器就是在求一个特征，那过滤器的数量就是这一层卷积层的单元数。</span></li><li><span>最后由于是2个过滤器，所以最后结果的图像是 4 x 4 x 2</span></li></ul></li></ul></li><li><h3 id='卷积神经网络的特征------避免过拟合'><span>卷积神经网络的特征——避免过拟合</span></h3><ul><li><p><span>无论你的图像是多大的，卷积层的参数数量只和过滤器数量有关系</span></p></li><li><p><span>比如一个过滤器是3x3x3，那就是27个参数，加上偏差 b，一共28个参数</span></p></li><li><p><span>假设有10个过滤器，那一共是280个参数。</span></p></li><li><h3 id='参数的个数和你的图片大小完全没有关系'><span>参数的个数和你的图片大小完全没有关系</span></h3></li></ul></li><li><h3 id='总结------如何描述卷积神经网络一层'><span>总结——如何描述卷积神经网络一层</span></h3><ul><li><h3 id='fl--过滤器大小'><span>f^[l] = 过滤器大小</span></h3><ul><li><span>f^[l]表示过滤器的大小是 f x f </span></li></ul></li><li><h3 id='pl--padding的数量'><span>p^[l] = padding的数量</span></h3><ul><li><span>可以是valid卷积或者是same卷积</span></li></ul></li><li><h3 id='l表示第几层'><span>l表示第几层</span></h3></li><li><h3 id='sl--步幅大小'><span>s^[l] = 步幅大小</span></h3></li><li><h3 id='input是某一个维度的矩阵'><span>Input是某一个维度的矩阵</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509211701955.png" referrerpolicy="no-referrer" alt="image-20230509211701955"></p><ul><li><span>l - 1表示上一层，n_w为高度，n_w宽度，n_c为通道数</span></li><li><span>也就是input=上一层图片的高 宽 通道数</span></li></ul></li><li><h3 id='output也是某一个维度的矩阵'><span>output也是某一个维度的矩阵</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509211957564.png" referrerpolicy="no-referrer" alt="image-20230509211957564"></p><ul><li><p><span>也就是本层的图片高 宽 通道数（过滤器数）</span></p></li><li><p><span>本层的层数可以使用之前给过的公式计算出来</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509212109586.png" referrerpolicy="no-referrer" alt="image-20230509212109586"></p><ul><li><span>n的上标是H就是高度，W就是计算宽度</span></li></ul></li></ul></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509212329100.png" referrerpolicy="no-referrer" alt="image-20230509212329100"></p><p><span>这个是每一层（除第0层）过滤器的数量</span></p></li><li><h3 id='如何确定过滤器的大小'><span>如何确定过滤器的大小？</span></h3><ul><li><p><span>之前说，过滤器的通道数要和彩色图片（也就是输入）的通道数一样，所以每一个过滤器的通道数和上一层的输出通道数一样</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509212624881.png" referrerpolicy="no-referrer" alt="image-20230509212624881"></p></li></ul></li><li><h3 id='激活值也就是输出'><span>激活值，也就是输出</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509212856501.png" referrerpolicy="no-referrer" alt="image-20230509212856501"></p><ul><li><p><span>如果你执行的是批量梯度下降（batch梯度下降），或者是小批量梯度下降（mini-batch梯度下降）时</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509213026831.png" referrerpolicy="no-referrer" alt="image-20230509213026831"></p></li></ul></li><li><h3 id='参数w--每个过滤器的大小--过滤器的个数'><span>参数W = 每个过滤器的大小 * 过滤器的个数</span></h3><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509213304708.png" referrerpolicy="no-referrer" alt="image-20230509213304708"></li></ul></li><li><h3 id='损失数量l就是这一层过滤器的数量-也就是-ncl'><span>损失数量L就是这一层过滤器的数量 也就是 n_c^[l]</span></h3></li><li><h3 id='偏差bl--维度为-过滤器的数量-ncl-的列向量'><span>偏差b^[l] = 维度为 过滤器的数量 n_c^[l] 的列向量</span></h3><ul><li><span>程序中它是一个四维的矩阵或者四维张量</span></li><li><span>b^[l] = 1 x 1 x 1 x n_c^[l]</span></li></ul></li></ul></li><li><h3 id='最后的一个注意'><span>最后的一个注意</span></h3><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509213959289.png" referrerpolicy="no-referrer" alt="image-20230509213959289"></li></ul></li></ul><h2 id='18-简-单-卷-积-网-络-示-例--a-simple-convolution-network--example）'><span>1.8 简 单 卷 积 网 络 示 例 （ A simple convolution network  example）</span></h2><p><span>上节课，我们讲了如何为卷积网络构建一个卷积层。今天我们看一个深度卷积神经网络 的具体示例，顺便练习一下我们上节课所学的标记法。</span></p><ul><li><h3 id='这就是本节课的例子一个经典的卷积神经网络convnet'><span>这就是本节课的例子，一个经典的卷积神经网络ConvNet</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509220710682.png" referrerpolicy="no-referrer" alt="image-20230509220710682"></p><ul><li><span>可以根据上面的标注，写出每一层的信息</span></li></ul></li><li><h3 id='这里重点讲最后这里'><span>这里重点讲最后这里</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509220826678.png" referrerpolicy="no-referrer" alt="image-20230509220826678"></p><ul><li><span>这是输出层的输入，一个 7 x 7 x 40的图像，由于要把他输入给最后的激活函数（一般是逻辑回归或者是softmax函数，取决于你想做的事情）</span></li><li><span>可以将其平滑或展开成 1960 个单元，把它们展开成一个很长的向量。为了预测最终的输出结果，我们把这个长向量填充到逻辑回归函数或者softmax函数中</span></li><li><span>最后由函数输出y帽子</span></li></ul></li><li><h3 id='经典卷积神经网络的趋势'><span>经典卷积神经网络的趋势</span></h3><ul><li><span>信道数越来越多</span></li><li><span>高度和宽度会在一段时间内保持一致，然后随着网络深度的加深而逐渐减小</span></li></ul></li><li><h3 id='一个典型的卷积神经网络通常有三层'><span>一个典型的卷积神经网络通常有三层</span></h3><ul><li><span>卷积层 Conv，上个例子用的就是卷积层</span></li><li><span>池化层 POOL，Pooling layer</span></li><li><span>全连接层 FC，Full Connected</span></li></ul></li><li><p><span>虽然只有卷积层的卷积神经网络可能也会能做出很好的东西，</span><strong><span>但大部分神经网络架构师依然会添加池化层和全连接层。幸运的是，池化层和全连接层比卷积层更容易设计。</span></strong></p></li><li><p><span>当你掌握了其他两个层的概念，你就可以构建强大的神经网络了</span></p></li></ul><h2 id='19-池化层pooling-layers）'><span>1.9 池化层（Pooling layers）</span></h2><ul><li><p><span>除了卷积层，卷积网络也经常使用</span><strong><span>池化层来缩减模型的大小</span></strong><span>，</span><strong><span>提高计算速度</span></strong><span>，同时提高 </span><strong><span>所提取特征的鲁棒性</span></strong><span>，我们来看一下。</span></p></li><li><h3 id='例子'><span>例子</span></h3><ul><li><span>输入一个4x4的矩阵，执行池化类型：最大池化（max pooling），结果是一个2x2的矩阵</span></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509224211910.png" referrerpolicy="no-referrer" alt="image-20230509224211910"></li><li><span>执行过程是，将4x4的区域，分成不同的区域，</span><strong><span>结果矩阵中每一个输出的元素都是对应颜色区域的最大元素值</span></strong></li><li><span>这个过程就像是应用一个f = 2 的过滤器，s = 2。所以我们先对左上象限做了卷积，结果放入了矩阵，然后是右上，左下，右下。</span></li></ul></li><li><h3 id='关于最大池化功能的理解'><span>关于最大池化功能的理解</span></h3><ul><li><p><span>把4x4的输入看成一个</span><strong><span>某些特征的集合</span></strong><span>，也就是神经网络中某一层的非激活值集合。</span><strong><span>数字大意味着可能探测到了某些特定的特征</span></strong></p><ul><li><span>那么左上象限可能就探测到了比如 人脸、边缘、眼睛之类的特征，右上象限就探测出来</span></li></ul></li><li><h3 id='所以最大化运算的作用就是'><span>所以最大化运算的作用就是</span></h3><ul><li><strong><span>如果在过滤器中提取到某个特征，那么保留其最大值</span></strong></li><li><span>如果没有提取到这个特征， 可能在右上象限中不存在这个特征，那么其中的最大值也还是很小</span></li></ul></li><li><h3 id='不过最大池化被应用广泛的理由是它在很多实验中的实现效果很好'><span>不过最大池化被应用广泛的理由是，它在很多实验中的实现效果很好</span></h3></li></ul></li><li><h3 id='最大池化的一个特点有一组超参数但是不需要参数学习'><span>最大池化的一个特点：有一组超参数，但是不需要参数学习</span></h3><ul><li><span>也就是说梯度下降几乎什么都学不到，也无需改变任何值</span></li><li><span>一旦确定好了这组超参数，就一直用这组超参数执行</span></li></ul></li><li><h3 id='计算最大池化输出大小的公式'><span>计算最大池化输出大小的公式</span></h3><ul><li><p><span>和之前卷积层的计算公式一样</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509225137971.png" referrerpolicy="no-referrer" alt="image-20230509225137971"></p></li></ul></li><li><h3 id='立方体的最大池化计算过程'><span>立方体的最大池化计算过程</span></h3><ul><li><span>如果输入是一个 5 x 5 x n_c^[l]的矩阵，假设我们使用规模为3的过滤器，那输出矩阵应该是 3 x 3 x n_c^[l]</span></li><li><span>计算过程就是不同信道的过滤器分别对不同的信道的输入矩阵做最大池化，最后输出一个3 x 3 x n_c^[l]立方体。</span></li></ul></li><li><h3 id='另一种类型的池化------平均池化不太常用）'><span>另一种类型的池化——平均池化（不太常用）</span></h3><ul><li><p><span>和最大池化的区别只是，最大池化是保留一个区域内的最大值。</span></p></li><li><p><span>平均池化保留的是一个区域内的平均值</span></p></li><li><h3 id='它基本只用在深度很深的神经网络中比如使用平均池化分解-7-x-7-x-1000的矩阵分解为-1-x-1-x-1000的矩阵后面有例子'><span>它基本只用在深度很深的神经网络中，比如使用平均池化分解 7 x 7 x 1000的矩阵，分解为 1 x 1 x 1000的矩阵，后面有例子</span></h3></li><li><h3 id='总体而言最大池化用的多'><span>总体而言最大池化用的多</span></h3></li></ul></li><li><h3 id='总结-3'><span>总结：</span></h3><ul><li><p><span>池化的超级参数有 过滤器大小 f 和 步幅 s</span></p></li><li><p><span>常用的参数值是 f = 2  s = 2（效果相当于高度宽度缩减一半）也有用 f 3 s 2的</span></p></li><li><p><span>其他的超级参数比如 p，就得看你要做最大池化还是平均池化了，</span><strong><span>大部分情况下，最大池化很少用 padding，目前𝑝最常用 的值是 0，即𝑝 = 0。</span></strong></p></li><li><p><span>输入和输出的维度</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509230301945.png" referrerpolicy="no-referrer" alt="image-20230509230301945"></p></li><li><h3 id='池化过程中没有需要学习的参数'><span>池化过程中没有需要学习的参数。</span></h3><ul><li><span>执行反向传播时，</span><strong><span>反向传播没有参数适用于最大池化</span></strong><span>。只有这些设置过的超参数，可能是手动设置的，也可能是通过交叉验证设置 的。</span></li></ul></li></ul></li></ul><h2 id='110-卷-积-神-经-网-络-示-例--convolutional-neural-network--example）'><span>1.10 卷 积 神 经 网 络 示 例 （ Convolutional neural network  example）</span></h2><ul><li><h3 id='构建全卷积神经网络的构造模块我们已经掌握得差不多了下面来看个例子'><span>构建全卷积神经网络的构造模块我们已经掌握得差不多了，下面来看个例子。</span></h3></li><li><h3 id='这个例子使用到的网络结构和letnet5的网络结构像使用到了很多一样的参数但不是同一个网络'><span>这个例子使用到的网络结构和LetNet5的网络结构像，使用到了很多一样的参数，但不是同一个网络。</span></h3></li><li><h3 id='任务-要做一个手写输入辨别图片里是手写的0---9-的任意一个数字'><span>任务 :要做一个手写输入，辨别图片里是手写的0 - 9 的任意一个数字</span></h3></li></ul><p><span>我的输入是 32 x 32 x 3的手写7</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509230810056.png" referrerpolicy="no-referrer" alt="image-20230509230810056"></p><p><span> 第一层使用过滤器大小为5，步幅是1，padding是0，过滤器个数为6个的卷积层。</span></p><p><span>卷积层的维度为 n_H^[1] = (n_H^[ 0 ] + 2p - f) / s + 1 = (32 + 0 - 5) / 1 + 1 = 28 = n_W^[1]，n_C^[1] = 6</span></p><p><span>所以卷积层的输出为 28 x 28 x 6，输出标记为CONV1</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509231141333.png" referrerpolicy="no-referrer" alt="image-20230509231141333"></p><p><span>接下来构建池化层，使用 f = 2 , s = 2 , p = 0的最大池化层</span></p><p><span>最大池化层的输出高和宽为 (28 + 0 - 2) / 2 + 1 = 14，信道数相同，所以池化层输出是 14 x 14 x 6，输出标记为POOL1</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509231511618.png" referrerpolicy="no-referrer" alt="image-20230509231511618"></p><p><span>注意：池化层的作用本身就是将高和宽缩减一半</span></p><ul><li><h3 id='关于卷积神经网络的一层的划分'><span>关于卷积神经网络的“一层”的划分</span></h3><ul><li><p><span>主要有两种划分</span></p><ul><li><span>卷积层+池化层算1 层</span></li><li><span>卷积层单独算1层，池化层单独算1层</span></li></ul></li><li><p><span>我们使用第一种划分，因为在人们计算神经网络的层数时候，只会计算有权重和参数的层。池化层只有超参数，没有权重和参数，所以我们把卷积层+池化层算做一层</span></p></li><li><p><span>可能会在文献中看到有人把卷积层算一层，池化层算一层的案例，别担心这只是两种不同的表达</span></p></li></ul></li></ul><p><span>继续我们的例子</span></p><p><span>之后我构架第二层（卷积+池化）</span></p><p><span>卷积 f = 5 , s = 1 n_C^[2] = 16，p = 0</span></p><p><span>那么输入是 14 x 14 x 6，输出就是 10 x 10 x 16标记为CONV2</span></p><p><span>池化 f = 2  s = 2，输出结果就是 5 x 5 x 16标记为POOL2</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230509232058932.png" referrerpolicy="no-referrer" alt="image-20230509232058932"></p><p>&nbsp;</p><p><span>5 x 5 x 16总共包含400个元素，现在把POOL2平整化为一个大小为400的一维向量，用400个单元来构建下一层</span></p><p>&nbsp;</p><p><span>下一层是全连接层 标记为FC3，含有120个单元。</span></p><p><span>这400个单元和120个单元每一项都相连，这一层参数值W^[3]的维度是120 x 400，加上一个偏差参数b^[3]，最后输出是( 120 , 1 )的向量。</span></p><p>&nbsp;</p><p><span>下一层还是一个全连接层，标记为FC4，含有84个单元，那输出就是84,1的向量</span></p><p>&nbsp;</p><p><span>最后，用这 84 个单元填充一个 softmax 单元，我们想要识别0 - 9的输出，所以softmax有10个输出</span></p><p><strong><span>超参数的选择不建议自己设置，而是看看文献，选用一个在别人任务中效果很好的架构，使用这里面的超参数。</span></strong></p><ul><li><h3 id='卷积神经网络的趋势'><span>卷积神经网络的趋势</span></h3><ul><li><span>随着层级的深入，高和宽会越来越小，而通道数会越来越高，最后连接全连接层</span></li></ul></li><li><h3 id='还有一种神经网络模式是一个或者多个卷积后面跟着一个池化层然后重复一边然后是几个全连接层最后是一个softmax'><span>还有一种神经网络模式是一个或者多个卷积后面跟着一个池化层，然后重复一边，然后是几个全连接层，最后是一个softmax</span></h3></li><li><h3 id='神经网络激活值矩阵的形状激活值矩阵的大小和每层参数数量'><span>神经网络激活值矩阵的形状，激活值矩阵的大小和每层参数数量</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510134700330.png" referrerpolicy="no-referrer" alt="image-20230510134700330"></p><ul><li><p><span>注意几个点（许多卷积神经网络都有这些属性，模式上也类似）</span></p></li><li><p><span>池化层没有参数</span></p></li><li><p><span>卷积层的参数比较少</span></p></li><li><p><span>全连接层才是参数最多的</span></p></li><li><p><span>随着神经网络的加深，激活值会逐渐变小</span></p><ul><li><h3 id='如果激活值下降太快也会影响网络性能'><span>如果激活值下降太快，也会影响网络性能</span></h3></li></ul></li></ul></li><li><h3 id='一个卷积网络包含卷积层池化层全连接层现在主要在研究如何把它们几个组合起来建立高效的神经网络'><span>一个卷积网络包含卷积层、池化层、全连接层，现在主要在研究如何把它们几个组合起来，建立高效的神经网络。</span></h3><ul><li><span>NG的经验是，如果你是自己做应用，如何组合这些东西最好的办法，就是阅读别人的文献，看看人家怎么做的</span></li></ul></li></ul><h2 id='111-为什么使用卷积why-convolutions）'><span>1.11 为什么使用卷积？（Why convolutions?）</span></h2><ul><li><h3 id='这是本周最后一节课我们来分析一下卷积在神经网络中如此受用的原因'><span>这是本周最后一节课，我们来分析一下卷积在神经网络中如此受用的原因</span></h3></li><li><h3 id='和全连接层相比卷积层的两个主要优势是参数共享和稀疏连接'><span>和全连接层相比，卷积层的两个主要优势是参数共享和稀疏连接</span></h3><ul><li><p><span>标准的神经网络都是全连接层对吧？</span></p></li><li><p><span>那如果我的输入是一个32 x 32 x 3的图，然后我用6个 5 x 5的过滤器，那么第一层的输出维度是 28 x 28 x 6。</span></p></li><li><p><span>如果我们使用FC，那这张图就会变成3072大小的特征向量，而过滤器就是4074个参数，那全连接层的输出维度就是 4074 x 3072 = 1400万多，要训练参数太多太多了。 这还只是32 x32的图，如果是1000 x 1000 的呢？ 没法计算了</span></p></li><li><p><span>但如果使用卷积层的话，5 x 5 x 6 + 6，那就是156个参数，参数数量非常少</span></p></li><li><h3 id='为什么卷积层参数这么少呢'><span>为什么卷积层参数这么少呢？</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510143552559.png" referrerpolicy="no-referrer" alt="image-20230510143552559"></p><ul><li><h3 id='参数共享'><span>参数共享</span></h3><ul><li><span>想一下，如果你要做某个特征的提取，那过滤器的参数是不是可以作用在整个图片上？而不是左上角某一个参数，右下角某一个参数？</span></li><li><span>因为参数对于整个图片都适用，所以不会产生那么多参数</span></li></ul></li><li><h3 id='稀疏连接'><span>稀疏连接</span></h3><ul><li><span>还记得卷积层的运算吗？ 如果是一个 3 x3 的过滤器，那它每次只会和图片中的 其中9个特征来做运算，其他的像素值不会产生任何影响，这就是稀疏连接的概念</span></li></ul></li></ul></li></ul></li></ul><p>&nbsp;</p><ul><li><h3 id='卷积神经网络善于捕捉平移不变不太懂）'><span>卷积神经网络善于捕捉平移不变（不太懂）</span></h3></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510140523898.png" referrerpolicy="no-referrer" alt="image-20230510140523898"></p></li></ul><p><strong><span>上述几点就是卷积神经网络在CV中表现比较好的原因</span></strong></p><p><strong><span>最后我们来把之前的整合起来构建出来一个猫咪检测器</span></strong></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510140923223.png" referrerpolicy="no-referrer" alt="image-20230510140923223"></p><ul><li><p><span>输入特征x是猫咪图片，输出标签y是 是否为猫</span></p></li><li><p><span>添加卷积层和池化层</span></p></li><li><p><span>最后添加全连接层，和激活函数（softmax或者逻辑回归）输出预测值</span></p></li><li><p><span>成本函数</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510141135489.png" referrerpolicy="no-referrer" alt="image-20230510141135489"></p></li><li><p><span>最后用之前学过的梯度下降来做优化即可</span></p></li></ul><h1 id='第二周-深度卷积网络实例探究deep-convolutional-models--case-studies）'><span>第二周 深度卷积网络：实例探究（Deep convolutional models:  case studies）  </span></h1><h2 id='21-为什么要进行实例探究why-look-at-case-studies）'><span>2.1 为什么要进行实例探究？（Why look at case studies?）</span></h2><p><span>这周我们首先来看看一些卷积神经网络的实例分析</span></p><ul><li><h2 id='经典网络'><span>经典网络</span></h2><ul><li><span>LeNet-5</span></li><li><span>AlexNet</span></li><li><span>VGG</span></li></ul></li><li><p><span>ResNet（残差网络，152层深）</span></p></li></ul><h2 id='22-经典网络classic-networks）'><span>2.2 经典网络（Classic networks）</span></h2><ul><li><h3 id='letnet-5'><span>LetNet-5</span></h3><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510145455518.png" referrerpolicy="no-referrer" alt="image-20230510145455518"></p></li><li><p><span>这是1998年左右提出的，当时人们用平均池化比较多，也没有padding的概念，基本都是用valid卷积</span></p></li><li><p><span>基本的结构和我们之前的例子一样，2层卷积池化+2层全连接，最后使用softmax（后面改用的这个，最开始LetNet用了现在很少用的分类器）</span></p></li><li><h3 id='以前全连接层的激活函数使用的是sigmoidtanh而不是relu'><span>以前全连接层的激活函数使用的是sigmoid/tanh，而不是ReLU</span></h3></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510145806832.png" referrerpolicy="no-referrer" alt="image-20230510145806832"></p></li><li><h3 id='这个网络有一个模式现在都还在用就是1个或多个卷积层跟一个池化层然后又是多个卷积层跟一个池化层然后是全连接层最后是输出'><span>这个网络有一个模式现在都还在用，就是1个或多个卷积层跟一个池化层，然后又是多个卷积层跟一个池化层，然后是全连接层，最后是输出。</span></h3></li></ul></li><li><h3 id='alexnet'><span>AlexNet</span></h3><ul><li><p><span>网络是用论文的第一作者来命名</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510152304173.png" referrerpolicy="no-referrer" alt="image-20230510152304173"></p><ul><li><span>AlexNet和LesNet-5的不同在于，它的参数比LesNet多，前者有6000多万个参数，后者6万多个</span></li><li><span>AlexNet还使用了ReLu激活函数</span></li><li><span>AlexNet比ResNet表现会好一些</span></li><li><span>不过AlexNet比较复杂，超参数比较多，论文中作者不得不给出这些超参数</span></li></ul></li></ul></li><li><h3 id='vgg-16'><span>VGG-16</span></h3><ul><li><h3 id='一种没有那么多超参数只专注于构建卷积层的简单网络'><span>一种没有那么多超参数，只专注于构建卷积层的简单网络</span></h3></li><li><h3 id='超参数如下'><span>超参数如下</span></h3><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510153244266.png" referrerpolicy="no-referrer" alt="image-20230510153244266"></li></ul></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510153229095.png" referrerpolicy="no-referrer" alt="image-20230510153229095"></p></li><li><p><span>CONV 64指的是 64个过滤器的卷积层，其他同理。 x2就代表连续两层</span></p></li><li><p><span>16的意思是这个结构中包含了16 个卷积层和全连接层。</span></p></li><li><p><span>包含约 1.38亿个参数</span></p></li><li><p><span>优点是结构简单，缺点是要训练的参数巨大</span></p></li></ul></li><li><h3 id='看论文的话建议先alexnet再vgg-最后letnet'><span>看论文的话建议先AlexNet再VGG 最后LetNet</span></h3></li></ul><h2 id='23-残差网络resnetsresidual-networks-resnets'><span>2.3 残差网络(ResNets)(Residual Networks (ResNets))</span></h2><ul><li><h3 id='本次课我们会学习一些更先高级更强大的神经网络结构'><span>本次课我们会学习一些更先高级更强大的神经网络结构</span></h3></li><li><p><span>非常非常深的神经网络是很难训练的，因为存在梯度消失和梯度爆炸问题。</span></p></li><li><p><span>这次我们学习跳跃链接（Skip connection），它可以从某一层网络层获取激活，然后迅速反馈给另外一层，甚至是更深层。我们使用跳跃链接能够训练深度网络的ResNets，有时候深度可以超过100层，让我们开始吧</span></p></li><li><h3 id='resnets是由残差块residual-block）构建的'><span>ResNets是由残差块（Residual block）构建的</span></h3><ul><li><p><span>在标准的神经网络中 ，前向传播有两个步骤，一个是计算特征的线性组合，另一个是计算非线性的激活函数，得出激活值</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510160326795.png" referrerpolicy="no-referrer" alt="image-20230510160326795"></p></li><li><p><span>而跳跃链接就是将第a^[l]的值和z^[l + 2]相加，一起传入激活函数中，计算a^[l + 2]，也就是给a^[l]添加了一条捷径</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510160341278.png" referrerpolicy="no-referrer" alt="image-20230510160341278"></p><ul><li><span>也就是图片上，a^[l + 2] = g^[l + 1]</span><span>(</span><span>z^[l + 2] + a^[l])</span></li><li><span>这样就产生了一个残差块</span></li></ul></li></ul></li><li><h3 id='那怎么构建一个resnet呢-'><span>那怎么构建一个ResNet呢 ？</span></h3><ul><li><p><span>在标准全链接神经网络中，加上残差块</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510160923183.png" referrerpolicy="no-referrer" alt="image-20230510160923183"></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510160928308.png" referrerpolicy="no-referrer" alt="image-20230510160928308"></p><p><span>如图所示，5个残差块，构成了 一个ResNet残差网络</span></p></li></ul></li><li><h3 id='resnet解决的问题'><span>ResNet解决的问题</span></h3><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510161019715.png" referrerpolicy="no-referrer" alt="image-20230510161019715"></li><li><span>传统的神经网络的错误率，并不是越深，越低。而是深度越深，错误率会反弹。因为越后面优化算法越难训练（有梯度爆炸和梯度消失的问题</span></li><li><span>而ResNet在深度学习方面表现的很好，</span><strong><span>能够有助于解决决梯度消失和梯度爆炸问题</span></strong><span> </span></li><li><span>也让我们在训练更深网络的同时， 又能保证良好的性能。也许从另外一个角度来看，随着网络越来深，网络连接会变得臃肿， 但是 ResNet 确实在训练深度网络方面非常有效。</span></li></ul></li></ul><h2 id='24-残差网络为什么有用why-resnets-work）'><span>2.4 残差网络为什么有用？（Why ResNets work?）</span></h2><ul><li><h3 id='上一节课提到深度网络的深度越深训练集上的训练效率就会有所减弱但resnet不是这样'><span>上一节课提到，深度网络的深度越深，训练集上的训练效率就会有所减弱，但ResNet不是这样</span></h3></li><li><p><span>假设有两个深度神经网络</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510202437974.png" referrerpolicy="no-referrer" alt="image-20230510202437974"></p><ul><li><p><span>输入X，输出a^[l]，中间深层用Big NN来表示，激活函数ReLU，也就是激活值是0或者大于0。</span></p></li><li><p><span>我们来看a^[l+2]如何计算，</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510202842901.png" referrerpolicy="no-referrer" alt="image-20230510202842901"></p></li><li><p><span>展开得</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510202855028.png" referrerpolicy="no-referrer" alt="image-20230510202855028"></p></li><li><p><span>如果我们使用了L2正则化，或者是权重衰减，假设这里的W^[l+2] = 0，和偏置项也为0（</span><strong><span>也就是此时梯度消失了</span></strong><span>），那有</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510203024283.png" referrerpolicy="no-referrer" alt="image-20230510203024283"></p></li><li><p><span>因为激活函数是ReLU，所以激活值都是非负数，所以有</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510203043193.png" referrerpolicy="no-referrer" alt="image-20230510203043193"></p></li><li><h3 id='也就是说残差块让我们的网络梯度消失时返回梯度消失之前的状态从而解决梯度消失问题'><span>也就是说残差块让我们的网络梯度消失时，返回梯度消失之前的状态，从而解决梯度消失问题</span></h3></li><li><h3 id='这意味着即使给神经网络增加了这两层不论是把残差块添加到神经网络的中间还是末端位置它的效率也并不逊色于更简单的神经网络都不会影响网络-的表现在保持效率的同时还能提升效率'><span>这意味着即使给神经网络增加了这两层，不论是把残差块添加到神经网络的中间还是末端位置，它的效率也并不逊色于更简单的神经网络，都不会影响网络 的表现，在保持效率的同时还能提升效率</span></h3></li></ul></li><li><h3 id='还有一个细节'><span>还有一个细节</span></h3><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510204011920.png" referrerpolicy="no-referrer" alt="image-20230510204011920"></li><li><span>z是一个列向量，a^[l]也是一个列向量对不对？ 矩阵中只有维度一样的才能做加法减法，</span><strong><span>之所以这样是因为ResNets中用了很多的same卷积</span></strong></li><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230510204155449.png" referrerpolicy="no-referrer" alt="image-20230510204155449"></li><li><span>再看这个，如果a^[l]和a^[l+2]的维度不一样，那么会在a^[l]面前乘以一个矩阵Ws，来让最后的结果维度相等</span></li><li><strong><span>比如a^[l]是128维度的，a^[l+2]是256维度的，那就需要给g中的a^[l]乘以一个参数W_s^[l]，W_s^[l]的维度是256 x 128的，这样就可以转换了</span></strong></li></ul></li><li><h3 id='最后来看看resnet的图像识别'><span>最后来看看ResNet的图像识别</span></h3><ul><li><p><span>这有一个普通卷积神经网络（Plain ResNet论文中的称呼），有很多的卷积层，最后池化，全连接来输出一个softmax</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230512193753204.png" referrerpolicy="no-referrer" alt="image-20230512193753204"></p></li><li><p><span>如何把它转换为ResNet呢</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230512193846274.png" referrerpolicy="no-referrer" alt="image-20230512193846274"></p><p><span>加上跳跃连接即可</span></p></li><li><p><span>这里有一个细节，因为里面基本是same卷积，所以这才能让 z^[l +2] + a^[l]，因为他们的维度一样。</span></p></li><li><h3 id='普通网络和-resnets-网络常用的结构是卷积层-卷积层-卷积层-池化层-卷积层-卷积层-卷积-层-池化层依此重复直到最后有一个通过-softmax-进行预测的全连接层'><span>普通网络和 ResNets 网络常用的结构是：卷积层-卷积层-卷积层-池化层-卷积层-卷积层-卷积 层-池化层……依此重复。直到最后，有一个通过 softmax 进行预测的全连接层。</span></h3></li><li><h3 id='resnets-类似于其它很多网络也会有很多卷积层其中偶尔会有池化层或类池化层的-层不论这些层是什么类型正如我们在上一张幻灯片看到的你都需要调整矩阵𝑊𝑠的维度'><span>ResNets 类似于其它很多网络，也会有很多卷积层，其中偶尔会有池化层或类池化层的 层。不论这些层是什么类型，正如我们在上一张幻灯片看到的，你都需要调整矩阵𝑊𝑠的维度。</span></h3></li></ul></li></ul><h2 id='25-网络中的网络以及-1×1-卷积network-in-network-and--1×1-convolutions）0'><span>2.5 网络中的网络以及 1×1 卷积（Network in Network and  1×1 convolutions）0</span></h2><ul><li><h3 id='总结1x1的卷积主要用于对通道数进行压缩对低通道数的图片作用不大'><span>总结1x1的卷积主要用于对通道数进行压缩，对低通道数的图片作用不大。</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230512204003222.png" referrerpolicy="no-referrer" alt="image-20230512204003222"></p><p><span>如果想要让输入，变成输出的维度，那就可以用32个 1 x 1的过滤器做卷积运算，用ReLU激活，就得到了。</span></p><p><span>它和same不一样的是，same管的是矩阵的宽高，这个管的是信道的数量</span></p></li><li><h3 id='什么是网络中的网络'><span>什么是网络中的网络</span></h3><ul><li><p><span>指的就是对一个输入用1x1进行卷积，它的效果就好像对输入做了一个全连接层，有点抽象。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230512204247650.png" referrerpolicy="no-referrer" alt="image-20230512204247650"></p></li><li><p><span>这个1x1的过滤器在很多的架构中都有用到，之后就会讲inception网络中的应用</span></p></li></ul></li></ul><h2 id='26-谷歌-inception-网络简介inception-network-motivation）'><span>2.6 谷歌 Inception 网络简介（Inception network motivation）</span></h2><ul><li><h3 id='构建卷积层时你要决定过滤器的大小究竟是-1×1原来是-1×3猜测为口误）3×3-还-是-5×5或者要不要添加池化层而-inception-网络的作用就是代替你来决定虽然网络架-构因此变得更加复杂但网络表现却非常好'><span>构建卷积层时，你要决定过滤器的大小究竟是 1×1（原来是 1×3，猜测为口误），3×3 还 是 5×5，或者要不要添加池化层。而 Inception 网络的作用就是代替你来决定，虽然网络架 构因此变得更加复杂，但网络表现却非常好。</span></h3></li><li><h3 id='inception网络或者叫inception层的作用就是代替人工来确定卷积层中的过滤器类型或者确定是否需要创建卷积层或池化层'><span>Inception网络或者叫inception层的作用就是代替人工来确定卷积层中的过滤器类型，或者确定是否需要创建卷积层或池化层</span></h3></li><li><h2 id='示例'><span>示例</span></h2><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513153447967.png" referrerpolicy="no-referrer" alt="image-20230513153447967"></p></li><li><p><span>比如有一个输入 28 x 28 x 192，我不知道该使用什么大小的过滤器，我就给网络说我想试试（64个）1x1 、（128个）3x3、(32个) 5x5的过滤器，以及一个不卷积的纯最大池化层（32个过滤器）。</span><strong><span>注意所有的都使用了same卷积，保证能堆叠在一起</span></strong></p></li><li><p><span>网络会把输出结果堆叠到一起，变成一个28 x 28 x (64 + 128 + 32 + 32)的输出， 28 x  28 x 256</span></p></li><li><h3 id='这个就是inception网络的核心内容基本思想是-inception-网络不需要人为决定使用哪个过滤器或者是否需要池化而是由网络-自行确定这些参数你可以给网络添加这些参数的所有可能值然后把这些输出连接起来-让网络自己学习它需要什么样的参数采用哪些过滤器组合'><span>这个就是inception网络的核心内容，基本思想是 Inception 网络不需要人为决定使用哪个过滤器或者是否需要池化，而是由网络 自行确定这些参数，你可以给网络添加这些参数的所有可能值，然后把这些输出连接起来， 让网络自己学习它需要什么样的参数，采用哪些过滤器组合。</span></h3></li></ul></li><li><h2 id='计算成本问题'><span>计算成本问题</span></h2><ul><li><p><span>但是上面的网络最大的问题就是计算成本问题，我们用 5 x 5 的过滤器来举例子</span></p></li><li><h2 id='5-x-5-过滤器的输出是-28-x-28-x-32的结果矩阵这个矩阵中就含有25088个数字-每一个数字都需要经过-5-x-5-x-192次计算得出那这一共需要12亿次计算这太难以承受了'><span>5 x 5 过滤器的输出是 28 x 28 x 32的结果矩阵，这个矩阵中就含有25088个数字。 每一个数字都需要经过 5 x 5 x 192次计算得出，那这一共需要1.2亿次计算，这太难以承受了</span></h2></li></ul></li><li><h2 id='解决成本问题'><span>解决成本问题</span></h2><ul><li><p><span>之前学到的1x1卷积在这里就排上了用场，我们假设输入和输出维度都不变，分别是28 x 28 x 192和28 x 28 x 32</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513154253276.png" referrerpolicy="no-referrer" alt="image-20230513154253276"></p></li><li><p><span>我们采用这样的结构，先是用1x1卷积压缩输入的信道数（192 -&gt; 16），然后将压缩后的输入做卷积运算，这样将大大减少计算成本。</span></p><p><span>中间这一层有时候叫做</span><strong><span>瓶颈层</span></strong><span>，指的是某个对象最小的部分，比如一个瓶子瓶颈就是最小的部分，同理，</span><strong><span>瓶颈层就是网络中最小的部分</span></strong><span>，我们先缩小网络，再扩大它。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513154518973.png" referrerpolicy="no-referrer" alt="image-20230513154518973"></p></li><li><p><span>我们来计算有了瓶颈层之后的计算成本</span></p><p><span>我们使用16个1x1的过滤器，每一个过滤器的维度是1 x 1 x 192，输出是28 x 28 x 16。 输出中共有12544个数字，每一个数字要经过 1 x 1 x 192次运算，此时，计算成本约为240万。</span></p><p><span>然后我们把瓶颈层做5x5卷积，输出 28 x 28 x 32 = 25088个数字，每一个数字都要经过400次计算，那就是1000万次，加上上次的240万，</span><strong><span>最终需要1240万次，这比1.2亿次好多了不是吗</span></strong></p></li></ul></li><li><h3 id='总结-4'><span>总结</span></h3><ul><li><h3 id='当你构建神经网络层的时候不想决定池化层用多少的过滤器那么inception网络就是最好的选择我们可以应用各种类型的过滤器只需要-把输出连接起来'><span>当你构建神经网络层的时候，不想决定池化层用多少的过滤器，那么inception网络就是最好的选择，我们可以应用各种类型的过滤器，只需要 把输出连接起来。</span></h3></li><li><h3 id='这其中要解决计算成本问题'><span>这其中要解决计算成本问题</span></h3></li><li><h3 id='你可能会问仅仅大幅缩小表示层规模会不会影响神经网络的性能事实证明只要合-理构建瓶颈层你既可以显著缩小表示层规模又不会降低网络性能从而节省了计算'><span>你可能会问，仅仅大幅缩小表示层规模会不会影响神经网络的性能？事实证明，只要合 理构建瓶颈层，你既可以显著缩小表示层规模，又不会降低网络性能，从而节省了计算。</span></h3></li></ul></li></ul><h2 id='27-inception-网络inception-network）'><span>2.7 Inception 网络（Inception network）</span></h2><ul><li><p><span>上个视频中，我们了解了一个inception模块的样子，这节课我们来把这些模块组合起来，组成inception网络。</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513160409450.png" referrerpolicy="no-referrer" alt="image-20230513160409450"></p><ul><li><span>这就是一个inception模块的样子，和我们上节课学到的一样</span></li><li><strong><span>唯一的区别在于注意最大池化操作的输出维度应*是28 x 28 x 192，这里在最大池化层之后加了一个1x1卷积，压缩它的信道数，让它别占领所有的信道。</span></strong></li></ul></li><li><h2 id='inception网络'><span>inception网络</span></h2><ul><li><p><span>这是inception网络的论文中的截图</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513160748849.png" referrerpolicy="no-referrer" alt="image-20230513160748849"></p><ul><li><h2 id='熟不熟悉'><span>熟不熟悉？</span></h2></li><li><p><span>是不是同一个模块复用了好几次？</span></p></li><li><p><span>有的模块有细微的差别在于，它使用了最大池化层来压缩高和宽</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513160832987.png" referrerpolicy="no-referrer" alt="image-20230513160832987"></p><h2 id='所以inception网络里面用到的就是咱们学到的东西卷积-池化-全连接'><span>所以inception网络里面用到的就是咱们学到的东西，卷积 池化 全连接。</span></h2></li><li><h3 id='还有一些细节需要补充'><span>还有一些细节需要补充</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513160947309.png" referrerpolicy="no-referrer" alt="image-20230513160947309"></p><p><span>绿色圆圈的这三个位置，都是分支，每一个分支中是隐层+全连接+softmax，最后都是输出一个softmax</span></p><ul><li><h2 id='它确保了即便是隐藏单元和中间层编号-5）也参与了特征计算它们也能预测图片的分类它在-inception-网络中起到一种调整的-效果并且能防止网络发生过拟合'><span>它确保了即便是隐藏单元和中间层（编号 5）也参与了特征计算，它们也能预测图片的分类。它在 Inception 网络中，起到一种调整的 效果，并且能防止网络发生过拟合。</span></h2></li></ul></li></ul></li></ul></li><li><h2 id='如果你理解了-inception-模块你就能理解-inception-网络无非是很多-个-inception-模块一环接一环最后组成了网络'><span>如果你理解了 Inception 模块，你就能理解 Inception 网络，无非是很多 个 Inception 模块一环接一环，最后组成了网络。</span></h2></li><li><h2 id='直到现在你已经了解了许多专用的神经网络结构在下节视频中我将会告诉你们如-何真正去使用这些算法来构建自己的计算机视觉系统我们下节视频再见'><span>直到现在，你已经了解了许多专用的神经网络结构。在下节视频中，我将会告诉你们如 何真正去使用这些算法来构建自己的计算机视觉系统，我们下节视频再见</span></h2></li></ul><p>&nbsp;</p><h2 id='28-使-用-开-源-的-实-现-方-案--using-open-source--implementations）'><span>2.8 使 用 开 源 的 实 现 方 案 （ Using open-source  implementations）</span></h2><ul><li><p><span>NG说，复制别人的方案是非常难的，甚至一些研究机构或者是顶级大学博士生有的也很难做到，所以作者有的会把自己的研究成果开源，放在GitHub上，这节课主要就是如何从github上获取</span></p></li><li><h2 id='cv应用开发者的常用工作流程'><span>CV应用开发者的常用工作流程</span></h2><ul><li><h1 id='找一个你喜欢的架构然后找一个开源实现把它从github上下载下来以此为基础开始构建'><span>找一个你喜欢的架构，然后找一个开源实现，把它从GitHub上下载下来，以此为基础开始构建</span></h1></li><li><p><span>因为这些网络通畅需要很久去训练，要用很多的GPU很多的数据集，所以你下载别人训练好的网络，然后做迁移学习会更好</span></p></li></ul></li></ul><h2 id='29-迁移学习transfer-learning）'><span>2.9 迁移学习（Transfer Learning）</span></h2><ul><li><p><span>如果你要做一个计算机视觉的应用，相比于从头训练权重，或者说从随机初始化权重开 始，如果你下载别人已经训练好网络结构的权重，你通常能够进展的相当快，用这个作为预 训练，然后转换到你感兴趣的任务上。</span></p></li><li><h2 id='根据训练集大小来决定训练多少层'><span>根据训练集大小来决定训练多少层</span></h2><h3 id='假如说我们要做一个猫猫分类器输出结果是a猫-b猫和没有猫'><span>假如说我们要做一个猫猫分类器，输出结果是A猫 B猫和没有猫</span></h3><ul><li><h3 id='如果你的训练集非常小'><span>如果你的训练集非常小</span></h3><ul><li><p><span>那你要做的是自己做一个新的softmax单元，把它之前的层都冻结，只训练softmax单元的参数</span></p></li><li><h3 id='一个小技巧由于你前面的层都冻结了你就可以把softmax前面看成固定函数了因为你不改变它-也不训练它）你可以把输入图片x映射到softmax层之前的激活函数'><span>一个小技巧：由于你前面的层都冻结了，你就可以把softmax前面看成固定函数了（因为你不改变它 也不训练它），你可以把输入图片X，映射到softmax层之前的激活函数。</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513190053704.png" referrerpolicy="no-referrer" alt="image-20230513190053704"></p><h3 id='计算特征或者激活值把他们存在电脑硬盘里这样你就相当于对所有的训练样本都做了预计算这样你就不用每次都遍历训练集再重新计算这个激活值了'><span>计算特征或者激活值，把他们存在电脑硬盘里，这样你就相当于对所有的训练样本，都做了预计算。这样你就不用每次都遍历训练集再重新计算这个激活值了</span></h3><h3 id='最后你就得到了一个很浅层的softmax网络也就是输入-softmax-输出）这会加快你的速度'><span>最后你就得到了一个很浅层的softmax网络，（也就是输入-softmax-输出），这会加快你的速度。</span></h3></li></ul></li><li><h3 id='如果你的训练集比较大那你需要做的是冻结更少的层训练这些没冻结的层'><span>如果你的训练集比较大，那你需要做的是冻结更少的层，训练这些没冻结的层</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513190327758.png" referrerpolicy="no-referrer" alt="image-20230513190327758"></p></li></ul></li><li><h2 id='有一个规律数据集越大冻结的层越少极端情况是所有层都不冻结把别人训练好的参数当做初始化用它们来代替随机初始-化接着你可以用梯度下降训练更新网络所有层的所有权重'><span>有一个规律：数据集越大，冻结的层越少，极端情况是所有层都不冻结，把别人训练好的参数当做初始化。用它们来代替随机初始 化，接着你可以用梯度下降训练，更新网络所有层的所有权重。</span></h2></li><li><h2 id='这就是卷积网络训练中的迁移学习如果你的训练集不够大那你会用很多次迁移学习如果你的数据集非常巨大那你就可以从头开始训练所有东西'><span>这就是卷积网络训练中的迁移学习，如果你的训练集不够大，那你会用很多次迁移学习。如果你的数据集非常巨大，那你就可以从头开始训练所有东西。</span></h2></li></ul><h2 id='210-数据增强data-augmentation）'><span>2.10 数据增强（Data augmentation）</span></h2><ul><li><h3 id='大部分的计算机视觉任务使用很多的数据所以数据增强是经常使用的一种技巧来提高-计算机视觉系统的表现'><span>大部分的计算机视觉任务使用很多的数据，所以数据增强是经常使用的一种技巧来提高 计算机视觉系统的表现。</span></h3></li><li><h3 id='在实践中更多的数据对大多数计算机视觉任务都有所帮助不像其他领域有时候得到充足的数据但是效果并不怎么样'><span>在实践中，更多的数据对大多数计算机视觉任务都有所帮助，不像其他领域，有时候得到充足的数据，但是效果并不怎么样。</span></h3></li><li><h3 id='计算机视觉的主要问题是没有办法得到充-足的数据对大多数机器学习应用这不是问题但是对计算机视觉数据就远远不够'><span>计算机视觉的主要问题是没有办法得到充 足的数据。对大多数机器学习应用，这不是问题，但是对计算机视觉，数据就远远不够。</span></h3></li><li><h2 id='计算机视觉中常见的数据增强方法'><span>计算机视觉中常见的数据增强方法</span></h2><ul><li><h2 id='简单反转和剪裁'><span>简单反转和剪裁</span></h2><ul><li><h3 id='垂直镜像对称'><span>垂直镜像对称</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513190749036.png" referrerpolicy="no-referrer" alt="image-20230513190749036"></p></li><li><h3 id='随机剪裁'><span>随机剪裁</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513190828705.png" referrerpolicy="no-referrer" alt="image-20230513190828705"></p></li><li><h3 id='或者是轻度的变形扭曲'><span>或者是轻度的变形扭曲</span></h3></li><li><h2 id='不过上面的操作较为繁琐所以现实里用的稍微少一些'><span>不过上面的操作较为繁琐，所以现实里用的稍微少一些</span></h2></li></ul></li><li><h2 id='彩色转换'><span>彩色转换</span></h2><ul><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513193024379.png" referrerpolicy="no-referrer" alt="image-20230513193024379"></p><p><span>通过修改图片上的R G B 三个通道的值，改变图片的颜色</span></p><h3 id='注意在实践中rg-和-b-的值是根-据某种概率分布来决定的'><span>注意：在实践中，R、G 和 B 的值是根 据某种概率分布来决定的。</span></h3><h3 id='这样做带来的提升在于模型对于照片的颜色更改更具有鲁棒性'><span>这样做带来的提升在于模型对于照片的颜色更改更具有鲁棒性</span></h3><p><span>比如阳光偏黄，灯光偏蓝或者橙色，照在猫咪身上，就会有不同的效果</span></p></li><li><p><span>有一种影响色彩失真的算法是PCA，主成分分析，但是具体颜色的改变在AlexNet的论文中，称为PCA颜色增强，它可以改变图片的颜色，但大体保持一致。</span></p></li><li><p><span>如果不懂，你可以搜索论文去看，或者直接在网上找PCA颜色增强的开源实现方法</span></p></li></ul></li></ul></li><li><h3 id='在训练时实现数据增强'><span>在训练时实现数据增强</span></h3><ul><li><p><span>如果你的数据都保存在了硬盘上，那现在常用的做法是</span></p></li><li><p><span>常使用一个线程或者是多线3程，这些可以用来加载数据，实 现变形失真</span></p></li><li><p><span>然后传给其他的线程或者其他进程，来训练这个（编号 2）和这个（编号 1）， 可以并行实现。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513194456824.png" referrerpolicy="no-referrer" alt="image-20230513194456824"></p></li></ul></li><li><h3 id='数据增强也有一些超参数需要调整比如说颜色变化了多少以及随机裁剪的时候使用的参数一个好的开始可能是使用别人的开源实现了解他们如何实现数据增强'><span>数据增强也有一些超参数需要调整，比如说颜色变化了多少，以及随机裁剪的时候使用的参数，一个好的开始可能是使用别人的开源实现，了解他们如何实现数据增强。</span></h3></li></ul><h2 id='211-计算机视觉现状the-state-of-computer-vision）'><span>2.11 计算机视觉现状（The state of computer vision）</span></h2><ul><li><h3 id='大部分机器学习问题是介于少量数据和大量数据范围之间的当数据量小人们会使用更多的手工模块而数据量大的时候人们往往选择简单的算法和少量-手工模块'><span>大部分机器学习问题是介于少量数据和大量数据范围之间的，当数据量小，人们会使用更多的手工模块，而数据量大的时候，人们往往选择简单的算法，和少量 手工模块</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513200125073.png" referrerpolicy="no-referrer" alt="image-20230513200125073"></p><ul><li><span>轴上第一个是目标检测，第二个是图像识别，第三个是语音识别</span></li></ul></li><li><h3 id='机器学习当中机器的学习来源就俩'><span>机器学习当中，机器的学习来源就俩</span></h3><ul><li><span>被标记的数据（x和y）</span></li><li><span>手工工程</span></li></ul></li><li><h3 id='计算机视觉是一个非常复杂的功能我们数据量基本不够完全不够需求所以过去和现如今我们才会设计出这么多的架构'><span>计算机视觉是一个非常复杂的功能，我们数据量基本不够，完全不够需求，所以过去和现如今我们才会设计出这么多的架构</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513200513267.png" referrerpolicy="no-referrer" alt="image-20230513200513267"></p><h3 id='虽然近几年计算机视觉的数据量在剧增减少了一部分手工工程但是在计算机视觉上仍然有很多的网络架构使用手工-工程这就是为什么你会在计算机视觉中看到非常复杂的超参数选择比你在其他领域中要-复杂的多'><span>虽然近几年计算机视觉的数据量在剧增，减少了一部分手工工程，但是在计算机视觉上仍然有很多的网络架构使用手工 工程，这就是为什么你会在计算机视觉中看到非常复杂的超参数选择，比你在其他领域中要 复杂的多。</span></h3></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230513200935383.png" referrerpolicy="no-referrer" alt="image-20230513200935383"></p><ul><li><span>这个先跳过了，是给发论文和比赛的朋友们分享的小技巧，集成。</span></li></ul></li></ul><h1 id='第三周-目标检测object-detection）'><span>第三周 目标检测（Object detection）</span></h1><h2 id='31-目标定位object-localization）'><span>3.1 目标定位（Object localization）</span></h2><ul><li><p><span>这一周我们学习的主要内容是</span><strong><span>对象检测</span></strong><span>，它是计算机视觉领域中一个</span><strong><span>新兴的应用方向</span></strong><span>， 相比前两年，它的性能越来越好。在构建对象检测之前，我们</span><strong><span>先了解一下目标定位</span></strong></p></li><li><h2 id='什么是目标定位和目标检测'><span>什么是目标定位和目标检测？</span></h2><ul><li><h2 id='图片分类'><span>图片分类</span></h2><ul><li><p><span>之前我们一直在讲的是图片分类，就是看图片上是什么，</span><strong><span>比如说看有没有猫</span></strong></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515143805147.png" referrerpolicy="no-referrer" alt="image-20230515143805147"></p></li></ul></li><li><h2 id='对象定位'><span>对象定位</span></h2><p><span>现在我们要在图片分类的基础上，</span><strong><span>给识别出来的对象，标记它的位置，用红色框给框起来。</span></strong></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515143609227.png" referrerpolicy="no-referrer" alt="image-20230515143609227"></p><h3 id='定位就是判断物体在图片中的具体位置'><strong><span>定位</span></strong><span>就是判断物体在图片中的具体位置</span></h3></li><li><h2 id='对象检测'><span>对象检测</span></h2><ul><li><span>之后我们再讲一个图片中多个物体的对象定位问题，也就是</span><strong><span>对象检测问题</span></strong></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515143700459.png" referrerpolicy="no-referrer" alt="image-20230515143700459"></p></li></ul></li><li><p><strong><span>这周我们研究的主要是对单个对象进行的识别和定位，也就是对象定位问题</span></strong></p></li><li><p><strong><span>也就是说，图片分类的思路可以帮助做对象定位，对象定位的思路可以帮助做对象检测</span></strong></p></li><li><h2 id='从图片分类开始做对象定位'><span>从图片分类开始做对象定位</span></h2><ul><li><p><span>图像分类已经不陌生了，把一个图片输入进卷积神经网络，最后使用一个softmax或者logistic输出预测的分类。</span></p></li><li><p><span>一个自动驾驶相关的图像可能有四种输出分类：人 车 摩托车 背景，如果前三个都没识别出来，那么也就识别出一个背景了，这就是一个图像分类的流程了。</span></p></li><li><h3 id='如果你要想定位图片中车子的位置该怎么做呢'><span>如果你要想定位图片中车子的位置该怎么做呢？ </span></h3><p><span>你可以让神经网络多输出几个单元，输出一个边界框。具体来说就是让神经网络再输出一个方形框的四个角的坐标，b_x b_y b_h b_w。</span></p></li></ul></li><li><h2 id='这周的符号'><span>这周的符号</span></h2><ul><li><h4 id='图片的左上角坐标是0-0右下角的坐标是1-1）'><span>图片的左上角坐标是（0, 0)，右下角的坐标是（1, 1）</span></h4><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515211052438.png" referrerpolicy="no-referrer" alt="image-20230515211052438"></p></li><li><h3 id='如果想要确定边界框的具体位置就需要指定红色方框的中心点bx-by）以及方形框的高和宽-bh和bw'><span>如果想要确定边界框的具体位置，就需要指定红色方框的中心点（b_x, b_y），以及方形框的高和宽 b_h和b_w。</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515211332840.png" referrerpolicy="no-referrer" alt="image-20230515211332840"></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515211603172.png" referrerpolicy="no-referrer" alt="image-20230515211603172"></p></li><li><p><span>比如这个图像中，理想值 </span></p><ul><li><span>bx = 0.5，因为在图片中心</span></li><li><span>by = 0.7，因为在图片下部分</span></li><li><span>bh = 0.3，因为是图片高度的0.3倍</span></li><li><span>bw = 0.4，因为是图片宽度的0.4倍</span></li></ul></li></ul></li><li><h3 id='如果是做对象定位那就需要训练集中不但要有分类标签y还需要有对象的中心点位置bx-by）和宽高bh-bw然后把他们丢到神经网络中做监督学习'><span>如果是做对象定位，那就需要训练集中不但要有分类标签y，还需要有对象的中心点位置（bx by）和宽高(bh bw)。然后把他们丢到神经网络中做监督学习。</span></h3></li><li><h2 id='如何为监督学习任务定义目标标签y'><span>如何为监督学习任务定义目标标签y</span></h2><ul><li><p><span>在这个任务中，我们的输出有四个分类和四个数值</span></p><p><strong><span>所以我们的目标标签y应该是一个列向量</span></strong></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515212117329.png" referrerpolicy="no-referrer" alt="image-20230515212117329"></p><ul><li><span>Pc 是 图片中是否含有被检测对象（除了背景之外的，行人、车和摩托车），如果有 Pc = 1，则有其他输出，反之Pc = 0，那后续所有的输出都不重要了。</span></li><li><span>bx by bh bw，边框的四要素</span></li><li><span>C1 - C3，分别对应有效分类行人、车、摩托车</span></li></ul></li><li><h3 id='举例'><span>举例</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515212326211.png" referrerpolicy="no-referrer" alt="image-20230515212326211"></p><ul><li><span>因为有车，所以Pc = 1，C2 = 1。</span></li><li><span>因为任何被检测对象，所以Pc = 0，其他都不重要</span></li></ul></li></ul></li><li><h2 id='训练神经网络的损失函数'><span>训练神经网络的损失函数</span></h2><ul><li><p><span>这里简化介绍一下损失函数L，函数整体使用平方误差策略。</span></p><p><span>Y和Y帽子，都各有八个分量，需要对八个分量都做平方误差</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515212806424.png" referrerpolicy="no-referrer" alt="image-20230515212806424"></p></li><li><h2 id='请注意这里简化了很多实际的做法应该是'><span>请注意：这里简化了很多，实际的做法应该是</span></h2></li><li><h2 id='对pc使用逻辑回归或者平方预测误差'><span>对Pc使用逻辑回归，或者平方预测误差</span></h2></li><li><h2 id='可以不对c1-c2-c3和softmax激活函数应用对数损失函数'><span>可以不对c1 c2 c3和softmax激活函数应用对数损失函数</span></h2></li><li><h2 id='对边框坐标应用平方差或者类似做法'><span>对边框坐标应用平方差或者类似做法</span></h2></li></ul></li></ul><h2 id='32-特征点检测landmark-detection）'><span>3.2 特征点检测（Landmark detection）</span></h2><ul><li><p><span>上节课，我们讲了如何利用神经网络进行对象定位，即通过输出四个参数值𝑏𝑥、𝑏𝑦、𝑏ℎ 和𝑏𝑤给出图片中对象的边界框。</span></p></li><li><h3 id='更概括地说神经网络可以通过输出图片上特征点的𝑥-𝑦-坐标来实现对目标特征的识别我们看几个例子'><span>更概括地说，神经网络可以通过输出图片上特征点的(𝑥, 𝑦) 坐标来实现对目标特征的识别，我们看几个例子。</span></h3></li><li><h2 id='人脸识别应用'><span>人脸识别应用</span></h2><ul><li><p><span>比如给一张图片，输出图片中人物的四个眼角（左眼两个，右眼两个）</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515213535918.png" referrerpolicy="no-referrer" alt="image-20230515213535918"></p><p><span>那你就可以让神经网络多输出四个坐标， (l_1x, l_1y) (l_2x, l_2y) 。。。，这四个坐标就是眼角四个特征点的坐标。</span></p><p><span>如果你要这么做，</span><strong><span>你就需要在训练集的分类标签y中，人工打上这四个特征点</span></strong><span>，然后做监督学习，最后神经网络就可以对图片输出这四个特征点了。</span></p><h3 id='一定要注意的是四个特征点所代表的特征一定不能改变比如-l1xl1y就是左眼-左眼角的位置那你在之后的所有标签中都要这么标记不能改变它代表的特征'><span>一定要注意的是，四个特征点所代表的特征一定不能改变，比如 l_1x，l_1y就是左眼 左眼角的位置，那你在之后的所有标签中都要这么标记，不能改变它代表的特征。</span></h3></li><li><h3 id='如果你想识别出来整张脸那你就可以按照你的需求去加特征点比如识别整个脸的轮廓使用64个特征点那你就需要在训练集中都标注出来这些点的位置然后监督训练'><span>如果你想识别出来整张脸，那你就可以按照你的需求去加特征点，比如识别整个脸的轮廓，使用64个特征点，那你就需要在训练集中都标注出来这些点的位置，然后监督训练。</span></h3></li></ul></li><li><h2 id='人体动作识别'><span>人体动作识别</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515214048673.png" referrerpolicy="no-referrer" alt="image-20230515214048673"></p><ul><li><span>相似的，你可以给人体上也打上特征点，比如一个胸部中心的点，左肩膀，右肩膀的点。这样你就可以通过这些特征点获得人体的姿势。</span></li></ul></li></ul><h2 id='33-目标检测object-detection）'><span>3.3 目标检测（Object detection）</span></h2><ul><li><p><span>学过了</span><strong><span>对象定位</span></strong><span>和特征点检测，今天我们来构建一个对象检测算法。这节课，我们将学习如何通过卷积网络进行</span><strong><span>对象检测</span></strong><span>，采用的是</span><strong><span>基于滑动窗口sliding windows 的目标检测算法detection algorithm</span></strong><span>。</span></p></li><li><h2 id='一个汽车检测算法例子'><span>一个汽车检测算法例子</span></h2><ul><li><h3 id='创建一个汽车检测算法你需要做什么呢'><span>创建一个汽车检测算法你需要做什么呢？</span></h3><ol start='' ><li><p><span>创建一个标签训练集用x和y表示，使用经过剪裁的汽车图片，也就是让车子占据近乎整张图片。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515214703947.png" referrerpolicy="no-referrer" alt="image-20230515214703947"></p></li><li><p><span>使用这个训练集训练卷积神经网络</span></p></li><li><p><span>实现滑动窗口目标检测，步骤如下</span></p><p><span>有这么一张测试图片</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515215023738.png" referrerpolicy="no-referrer" alt="image-20230515215023738"></p><p><span>以及一个你设置好了特定大小的窗口</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515215057166.png" referrerpolicy="no-referrer" alt="image-20230515215057166"></p><ol start='' ><li><p><span>从图片最左上角开始，窗口放置在图片的左上角</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515215229307.png" referrerpolicy="no-referrer" alt="image-20230515215229307"></p></li><li><p><span>将窗口内的图片小方块输入进卷积神经网络，神经网络做出预测，预测出来0或者1。</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515215326494.png" referrerpolicy="no-referrer" alt="image-20230515215326494"></p><p><strong><span>这里使用的步幅是固定的</span></strong></p></li><li><p><span>使用其他两个更大的窗口，重复上述的操作。 也就是说对于一个图片要重复三次滑动窗口算法</span></p><ul><li><h2 id='如果你想让窗口滑动的更快你可以用一个更大的窗口截取更大的区域给卷积神经网络处理'><span>如果你想让窗口滑动的更快，你可以用一个更大的窗口，截取更大的区域，给卷积神经网络处理。</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230515215944363.png" referrerpolicy="no-referrer" alt="image-20230515215944363"></p></li></ul></li><li><p><span>如果你针对同一张图片，使用了大小不同的窗口，多次检测，那总有一个窗口可以检测出车子。</span></p></li></ol></li></ol></li></ul></li><li><h2 id='滑动窗口算法的缺点------计算成本'><span>滑动窗口算法的缺点——计算成本</span></h2><ul><li><span>如果你的</span><strong><span>窗口很小</span></strong><span>，那卷积神经网络要处理的窗口就会很多，</span><strong><span>超高计算成本</span></strong></li><li><span>如果你的</span><strong><span>窗口比较大</span></strong><span>，虽然处理的窗口数量少了，但是那粗颗粒度会</span><strong><span>影响性能</span></strong></li><li><span>在神经网络还没出现的时候，人们主要使用简单的线性分类器做滑动窗口算法，那个表现还不错。</span></li><li><span>但是有了卷积神经网络之后，再做滑动窗口，计算成本一下就起来了，</span><strong><span>运行这个滑动窗口会很慢</span></strong></li><li><strong><span>除非使用超细小的窗口或者极小的步幅，否则无法准确定位图片中的对象</span></strong></li></ul></li><li><p><span>不过卷积神经网络的计算成本问题已经有了解决，大大提高了效率，具体实现看下节课。 </span></p></li></ul><h2 id='34-滑动窗口的卷积实现convolutional-implementation-of--sliding-windows）'><span>3.4 滑动窗口的卷积实现（Convolutional implementation of  sliding windows）</span></h2><ul><li><p><span>今天来讲如何在卷积层上应用这个算法</span></p></li><li><p><span>之前我们介绍在传统卷积网络中如何实现滑动窗口。就是窗口从头到尾滑动，然后每次把窗口中的部分送入CNN中判断结果。</span><strong><span>这样效率太低，并且计算重复的东西太多</span></strong><span>，那么我们能不能一次性把一整张图片都送到CNN里判断结果？ 那么这节课就是介绍这个内容</span></p></li><li><p><span>首先是为了解除输入图片的大小限制，所以网络中不能用FC，因此我们需要把原来的FC转换为Conv。</span></p></li><li><h2 id='先来看如何将神经网络的全连接层转化成卷积层'><span>先来看如何将神经网络的全连接层转化成卷积层 </span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230516203318736.png" referrerpolicy="no-referrer" alt="image-20230516203318736"></p><ul><li><span>这是一个比较传统的卷积神经网络，输入了一个14 x 14 x 3 的图片，使用 16个5 x 5 x 3的过滤器进行卷积，输出 10 x 10 x 16的特征矩阵</span></li><li><span>再做最大池化，变成 5 x 5 x 16的特征矩阵</span></li><li><span>最后使用FC， FC单元个数 400，再做一次全连接，依旧输出400单元。</span></li><li><span>最后使用softmax输出预测标签y，y是一个向量，里面有四个分类出现的概率</span></li></ul><h3 id='转换后的结果'><span>转换后的结果</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230516203630135.png" referrerpolicy="no-referrer" alt="image-20230516203630135"></p><ul><li><span>不同主要是在后面，也就是FC被换成了 400个 5 x 5 x 16 的过滤器，输出 1 x 1 x 400 的特征矩阵</span></li><li><span>第二个FC也换成了400个 1 x 1 x 400 的过滤器，最后输出 1 x 1 x 400的特征矩阵</span></li><li><span>最最后使用 4个 1 x 1 x 400 的矩阵，输出 1 x 1 x 4的矩阵，也就是直接输出了一个y。</span></li></ul></li><li><h2 id='知道了如何把fc转换为conv接下来看如何实现滑动窗口算法'><span>知道了如何把FC转换为Conv接下来看如何实现滑动窗口算法</span></h2><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517183802703.png" referrerpolicy="no-referrer" alt="image-20230517183802703"></p><p><strong><span>假设我们训练集的图片是14 x 14 x 3，那最后输出的是一个 1 x 1 x 4的矩阵。</span></strong></p><p>&nbsp;</p><h3 id='然后我们的输入一张-16-x-16-x-3的图片窗口大小-14-x-14-x-3滑动步幅2卷积步幅1'><span>然后我们的输入一张 16 x 16 x 3的图片，窗口大小 14 x 14 x 3，滑动步幅2，卷积步幅1。</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517185256233.png" referrerpolicy="no-referrer" alt="image-20230517185256233"></p><p><span>滑动窗口从左上角开始，将窗口内的部分送给卷积神经网络，让它预测分类0或者1</span></p><p><span>然后向右挪动2步，再预测分类</span></p><p><span>最后神经网络运行了四次，输出了四个标签</span></p><ul><li><p><strong><span>输出的标签矩阵中。每一个值都对应了一个滑动窗口</span></strong></p></li><li><h3 id='结果发现这-4-次卷积操作中很多计算都是重复的所以执行滑动窗口的卷积时使得卷-积网络在这-4-次前向传播过程中共享很多计算'><span>结果发现，这 4 次卷积操作中很多计算都是重复的。所以执行滑动窗口的卷积时使得卷 积网络在这 4 次前向传播过程中共享很多计算</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517185725429.png" referrerpolicy="no-referrer" alt="image-20230517185725429"></p><p><span>比如编号1，这一步使用了相同参数的filter去做了卷积操作</span></p><p><span>编号2，这一步执行了同样最大池化</span></p><p><span>编号3使用了相同参数的 5 x 5 filter做了卷积操作</span></p><p><span>编号4,5也是一样，最后得到了 2 x 2 x 4的输出。输出的左上角是第一个窗口、以此类推。</span></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517190201830.png" referrerpolicy="no-referrer" alt="image-20230517190201830"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517190248131.png" referrerpolicy="no-referrer" alt="image-20230517190248131"></p></li></ul><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517190259802.png" referrerpolicy="no-referrer" alt="image-20230517190259802"></p><h2 id='但算法依旧存在问题不能输出最精准的边界框'><span>但算法依旧存在问题：不能输出最精准的边界框</span></h2><h2 id='35-bounding-box-预测bounding-box-predictions）'><span>3.5 Bounding Box 预测（Bounding box predictions）</span></h2><ul><li><h3 id='在这个视频中我们看看如何得到更精准的边界框'><span>在这个视频中，我们看看如何得到更精准的边界框</span></h3></li><li><h2 id='yolo算法'><span>YOLO算法</span></h2><h3 id='在滑动窗口算法中你会取得这些离散的位置值'><span>在滑动窗口算法中，你会取得这些离散的位置值</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517191606691.png" referrerpolicy="no-referrer" alt="image-20230517191606691"></p><p><span>有两个问题</span></p><ul><li><p><strong><span>但是有可能这些窗口并不能很完美的去匹配到车子的位置</span></strong></p></li><li><p><strong><span>并且可能适合车子的边框是长方形，而不是正方形，</span></strong></p><p><span>这时候怎么办呢？</span></p></li></ul><h2 id='可以让边框更加精准的算法是yoloyou-only-look-once-你只看一次）算法'><span>可以让边框更加精准的算法是YOLO（You only look once 你只看一次）算法</span></h2><p><span>比如一个 100 x 100的图，在图片上放上 3 x 3 的网格（实际应用的时候基本是19 x 19）</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517195454768.png" referrerpolicy="no-referrer" alt="image-20230517195454768"></p><ul><li><p><span>基本思路是使用第一周讲的图像分类和对象定位，也就是标签y定义成一个8维的，里面都是之前讲过的</span></p></li><li><p><span>每一个格子，对应着一个标签向量y，那也就是说最后的输出结果是一个 3 x 3 x 8 的矩阵</span></p></li><li><h2 id='步骤-1'><span>步骤</span></h2><ul><li><p><span>从左上角开始看每个格子里是否有检测的对象，如果没有，Pc = 0，其他 dont care  然后看下一个格子</span></p></li><li><p><span>如果某个格子有对象，Pc = 1，其他数值相应的填入；然后取图中对象的中点（也就是边框的中点），将它分配到包含了对象中点的格子里，如下图所示</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517195834538.png" referrerpolicy="no-referrer" alt="image-20230517195834538"></p></li><li><p><span>重复上述操作，直到整个图片遍历一遍</span></p></li></ul></li><li><p><span>使用YOLO你也就需要把输入图片x放入网络中，正向传播，得到对应的输出矩阵和结果。</span></p></li><li><h3 id='只要每个格子里的对象不超过1这个算法就是好用的'><span>只要每个格子里的对象不超过1，这个算法就是好用的</span></h3></li></ul></li><li><h2 id='yolo算法的优点'><span>YOLO算法的优点</span></h2><ul><li><span>可以让神经网络准确的输出边界框，任意的长宽比，不受滑动窗口分类器的步长大小限制</span></li><li><span>网络使用全卷积层实现，所以计算成本较低，</span><strong><span>甚至可以实时计算</span></strong></li></ul></li><li><h2 id='如何编码向量中的-bx-by-bh-bw-分量呢'><span>如何编码向量中的 bx by bh bw 分量呢</span></h2><ul><li><p><span>把这个格子假设成输入图片，假定网格左上角和右下角坐标分别是(0,0)和(1, 1)</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517201007157.png" referrerpolicy="no-referrer" alt="image-20230517201007157"></p></li><li><p><span>按照上次的目测法， bx = 0.4, by = 0.3, bh = 0.9 bw = 0.5，也就是说</span><strong><span>边界框的高度用格子总体宽度的比例表示</span></strong></p></li><li><p><span>所以bx 和 by的值应该是 0 - 1 ，如果超过了这个值，那车子应该在其他的格子里</span></p></li><li><p><span>如果车子过于庞大， 那有可能让bh 和 bw 超过1</span></p></li><li><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517201339986.png" referrerpolicy="no-referrer" alt="image-20230517201339986"></p></li></ul></li></ul><h2 id='36-交并比intersection-over-union）'><span>3.6 交并比（Intersection over union）</span></h2><ul><li><p><span>你如何判断对象检测算法运作良好呢？</span><strong><span>使用交并比函数(IoU)，可以用来评价对象检测算法</span></strong></p></li><li><h2 id='如何判断对象检测算法运行的怎么样'><span>如何判断对象检测算法运行的怎么样？</span></h2><p><span>如果你遇到了这种情况</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517201528654.png" referrerpolicy="no-referrer" alt="image-20230517201528654"></p><h3 id='那这个算法是运行的好还是不好呢交正比函数做的就是这个事情------判断对象定位的准确率'><span>那这个算法是运行的好还是不好呢？交正比函数做的就是这个事情——判断对象定位的准确率</span></h3></li><li><h2 id='交并比'><span>交并比</span></h2><ul><li><h3 id='它计算的就是两个边界框的交集和并集之比即-重合面积-总面积'><span>它计算的就是两个边界框的交集和并集之比，即 重合面积 /总面积 </span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517203918065.png" referrerpolicy="no-referrer" alt="image-20230517203918065"></p></li><li><p><span>在一般的CV任务中，一般IoU &gt;= 0.5 就是检测正确；如果他俩完全重合，那IoU是1，不过基本 &gt;= 0.5就算正确</span></p></li><li><p><span>这个阈值 0.5是可以自己调整的，如果想严格一些，就可以调高。这也是经验规定的，并不是什么科学论证的结果。</span></p></li></ul></li></ul><h2 id='37-非极大值抑制non-max-suppression）'><span>3.7 非极大值抑制（Non-max suppression）</span></h2><ul><li><p><span>到目前为止你们学到的对象检测中的一个问题是，</span><strong><span>你的算法可能对同一个对象做出多次 检测</span></strong><span>，所以算法不是对某个对象检测出一次，而是检测出多次。非极大值抑制这个方法可以 确保你的算法对每个对象只检测一次，我们讲一个例子。</span></p></li><li><h2 id='问题引入'><span>问题引入</span></h2><ul><li><h3 id='你需要做对象检测假设你在使用yolo算法19x19网格给了你这样的图片'><span>你需要做对象检测，假设你在使用YOLO算法，19x19网格，给了你这样的图片</span></h3><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517204404794.png" referrerpolicy="no-referrer" alt="image-20230517204404794"></p><p><strong><span>理想状态</span></strong><span>：虽然车子周围的很多格子都有车子的特征，但应该只有包含了车子中点的值的格子被标记有车子，最后只有一个框框</span></p><p><strong><span>现实情况：</span></strong><span>所有的格子都认为自己格子内有车，最后会变成好多个框框</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517205323190.png" referrerpolicy="no-referrer" alt="image-20230517205323190"></p></li></ul></li><li><h2 id='非最大值抑制是怎么起效果的'><span>非最大值抑制是怎么起效果的</span></h2><ul><li><p><span>非最大值抑制的作用就是清理多余的格子</span></p></li><li><h3 id='步骤-2'><span>步骤：</span></h3><ol start='' ><li><p><span>查看每个检测结果相关的概率Pc（Pc = Pc * c1 </span><span>	</span><span>或者c2 或者c3）或者是IoU</span></p></li><li><p><span>然后取比对相关结果中值最大的，剔除其他的。</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517210049091.png" referrerpolicy="no-referrer" alt="image-20230517210049091"></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517210040600.png" referrerpolicy="no-referrer" alt="image-20230517210040600"></p></li></ol><ul><li><span>就像上面的图，最后的结果被高亮，剔除的部分被变暗。</span></li></ul></li></ul></li><li><h2 id='算法细节'><span>算法细节</span></h2><ul><li><p><span>给你一张 19 x 19的图片，假设我们只做车辆识别，也就是输出向量y中去掉了 分类预测c1 c2 c3的。</span></p></li><li><p><span>那么19 x 19 的361个输出中，每个y就是，bx by bh bw是边界框，Pc是格子中有对象的概率</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517215623509.png" referrerpolicy="no-referrer" alt="image-20230517215623509"></p></li><li><h3 id='算法步骤'><span>算法步骤</span></h3><ol start='' ><li><p><span>将所有 Pc &lt;= 0.6的 边界框去掉</span></p></li><li><p><span>然后对剩下的格子</span></p><ul><li><span>一直选择概率最高的格子，把它输出成最后的结果（上个例子中高亮）</span></li><li><span>把其他的，和这些概率最高的格子有很高交并比的边框全部抛弃（上个例子中变暗）</span></li></ul></li></ol></li></ul></li></ul><h2 id='38-anchor-boxes'><span>3.8 Anchor Boxes</span></h2><ul><li><h3 id='目前yolo算法的问题在于每个格子只能检测出一个对象如果想让一个格子检测出多个对象使用anchor-box'><span>目前YOLO算法的问题在于每个格子只能检测出一个对象，如果想让一个格子检测出多个对象，使用Anchor Box</span></h3></li><li><h2 id='例子引入'><span>例子引入</span></h2><ul><li><p><span>有这么张图片</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517220218225.png" referrerpolicy="no-referrer" alt="image-20230517220218225"></p></li><li><p><span>可以看到有一个格子，又有车子又有人，如果你使用了右面的输出向量y，那你能做到的只是检测出其中一个对象（车或者人），你必须从两者中选择一个。</span></p></li></ul></li><li><h2 id='anchor-box'><span>Anchor Box</span></h2><ul><li><p><span>它的思路是，预先定义两个不同形状的Anchor Box，或者Anchor Box形状，你要做的就是把预测的结果和这两个Anchor box关联起来</span></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517220519983.png" referrerpolicy="no-referrer" alt="image-20230517220519983"></p><ul><li><strong><span>实际情况来说，你可能需要5个或者更多Anchor Box个数</span></strong></li></ul></li><li><p><span>然后你的输出标签y，也要和你定义的Anchor Box个数匹配</span></p><ul><li><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230517221119760.png" referrerpolicy="no-referrer" alt="image-20230517221119760"></li></ul></li><li><p><span>如果有车子，那么anchor box2的Pc = 1，其他参数也设置成对应的；如果有行人，那anchor box1的Pc = 1，其他参数设置成对应的</span></p></li></ul></li><li><h2 id='之前的yolo和含有anchor-box-的yolo'><span>之前的YOLO和含有anchor box 的YOLO</span></h2><ul><li><span>之前我们只需要对每个训练集的图像，加上 3 x 3的格子，每个格子输出一个标签y，维度8。</span></li><li></li></ul></li></ul></div></div>
</body>
</html>